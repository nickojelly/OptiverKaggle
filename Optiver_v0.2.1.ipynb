{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import public_timeseries_testing_util as optiver2023\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pack_sequence, unpack_sequence, unpad_sequence\n",
    "import torch\n",
    "from tqdm.notebook import trange,tqdm\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import torch_classes\n",
    "from model_saver import model_saver_wandb as model_saver\n",
    "import training_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = optiver2023.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date_id\n",
       "480    11000\n",
       "353    11000\n",
       "363    11000\n",
       "362    11000\n",
       "360    11000\n",
       "       ...  \n",
       "4      10560\n",
       "2      10505\n",
       "1      10505\n",
       "3      10505\n",
       "0      10505\n",
       "Name: count, Length: 481, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train.head()\n",
    "train.date_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 19/200 [00:08<01:19,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Targets for day=438,for stock_id=19, Excluding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 101/200 [00:41<00:34,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Targets for day=328,for stock_id=101, Excluding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 131/200 [00:54<00:44,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Targets for day=35,for stock_id=131, Excluding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 158/200 [01:04<00:14,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Targets for day=388,for stock_id=158, Excluding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:23<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train: 385, Length of test 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [00:00<00:00, 11226.33it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 13557.17it/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(torch_classes)\n",
    "trading_data = torch_classes.TradingData(train)\n",
    "hidden_size = 64\n",
    "trading_data.generate_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.0000e+00,  2.5554e+06, -1.0000e+00,  ...,  1.0000e+00,\n",
       "          2.4081e+03,  1.0000e+00],\n",
       "        [ 0.0000e+00,  2.7470e+05,  1.0000e+00,  ...,  1.0000e+00,\n",
       "          1.7408e+02,  1.0000e+00],\n",
       "        [ 0.0000e+00,  4.1553e+05,  1.0000e+00,  ...,  1.0003e+00,\n",
       "          2.2485e+04,  1.0000e+00],\n",
       "        ...,\n",
       "        [ 5.4000e+02,  0.0000e+00,  0.0000e+00,  ...,  9.9962e-01,\n",
       "          1.0968e+02,  9.9962e-01],\n",
       "        [ 5.4000e+02,  0.0000e+00,  0.0000e+00,  ...,  9.9673e-01,\n",
       "          2.1029e+05,  9.9664e-01],\n",
       "        [ 5.4000e+02,  0.0000e+00,  0.0000e+00,  ...,  1.0019e+00,\n",
       "          8.6312e+04,  1.0017e+00]], device='cuda:0'), batch_sizes=tensor([200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
       "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
       "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
       "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]), sorted_indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
       "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
       "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "        196, 197, 198, 199], device='cuda:0'), unsorted_indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
       "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
       "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "        196, 197, 198, 199], device='cuda:0'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trading_data.packed_val_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch_classes' from 'c:\\\\Users\\\\Nick\\\\Documents\\\\GitHub\\\\OptiverKaggle\\\\torch_classes.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(torch_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = torch_classes.GRUNetV2(12,64).to(device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output,hidden = model(trading_data.packed_x[0],p1=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = [trading_data.stocksDict[x] for x in trading_data.stock_batches[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[setattr(obj, 'hidden_all', val) for obj, val in zip(stocks,output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stocks[0].hidden_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.stack([stocks[x].hidden_out ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocks[0].hidden_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,199):\n",
    "    # print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_hidden,targets = trading_data.fetch_daily_data(day=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([55, 64])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_hidden[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.cat(stocks_hidden,dim=-1)\n",
    "# Y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([55])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[199].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.stack(targets).transpose(0,1).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0058, -0.0413, -0.0066,  0.0335, -0.0289, -0.0394,  0.0134, -0.0758,\n",
       "         0.0345,  0.0766,  0.0455, -0.0281, -0.0219,  0.0238,  0.0150, -0.0242,\n",
       "        -0.0660,  0.0787, -0.0352, -0.0648,  0.0190, -0.0238, -0.0089,  0.0385,\n",
       "         0.0180, -0.0844, -0.0150,  0.0454, -0.0312, -0.0190,  0.0099,  0.0214,\n",
       "        -0.0388, -0.0201, -0.0440, -0.0365,  0.0758,  0.0779, -0.0650, -0.0615,\n",
       "        -0.0058, -0.0082,  0.0397, -0.0063, -0.0342,  0.0479,  0.0241,  0.0055,\n",
       "        -0.0034, -0.0661,  0.0429,  0.0385, -0.0236,  0.0370,  0.0195,  0.0504,\n",
       "        -0.0141,  0.0525, -0.0613,  0.0284,  0.0181, -0.0586,  0.0746, -0.0345,\n",
       "        -0.0625,  0.0061,  0.0063,  0.0357, -0.0426, -0.0433, -0.0269, -0.0140,\n",
       "         0.0510,  0.0782,  0.0446,  0.0522,  0.0528,  0.0066,  0.0184,  0.0176,\n",
       "         0.0048, -0.0657,  0.0610,  0.0378,  0.0147,  0.0407, -0.0488,  0.0028,\n",
       "         0.0193, -0.0115,  0.0151,  0.0336,  0.0194, -0.0610, -0.0204,  0.0619,\n",
       "        -0.0443,  0.0384, -0.0529, -0.0628,  0.0554, -0.0022,  0.0299,  0.0494,\n",
       "        -0.0411,  0.0096, -0.0812,  0.0060, -0.0505,  0.0440, -0.0354,  0.0588,\n",
       "        -0.0177, -0.0462,  0.0505, -0.0509, -0.0484, -0.0056, -0.0652, -0.0581,\n",
       "        -0.0655, -0.0395,  0.0580, -0.0731, -0.0434, -0.0481,  0.0032, -0.0348,\n",
       "         0.0580,  0.0176,  0.0642, -0.0555,  0.0271, -0.0288, -0.0193,  0.0594,\n",
       "         0.0052,  0.0684,  0.0536,  0.0193,  0.0414, -0.0115, -0.0408,  0.0020,\n",
       "         0.0212,  0.0431, -0.0643, -0.0055,  0.0292,  0.0110,  0.0659, -0.0309,\n",
       "         0.0090,  0.0484,  0.0272, -0.0637, -0.0154,  0.0091, -0.0998, -0.0146,\n",
       "        -0.0486,  0.0622,  0.0581, -0.0632, -0.0614, -0.0075,  0.0005,  0.0066,\n",
       "        -0.0140,  0.0051,  0.0698, -0.0251,  0.0139,  0.0652,  0.0464, -0.0613,\n",
       "         0.0685,  0.0447, -0.0223, -0.0612,  0.0140,  0.0198, -0.0028,  0.0156,\n",
       "        -0.0588, -0.0472,  0.0456, -0.0154,  0.0051,  0.0172, -0.0040, -0.0260,\n",
       "         0.0224, -0.0081,  0.0567, -0.0744,  0.0251, -0.0301,  0.0398,  0.0692],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_L1 = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3362, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_L1(output, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -3.0297,  -5.5200,  -8.3899,  -4.0102,  -7.3498,   6.7794,  -2.4998,\n",
       "         -1.9598,  -5.9700,   7.9703,   5.3501,   2.5594,   8.3995, -10.7503,\n",
       "          2.3198,   1.7202,  -8.1700,  -3.5900, -11.4399,   4.2999,  -4.1801,\n",
       "        -11.5401,   8.9598,   2.4402,   4.4894,  -2.5600,  -0.3499,  -0.0602,\n",
       "         -1.6898,   4.6396,   4.4096,  -1.3900,   7.8106,   4.4203,  -7.7599,\n",
       "          0.1895,   3.5405,  -0.4297,   7.8297,  -4.6998,  -1.3798,   4.9603,\n",
       "        -10.6698,  -3.3402,   3.1400,  -1.4400,  17.1494,  10.1900,  -7.7599,\n",
       "          9.2602,   4.2605,   6.7103,   1.0002,  -6.3199,  -3.6699,  -1.2100,\n",
       "         10.4797,   6.8295,  -1.2797,  -3.6001,  -1.1098,   3.1304,  13.7997,\n",
       "          2.0301, -22.7302,  12.6195,  -5.0402, -11.9698,   4.0805,   0.0000,\n",
       "          9.6703, -17.8599,  -5.5802,  -2.5600,   3.3605,  -6.6799,  -1.9801,\n",
       "          0.2694,   0.0000,   0.0000,  -9.6399,  16.6905,   6.5303,  -1.0800,\n",
       "          6.3896, -18.8702,  30.5200,   6.9702,  -5.0598,  -2.5600,   1.5104,\n",
       "          2.8396, -42.3503,  10.5095, -10.0702,  -8.3101,  -5.0002,  11.2998,\n",
       "         -2.4700,  13.6805,  -5.2297,   8.4496,   0.0000,  -1.7601,  -5.2100,\n",
       "         -1.2797,  -8.0401,   0.5400,  -6.0201,  -2.6500,  -2.0403,  -7.2199,\n",
       "         -1.4001,  -5.1498,  -2.9802,  -5.3501,   3.3998,   2.6202,  -7.7403,\n",
       "          4.1699,   3.5405,  -2.0200,   9.5296,   4.1103,  -8.4198,   9.0897,\n",
       "         -8.5801,   5.8305,   0.6700,   2.1994,  -5.3298,   5.9104, -17.6603,\n",
       "         -2.0200,   4.1902,   0.0000,  -2.8402,   2.2197,  -8.0502,   2.7895,\n",
       "         -1.2100,  -9.0802,  -4.1199,  -6.8802,  -3.0398,   7.1502,   0.8404,\n",
       "          1.5903,  11.3106,  -1.9902,   0.0000,  -2.5302, -24.0803,   0.0000,\n",
       "         -4.9299,   4.3297,   0.0000,   0.8798, -23.6303,  -6.4701,  -2.5803,\n",
       "         -4.0001,  -6.1101,  -2.5398,   4.8995,   4.4203,   4.9198,   0.9704,\n",
       "          0.9799, -11.4399,   6.0296,  -3.4600, -17.7199,  -1.4800,  19.1998,\n",
       "          3.3796,  10.5095, -35.9303,   8.6999, -11.4697,   0.3695,   0.2897,\n",
       "         14.5996,  -4.3797,  -4.7398,   3.0100,  -3.6800,   6.3801,  19.0198,\n",
       "          5.4801,   6.3705,  11.9400, -11.5299,  -6.4898,   3.9995,  -0.6902,\n",
       "         -0.8100,  -8.4400,  -0.5102,   0.0000], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(trading_df:torch_classes.TradingData, config:dict):\n",
    "    with wandb.init(project=\"Optviver\", config=config,save_code=True):\n",
    "        wandb.define_metric(\"val_epoch_loss_l1\", summary=\"min\")\n",
    "        wandb.define_metric(\"epoch_l1_loss\", summary=\"min\")\n",
    "        model = torch_classes.GRUNetV2(12,hidden_size).to('cuda:0')\n",
    "        config = wandb.config\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "        trading_df.reset_hidden(config['hidden_size'])\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        \n",
    "        training_testing.train_model(trading_df,model,config,optimizer,criterion)\n",
    "\n",
    "    return(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'training_testing' from 'c:\\\\Users\\\\Nick\\\\Documents\\\\GitHub\\\\OptiverKaggle\\\\training_testing.py'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(training_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_static = {'learning_rate':0.00005, 'hidden_size':64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231003_140027-6xng3tv8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/Optviver/runs/6xng3tv8' target=\"_blank\">warm-bush-116</a></strong> to <a href='https://wandb.ai/nickojelly/Optviver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/Optviver' target=\"_blank\">https://wandb.ai/nickojelly/Optviver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/Optviver/runs/6xng3tv8' target=\"_blank\">https://wandb.ai/nickojelly/Optviver/runs/6xng3tv8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1debfd60ef49ae90ab6b161a009ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Temp\\ipykernel_23872\\4203262112.py\", line 11, in model_pipeline\n",
      "    training_testing.train_model(trading_df,model,config,optimizer,criterion)\n",
      "  File \"c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\training_testing.py\", line 37, in train_model\n",
      "    output,hidden = model(X,hidden_in,p1=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\torch_classes.py\", line 227, in forward\n",
      "  File \"c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\utils\\rnn.py\", line 517, in unpack_sequence\n",
      "    padded_sequences, lengths = pad_packed_sequence(packed_sequences, batch_first=True)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\utils\\rnn.py\", line 337, in pad_packed_sequence\n",
      "    return padded_output.index_select(batch_dim, unsorted_indices), lengths[unsorted_indices.cpu()]\n",
      "                                                                            ^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efe2203d3044fb4bb72fd54cb55eb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch_l1_loss</td><td>█▇▅▄▂▃▃▃▂▁▂▂▂▁▂▂▂▂▁▂▁▁▁▃▂▁▂▂▂▂▂▂▂▂▂▂▂▁▁▂</td></tr><tr><td>epoch_loss</td><td>█▅▅▄▃▂▃▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_1</td><td>▁█▁▃█▃█▁▃▁▃█▁▃▁▃█▁█▁▃█▃█▁▃█▃█▁▃▁▃█▁█▁▃█▁</td></tr><tr><td>val_epoch_loss</td><td>▇██▇▆▄▅▃▃▁▂▂▃▁▂▃▂▂▂▃▁▂▃▃▁▄▄▂▃▂▃▂▂▃▃▃▃▃▂▃</td></tr><tr><td>val_epoch_loss_l1</td><td>▆██▆▄▅▄▄▃▂▃▅▃▁▃▄▄▃▄▃▂▃▄▅▃▃▃▃▃▂▄▃▂▃▅▅▄▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>746</td></tr><tr><td>epoch_loss</td><td>5.93944</td></tr><tr><td>loss_1</td><td>5.14133</td></tr><tr><td>val_epoch_loss</td><td>5.02979</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">warm-bush-116</strong> at: <a href='https://wandb.ai/nickojelly/Optviver/runs/6xng3tv8' target=\"_blank\">https://wandb.ai/nickojelly/Optviver/runs/6xng3tv8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231003_140027-6xng3tv8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\Optiver_v0.2.1.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/OptiverKaggle/Optiver_v0.2.1.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m model_pipeline(trading_data, config_static)\n",
      "\u001b[1;32mc:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\Optiver_v0.2.1.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/OptiverKaggle/Optiver_v0.2.1.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     trading_df\u001b[39m.\u001b[39mreset_hidden(config[\u001b[39m'\u001b[39m\u001b[39mhidden_size\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/OptiverKaggle/Optiver_v0.2.1.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSmoothL1Loss()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/OptiverKaggle/Optiver_v0.2.1.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     training_testing\u001b[39m.\u001b[39;49mtrain_model(trading_df,model,config,optimizer,criterion)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/OptiverKaggle/Optiver_v0.2.1.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mreturn\u001b[39;00m(model)\n",
      "File \u001b[1;32mc:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\training_testing.py:37\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(trading_df, model, config, optimizer, criterion)\u001b[0m\n\u001b[0;32m     33\u001b[0m Y \u001b[39m=\u001b[39m trading_df\u001b[39m.\u001b[39mpacked_y[i]\u001b[39m.\u001b[39mdata\n\u001b[0;32m     35\u001b[0m hidden_in \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([x\u001b[39m.\u001b[39mhidden \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m stocks])\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m output,hidden \u001b[39m=\u001b[39m model(X,hidden_in,p1\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     38\u001b[0m hidden \u001b[39m=\u001b[39m hidden\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[39m# output  = torch.flatten(output)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\torch_classes.py:227\u001b[0m, in \u001b[0;36mGRUNetV2.forward\u001b[1;34m(self, x, h, test, p1)\u001b[0m\n\u001b[0;32m    224\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39m_replace(data\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_norm(x\u001b[39m.\u001b[39mdata))\n\u001b[0;32m    226\u001b[0m     x,hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgru(x,h)\n\u001b[1;32m--> 227\u001b[0m     x \u001b[39m=\u001b[39m unpack_sequence(x)\n\u001b[0;32m    229\u001b[0m     \u001b[39mreturn\u001b[39;00m x,hidden\n\u001b[0;32m    231\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\utils\\rnn.py:517\u001b[0m, in \u001b[0;36munpack_sequence\u001b[1;34m(packed_sequences)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munpack_sequence\u001b[39m(packed_sequences: PackedSequence) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Tensor]:\n\u001b[0;32m    489\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Unpacks PackedSequence into a list of variable length Tensors\u001b[39;00m\n\u001b[0;32m    490\u001b[0m \n\u001b[0;32m    491\u001b[0m \u001b[39m    ``packed_sequences`` should be a PackedSequence object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[39m        a list of :class:`Tensor` objects\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     padded_sequences, lengths \u001b[39m=\u001b[39m pad_packed_sequence(packed_sequences, batch_first\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    518\u001b[0m     unpacked_sequences \u001b[39m=\u001b[39m unpad_sequence(padded_sequences, lengths, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    519\u001b[0m     \u001b[39mreturn\u001b[39;00m unpacked_sequences\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\utils\\rnn.py:337\u001b[0m, in \u001b[0;36mpad_packed_sequence\u001b[1;34m(sequence, batch_first, padding_value, total_length)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[39mif\u001b[39;00m unsorted_indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m     batch_dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m batch_first \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m padded_output\u001b[39m.\u001b[39mindex_select(batch_dim, unsorted_indices), lengths[unsorted_indices\u001b[39m.\u001b[39;49mcpu()]\n\u001b[0;32m    338\u001b[0m \u001b[39mreturn\u001b[39;00m padded_output, lengths\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = model_pipeline(trading_data, config_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUNetV2(\n",
       "  (gru): GRU(12, 64, dropout=0.3)\n",
       "  (relu0): ReLU()\n",
       "  (batch_norm): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc0): Linear(in_features=12800, out_features=1024, bias=True)\n",
       "  (rl1): ReLU()\n",
       "  (drop1): Dropout(p=0.3, inplace=False)\n",
       "  (fc1): Linear(in_features=1024, out_features=640, bias=True)\n",
       "  (rl2): ReLU()\n",
       "  (drop2): Dropout(p=0.3, inplace=False)\n",
       "  (fc2): Linear(in_features=640, out_features=200, bias=True)\n",
       "  (rl3): ReLU()\n",
       "  (drop3): Dropout(p=0.3, inplace=False)\n",
       "  (fc3): Linear(in_features=200, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
