{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import public_timeseries_testing_util as optiver2023\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pack_sequence, unpack_sequence, unpad_sequence\n",
    "import torch\n",
    "from tqdm.notebook import trange,tqdm\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import torch_classes\n",
    "from model_saver import model_saver_wandb as model_saver\n",
    "import training_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = optiver2023.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date_id\n",
       "480    11000\n",
       "353    11000\n",
       "363    11000\n",
       "362    11000\n",
       "360    11000\n",
       "       ...  \n",
       "4      10560\n",
       "2      10505\n",
       "1      10505\n",
       "3      10505\n",
       "0      10505\n",
       "Name: count, Length: 481, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train.head()\n",
    "train.date_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 19/200 [00:08<01:16,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Targets for day=438,for stock_id=19, Excluding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 101/200 [00:41<00:35,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Targets for day=328,for stock_id=101, Excluding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 131/200 [00:55<00:46,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Targets for day=35,for stock_id=131, Excluding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 158/200 [01:05<00:15,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Targets for day=388,for stock_id=158, Excluding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:25<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train: 385, Length of test 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [00:00<00:00, 7736.53it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 11866.67it/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(torch_classes)\n",
    "trading_data = torch_classes.TradingData(train)\n",
    "hidden_size = 64\n",
    "trading_data.generate_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.0000e+00,  2.5554e+06, -1.0000e+00,  ...,  1.0000e+00,\n",
       "          2.4081e+03,  1.0000e+00],\n",
       "        [ 0.0000e+00,  2.7470e+05,  1.0000e+00,  ...,  1.0000e+00,\n",
       "          1.7408e+02,  1.0000e+00],\n",
       "        [ 0.0000e+00,  4.1553e+05,  1.0000e+00,  ...,  1.0003e+00,\n",
       "          2.2485e+04,  1.0000e+00],\n",
       "        ...,\n",
       "        [ 5.4000e+02,  0.0000e+00,  0.0000e+00,  ...,  9.9962e-01,\n",
       "          1.0968e+02,  9.9962e-01],\n",
       "        [ 5.4000e+02,  0.0000e+00,  0.0000e+00,  ...,  9.9673e-01,\n",
       "          2.1029e+05,  9.9664e-01],\n",
       "        [ 5.4000e+02,  0.0000e+00,  0.0000e+00,  ...,  1.0019e+00,\n",
       "          8.6312e+04,  1.0017e+00]], device='cuda:0'), batch_sizes=tensor([200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
       "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
       "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
       "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]), sorted_indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
       "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
       "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "        196, 197, 198, 199], device='cuda:0'), unsorted_indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
       "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
       "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "        196, 197, 198, 199], device='cuda:0'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trading_data.packed_val_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch_classes' from 'c:\\\\Users\\\\Nick\\\\Documents\\\\GitHub\\\\OptiverKaggle\\\\torch_classes.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(torch_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = torch_classes.GRUNetV2(12,64).to(device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output,hidden = model(trading_data.packed_x[0],p1=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = [trading_data.stocksDict[x] for x in trading_data.stock_batches[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[setattr(obj, 'hidden_all', val) for obj, val in zip(stocks,output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stocks[0].hidden_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.stack([stocks[x].hidden_out ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocks[0].hidden_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,199):\n",
    "    # print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_hidden,targets = trading_data.fetch_daily_data(day=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([55, 64])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_hidden[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.cat(stocks_hidden,dim=-1)\n",
    "# Y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([55])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[199].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.stack(targets).transpose(0,1).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.4300e-02, -4.0778e-02,  1.5305e-02,  3.1057e-02, -1.7071e-02,\n",
       "        -4.4198e-04, -3.7108e-02,  8.5022e-02,  5.6072e-05, -7.7676e-03,\n",
       "        -1.3965e-02, -7.6019e-02,  4.2677e-02,  6.0845e-02, -8.0050e-02,\n",
       "         4.5991e-02, -2.9843e-02, -4.3675e-02,  1.0126e-02,  6.3449e-02,\n",
       "        -9.2830e-02, -1.2253e-02, -1.6795e-02,  9.9773e-02, -3.3508e-02,\n",
       "        -6.8902e-02, -5.6889e-02, -2.7819e-02,  3.8781e-03, -3.1606e-02,\n",
       "         8.1263e-02, -6.8777e-02,  3.6871e-02, -3.8080e-02,  2.4221e-02,\n",
       "         1.4312e-02, -4.3721e-02, -5.1106e-02,  6.8797e-02, -4.3544e-02,\n",
       "         7.1691e-02, -6.3958e-02,  4.3330e-02,  1.5699e-02, -3.6409e-02,\n",
       "         5.8562e-02, -2.8182e-02,  2.4726e-02,  1.1522e-02,  2.2974e-02,\n",
       "        -9.4520e-04,  3.5101e-02, -3.3950e-02,  6.8962e-02,  2.0669e-02,\n",
       "         6.0671e-02, -2.5218e-02, -6.0350e-02, -3.3994e-02, -3.0336e-02,\n",
       "         9.9465e-03,  9.3753e-03,  3.5077e-02,  8.8010e-02,  6.3564e-02,\n",
       "        -2.8295e-02, -8.8596e-03,  5.0479e-02,  3.6945e-02, -4.3264e-02,\n",
       "         3.3369e-02, -4.5076e-02, -2.4905e-02,  3.3539e-02,  6.2544e-02,\n",
       "         3.5347e-02, -5.7034e-02,  4.3738e-02,  3.3936e-02, -2.0541e-02,\n",
       "         6.3094e-02,  2.9216e-02,  4.4906e-03,  2.7287e-02, -4.7222e-02,\n",
       "         2.8399e-02,  5.0518e-02,  2.7444e-02, -5.0592e-02,  5.2511e-03,\n",
       "        -1.8282e-02, -2.9810e-02, -6.2477e-02,  6.6426e-02, -6.4215e-02,\n",
       "         1.6293e-02, -1.4676e-02, -2.9576e-02, -6.9723e-02,  7.8269e-03,\n",
       "         6.9007e-02,  1.3366e-02,  1.0006e-02,  3.0977e-03, -4.5964e-02,\n",
       "        -2.8746e-03,  4.4478e-03, -8.7668e-03, -6.5525e-02,  5.6437e-02,\n",
       "         4.1620e-03, -3.8227e-02, -7.6090e-02,  1.1163e-02,  5.2203e-02,\n",
       "        -5.0562e-02,  4.4827e-02,  6.7632e-03, -4.6860e-02,  5.6911e-02,\n",
       "         1.9634e-02,  4.9348e-02, -7.3264e-03,  1.3149e-02, -4.2473e-02,\n",
       "         4.9738e-02,  3.5821e-02,  3.6686e-02,  2.6705e-02, -4.6060e-02,\n",
       "         8.0297e-02,  6.1291e-04,  6.6557e-02,  1.0421e-02, -7.0604e-02,\n",
       "        -9.5039e-03, -1.7778e-03,  6.7986e-03, -7.0934e-02,  4.2854e-02,\n",
       "         3.6447e-02,  6.4487e-03,  1.1186e-02, -9.5594e-03, -3.1794e-02,\n",
       "         2.5090e-02, -5.9340e-02, -2.3130e-03, -1.9720e-03,  3.5313e-02,\n",
       "        -9.8157e-02,  1.4231e-02,  2.0763e-03, -7.1620e-02, -1.4874e-03,\n",
       "        -6.4474e-02,  4.4372e-02, -6.0332e-02,  8.3999e-02, -1.0763e-02,\n",
       "         5.2841e-03, -5.0540e-02,  8.0225e-02,  4.1952e-02, -2.1725e-02,\n",
       "         6.2965e-02,  7.5662e-02, -6.2898e-04, -7.5499e-03,  2.2835e-02,\n",
       "        -1.3850e-02, -2.8618e-03,  9.1274e-02, -3.8758e-02, -4.9169e-02,\n",
       "         5.9685e-02, -2.0991e-02,  5.2926e-02, -2.9420e-02, -2.4244e-02,\n",
       "         8.5579e-03,  8.0153e-02,  3.5170e-02, -4.0706e-02,  7.5281e-03,\n",
       "         6.0455e-02, -9.3784e-02, -5.4192e-02,  8.6760e-03,  2.4662e-02,\n",
       "        -7.0699e-02, -2.5481e-02, -5.3668e-02,  3.0228e-02, -5.3345e-02,\n",
       "         8.9081e-03,  9.4701e-02, -7.2559e-02,  6.6443e-02,  9.3673e-03],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_L1 = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3359, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_L1(output, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -3.0297,  -5.5200,  -8.3899,  -4.0102,  -7.3498,   6.7794,  -2.4998,\n",
       "         -1.9598,  -5.9700,   7.9703,   5.3501,   2.5594,   8.3995, -10.7503,\n",
       "          2.3198,   1.7202,  -8.1700,  -3.5900, -11.4399,   4.2999,  -4.1801,\n",
       "        -11.5401,   8.9598,   2.4402,   4.4894,  -2.5600,  -0.3499,  -0.0602,\n",
       "         -1.6898,   4.6396,   4.4096,  -1.3900,   7.8106,   4.4203,  -7.7599,\n",
       "          0.1895,   3.5405,  -0.4297,   7.8297,  -4.6998,  -1.3798,   4.9603,\n",
       "        -10.6698,  -3.3402,   3.1400,  -1.4400,  17.1494,  10.1900,  -7.7599,\n",
       "          9.2602,   4.2605,   6.7103,   1.0002,  -6.3199,  -3.6699,  -1.2100,\n",
       "         10.4797,   6.8295,  -1.2797,  -3.6001,  -1.1098,   3.1304,  13.7997,\n",
       "          2.0301, -22.7302,  12.6195,  -5.0402, -11.9698,   4.0805,   0.0000,\n",
       "          9.6703, -17.8599,  -5.5802,  -2.5600,   3.3605,  -6.6799,  -1.9801,\n",
       "          0.2694,   0.0000,   0.0000,  -9.6399,  16.6905,   6.5303,  -1.0800,\n",
       "          6.3896, -18.8702,  30.5200,   6.9702,  -5.0598,  -2.5600,   1.5104,\n",
       "          2.8396, -42.3503,  10.5095, -10.0702,  -8.3101,  -5.0002,  11.2998,\n",
       "         -2.4700,  13.6805,  -5.2297,   8.4496,   0.0000,  -1.7601,  -5.2100,\n",
       "         -1.2797,  -8.0401,   0.5400,  -6.0201,  -2.6500,  -2.0403,  -7.2199,\n",
       "         -1.4001,  -5.1498,  -2.9802,  -5.3501,   3.3998,   2.6202,  -7.7403,\n",
       "          4.1699,   3.5405,  -2.0200,   9.5296,   4.1103,  -8.4198,   9.0897,\n",
       "         -8.5801,   5.8305,   0.6700,   2.1994,  -5.3298,   5.9104, -17.6603,\n",
       "         -2.0200,   4.1902,   0.0000,  -2.8402,   2.2197,  -8.0502,   2.7895,\n",
       "         -1.2100,  -9.0802,  -4.1199,  -6.8802,  -3.0398,   7.1502,   0.8404,\n",
       "          1.5903,  11.3106,  -1.9902,   0.0000,  -2.5302, -24.0803,   0.0000,\n",
       "         -4.9299,   4.3297,   0.0000,   0.8798, -23.6303,  -6.4701,  -2.5803,\n",
       "         -4.0001,  -6.1101,  -2.5398,   4.8995,   4.4203,   4.9198,   0.9704,\n",
       "          0.9799, -11.4399,   6.0296,  -3.4600, -17.7199,  -1.4800,  19.1998,\n",
       "          3.3796,  10.5095, -35.9303,   8.6999, -11.4697,   0.3695,   0.2897,\n",
       "         14.5996,  -4.3797,  -4.7398,   3.0100,  -3.6800,   6.3801,  19.0198,\n",
       "          5.4801,   6.3705,  11.9400, -11.5299,  -6.4898,   3.9995,  -0.6902,\n",
       "         -0.8100,  -8.4400,  -0.5102,   0.0000], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(trading_df:torch_classes.TradingData, config:dict):\n",
    "    with wandb.init(project=\"Optviver\", config=config,save_code=True):\n",
    "        wandb.define_metric(\"val_epoch_loss_l1\", summary=\"min\")\n",
    "        wandb.define_metric(\"epoch_l1_loss\", summary=\"min\")\n",
    "        model = torch_classes.GRUNetV2(12,hidden_size).to('cuda:0')\n",
    "        config = wandb.config\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "        trading_df.reset_hidden(config['hidden_size'])\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        \n",
    "        training_testing.train_model(trading_df,model,config,optimizer,criterion)\n",
    "\n",
    "    return(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'training_testing' from 'c:\\\\Users\\\\Nick\\\\Documents\\\\GitHub\\\\OptiverKaggle\\\\training_testing.py'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(training_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_static = {'learning_rate':0.00002, 'hidden_size':64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnickojelly\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231003_184310-68swunvk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/Optviver/runs/68swunvk' target=\"_blank\">fiery-resonance-118</a></strong> to <a href='https://wandb.ai/nickojelly/Optviver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/Optviver' target=\"_blank\">https://wandb.ai/nickojelly/Optviver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/Optviver/runs/68swunvk' target=\"_blank\">https://wandb.ai/nickojelly/Optviver/runs/68swunvk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7834405ad740dcaa3ecba9978a7357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model_pipeline(trading_data, config_static)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
