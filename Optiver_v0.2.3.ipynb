{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import public_timeseries_testing_util as optiver2023\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pack_sequence, unpack_sequence, unpad_sequence\n",
    "import torch\n",
    "from tqdm.notebook import trange,tqdm\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import torch_classes\n",
    "from model_saver import model_saver_wandb as model_saver\n",
    "import training_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = optiver2023.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date_id\n",
       "480    11000\n",
       "353    11000\n",
       "363    11000\n",
       "362    11000\n",
       "360    11000\n",
       "       ...  \n",
       "4      10560\n",
       "2      10505\n",
       "1      10505\n",
       "3      10505\n",
       "0      10505\n",
       "Name: count, Length: 481, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train.head()\n",
    "train.date_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 19/200 [00:08<01:19,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Targets for day=438,for stock_id=19, Excluding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 101/200 [00:42<00:34,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Targets for day=328,for stock_id=101, Excluding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 131/200 [00:56<00:45,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Targets for day=35,for stock_id=131, Excluding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 158/200 [01:09<00:15,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Targets for day=388,for stock_id=158, Excluding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:29<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train: 385, Length of test 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [00:00<00:00, 11973.77it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 12304.57it/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(torch_classes)\n",
    "trading_data = torch_classes.TradingData(train)\n",
    "hidden_size = 64\n",
    "trading_data.generate_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.0000e+00,  2.5554e+06, -1.0000e+00,  ...,  1.0000e+00,\n",
       "          2.4081e+03,  1.0000e+00],\n",
       "        [ 0.0000e+00,  2.7470e+05,  1.0000e+00,  ...,  1.0000e+00,\n",
       "          1.7408e+02,  1.0000e+00],\n",
       "        [ 0.0000e+00,  4.1553e+05,  1.0000e+00,  ...,  1.0003e+00,\n",
       "          2.2485e+04,  1.0000e+00],\n",
       "        ...,\n",
       "        [ 5.4000e+02,  0.0000e+00,  0.0000e+00,  ...,  9.9962e-01,\n",
       "          1.0968e+02,  9.9962e-01],\n",
       "        [ 5.4000e+02,  0.0000e+00,  0.0000e+00,  ...,  9.9673e-01,\n",
       "          2.1029e+05,  9.9664e-01],\n",
       "        [ 5.4000e+02,  0.0000e+00,  0.0000e+00,  ...,  1.0019e+00,\n",
       "          8.6312e+04,  1.0017e+00]], device='cuda:0'), batch_sizes=tensor([200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
       "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
       "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
       "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]), sorted_indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
       "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
       "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "        196, 197, 198, 199], device='cuda:0'), unsorted_indices=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
       "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
       "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "        196, 197, 198, 199], device='cuda:0'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trading_data.packed_val_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'training_testing' from 'c:\\\\Users\\\\Nick\\\\Documents\\\\GitHub\\\\OptiverKaggle\\\\training_testing.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(torch_classes)\n",
    "importlib.reload(training_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = torch_classes.GRUNetV2(12,64).to(device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output,hidden = model(trading_data.packed_x[0],p1=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = [trading_data.stocksDict[x] for x in trading_data.stock_batches[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[setattr(obj, 'hidden_all', val) for obj, val in zip(stocks,output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stocks[0].hidden_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.stack([stocks[x].hidden_out ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocks[0].hidden_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,199):\n",
    "    # print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_hidden,targets = trading_data.fetch_daily_data(day=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([55, 64])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_hidden[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.cat(stocks_hidden,dim=-1)\n",
    "# Y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([55])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[199].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.stack(targets).transpose(0,1).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0268, -0.0544,  0.0107,  0.0213, -0.0483,  0.0242,  0.0631, -0.0367,\n",
       "         0.0432, -0.0560, -0.0651,  0.0333,  0.0663,  0.0105,  0.0457,  0.0394,\n",
       "        -0.0326,  0.0270,  0.0400,  0.0032, -0.0464,  0.0644, -0.0442,  0.0012,\n",
       "        -0.0027, -0.0407,  0.0354,  0.0178,  0.0320, -0.0063, -0.0045, -0.0023,\n",
       "         0.0263,  0.0063,  0.0388, -0.0637, -0.0301,  0.0262, -0.0151,  0.0120,\n",
       "         0.0074,  0.0476,  0.0055,  0.0150, -0.0377, -0.0293, -0.0544, -0.0632,\n",
       "         0.0201, -0.0400, -0.0158,  0.0294,  0.0650,  0.0085,  0.0094,  0.0176,\n",
       "        -0.0158, -0.0436,  0.0020,  0.0647,  0.0196,  0.0493, -0.0661, -0.0575,\n",
       "         0.0301, -0.0412,  0.0603,  0.0074, -0.0011, -0.0464, -0.0049,  0.0356,\n",
       "        -0.0376, -0.0148,  0.0644, -0.0175,  0.0120,  0.0250,  0.0511, -0.0563,\n",
       "        -0.0458,  0.0245, -0.0429,  0.0656,  0.0811,  0.0451, -0.0747, -0.0507,\n",
       "        -0.0093, -0.0228,  0.0547, -0.0590,  0.0345,  0.0192, -0.0043, -0.0391,\n",
       "         0.0380, -0.0393, -0.0400, -0.0202, -0.0536,  0.0031, -0.0306, -0.0016,\n",
       "        -0.0410,  0.0233, -0.0792, -0.0231, -0.0383,  0.0446,  0.0038,  0.0040,\n",
       "        -0.0275, -0.0599,  0.0281,  0.0402,  0.0484,  0.0286,  0.0545, -0.0067,\n",
       "         0.0430,  0.0686,  0.0534, -0.0273,  0.0291,  0.0068, -0.0302,  0.0220,\n",
       "        -0.0079, -0.0348, -0.0232, -0.0421, -0.0418,  0.0433,  0.0016,  0.0302,\n",
       "         0.0414, -0.0674,  0.0506, -0.0095, -0.0388,  0.0776,  0.0493, -0.0094,\n",
       "         0.0246,  0.0342,  0.0443,  0.0343, -0.0238, -0.0598, -0.0398, -0.0692,\n",
       "        -0.0841, -0.0077,  0.0170,  0.0215,  0.0059, -0.0263, -0.0073,  0.0153,\n",
       "        -0.0558, -0.0333,  0.0280,  0.0395, -0.0273,  0.0756,  0.0082,  0.0082,\n",
       "        -0.0214,  0.0094, -0.0078,  0.0614,  0.0021, -0.0422, -0.0684, -0.0700,\n",
       "        -0.0104, -0.0227,  0.0440, -0.0207,  0.0058, -0.0095, -0.0203,  0.0677,\n",
       "        -0.0143, -0.0080,  0.0192,  0.0123, -0.0035, -0.0237, -0.0458,  0.0473,\n",
       "         0.0202,  0.0359,  0.0237,  0.0108,  0.0325,  0.0007,  0.0692,  0.0168],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_L1 = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3364, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_L1(output, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -3.0297,  -5.5200,  -8.3899,  -4.0102,  -7.3498,   6.7794,  -2.4998,\n",
       "         -1.9598,  -5.9700,   7.9703,   5.3501,   2.5594,   8.3995, -10.7503,\n",
       "          2.3198,   1.7202,  -8.1700,  -3.5900, -11.4399,   4.2999,  -4.1801,\n",
       "        -11.5401,   8.9598,   2.4402,   4.4894,  -2.5600,  -0.3499,  -0.0602,\n",
       "         -1.6898,   4.6396,   4.4096,  -1.3900,   7.8106,   4.4203,  -7.7599,\n",
       "          0.1895,   3.5405,  -0.4297,   7.8297,  -4.6998,  -1.3798,   4.9603,\n",
       "        -10.6698,  -3.3402,   3.1400,  -1.4400,  17.1494,  10.1900,  -7.7599,\n",
       "          9.2602,   4.2605,   6.7103,   1.0002,  -6.3199,  -3.6699,  -1.2100,\n",
       "         10.4797,   6.8295,  -1.2797,  -3.6001,  -1.1098,   3.1304,  13.7997,\n",
       "          2.0301, -22.7302,  12.6195,  -5.0402, -11.9698,   4.0805,   0.0000,\n",
       "          9.6703, -17.8599,  -5.5802,  -2.5600,   3.3605,  -6.6799,  -1.9801,\n",
       "          0.2694,   0.0000,   0.0000,  -9.6399,  16.6905,   6.5303,  -1.0800,\n",
       "          6.3896, -18.8702,  30.5200,   6.9702,  -5.0598,  -2.5600,   1.5104,\n",
       "          2.8396, -42.3503,  10.5095, -10.0702,  -8.3101,  -5.0002,  11.2998,\n",
       "         -2.4700,  13.6805,  -5.2297,   8.4496,   0.0000,  -1.7601,  -5.2100,\n",
       "         -1.2797,  -8.0401,   0.5400,  -6.0201,  -2.6500,  -2.0403,  -7.2199,\n",
       "         -1.4001,  -5.1498,  -2.9802,  -5.3501,   3.3998,   2.6202,  -7.7403,\n",
       "          4.1699,   3.5405,  -2.0200,   9.5296,   4.1103,  -8.4198,   9.0897,\n",
       "         -8.5801,   5.8305,   0.6700,   2.1994,  -5.3298,   5.9104, -17.6603,\n",
       "         -2.0200,   4.1902,   0.0000,  -2.8402,   2.2197,  -8.0502,   2.7895,\n",
       "         -1.2100,  -9.0802,  -4.1199,  -6.8802,  -3.0398,   7.1502,   0.8404,\n",
       "          1.5903,  11.3106,  -1.9902,   0.0000,  -2.5302, -24.0803,   0.0000,\n",
       "         -4.9299,   4.3297,   0.0000,   0.8798, -23.6303,  -6.4701,  -2.5803,\n",
       "         -4.0001,  -6.1101,  -2.5398,   4.8995,   4.4203,   4.9198,   0.9704,\n",
       "          0.9799, -11.4399,   6.0296,  -3.4600, -17.7199,  -1.4800,  19.1998,\n",
       "          3.3796,  10.5095, -35.9303,   8.6999, -11.4697,   0.3695,   0.2897,\n",
       "         14.5996,  -4.3797,  -4.7398,   3.0100,  -3.6800,   6.3801,  19.0198,\n",
       "          5.4801,   6.3705,  11.9400, -11.5299,  -6.4898,   3.9995,  -0.6902,\n",
       "         -0.8100,  -8.4400,  -0.5102,   0.0000], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(trading_df:torch_classes.TradingData, config:dict):\n",
    "    with wandb.init(project=\"Optviver\", config=config,save_code=True):\n",
    "        wandb.define_metric(\"val_epoch_loss_l1\", summary=\"min\")\n",
    "        wandb.define_metric(\"epoch_l1_loss\", summary=\"min\")\n",
    "        model = torch_classes.GRUNetV2(12,hidden_size).to('cuda:0')\n",
    "        config = wandb.config\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "        trading_df.reset_hidden(config['hidden_size'])\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        \n",
    "        training_testing.train_model(trading_df,model,config,optimizer,criterion)\n",
    "\n",
    "    return(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'training_testing' from 'c:\\\\Users\\\\Nick\\\\Documents\\\\GitHub\\\\OptiverKaggle\\\\training_testing.py'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(training_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_static = {'learning_rate':0.0002, 'hidden_size':64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7fdd5bfbd14a5fbaedfa4ed04cbdc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231004_155234-3j8whm6w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/Optviver/runs/3j8whm6w' target=\"_blank\">trim-river-120</a></strong> to <a href='https://wandb.ai/nickojelly/Optviver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/Optviver' target=\"_blank\">https://wandb.ai/nickojelly/Optviver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/Optviver/runs/3j8whm6w' target=\"_blank\">https://wandb.ai/nickojelly/Optviver/runs/3j8whm6w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16924f0d5f85442a9e38cf78e64f1ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock\n",
      "1034000\n",
      "day\n",
      "1034000\n",
      "target\n",
      "1034000\n",
      "pred\n",
      "1034000\n",
      "   stock  day    target      pred\n",
      "0      0    0 -3.029704 -0.102660\n",
      "1      1    0 -5.519986  0.100276\n",
      "2      2    0 -8.389950 -0.011000\n",
      "3      3    0 -4.010201 -0.144121\n",
      "4      4    0 -7.349849 -0.080975\n",
      "5      5    0  6.779432 -0.035082\n",
      "6      6    0 -2.499819  0.046695\n",
      "7      7    0 -1.959801  0.025310\n",
      "8      8    0 -5.970001 -0.079216\n",
      "9      9    0  7.970333 -0.069829\n",
      "created path\n",
      "stock\n",
      "1034000\n",
      "day\n",
      "1034000\n",
      "target\n",
      "1034000\n",
      "pred\n",
      "1034000\n",
      "stock\n",
      "1034000\n",
      "day\n",
      "1034000\n",
      "target\n",
      "1034000\n",
      "pred\n",
      "1034000\n",
      "stock\n",
      "1034000\n",
      "day\n",
      "1034000\n",
      "target\n",
      "1034000\n",
      "pred\n",
      "1034000\n",
      "stock\n",
      "1034000\n",
      "day\n",
      "1034000\n",
      "target\n",
      "1034000\n",
      "pred\n",
      "1034000\n"
     ]
    }
   ],
   "source": [
    "model = model_pipeline(trading_data, config_static)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
