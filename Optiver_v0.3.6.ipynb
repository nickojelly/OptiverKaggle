{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils.public_timeseries_testing_util as optiver2023\n",
    "from torch.nn.utils.rnn import (\n",
    "    pack_padded_sequence,\n",
    "    pack_sequence,\n",
    "    unpack_sequence,\n",
    "    unpad_sequence,\n",
    ")\n",
    "import torch\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import utils.torch_classes\n",
    "from utils.model_saver import model_saver_wandb as model_saver\n",
    "import utils.training_testing\n",
    "from itertools import combinations\n",
    "import gc\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import utils.training_testing_double\n",
    "from utils.conts import *\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: \"WANDB_NOTEBOOK_NAME\"=os.path.basename(__file__)\n"
     ]
    }
   ],
   "source": [
    "%env \"WANDB_NOTEBOOK_NAME\" os.path.basename(__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = optiver2023.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\n",
    "        \"cuda:0\"\n",
    "    )  # you can continue going on here, like cuda:1 cuda:2....etc.\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date_id\n",
       "480    11000\n",
       "353    11000\n",
       "363    11000\n",
       "362    11000\n",
       "360    11000\n",
       "       ...  \n",
       "4      10560\n",
       "2      10505\n",
       "1      10505\n",
       "3      10505\n",
       "0      10505\n",
       "Name: count, Length: 481, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "train.head()\n",
    "train.date_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_columns = [\n",
    "    \"stock_id\",\n",
    "    \"seconds_in_bucket\",\n",
    "    \"imbalance_size\",\n",
    "    \"imbalance_buy_sell_flag\",\n",
    "    \"reference_price\",\n",
    "    \"matched_size\",\n",
    "    \"far_price\",\n",
    "    \"near_price\",\n",
    "    \"bid_price\",\n",
    "    \"bid_size\",\n",
    "    \"ask_price\",\n",
    "    \"ask_size\",\n",
    "    \"wap\",\n",
    "    \"overall_medvol\",\n",
    "    \"first5min_medvol\",\n",
    "    \"last5min_medvol\",\n",
    "    \"bid_plus_ask_sizes\",\n",
    "    \"imbalance_ratio\",\n",
    "    \"imb_s1\",\n",
    "    \"imb_s2\",\n",
    "    \"ask_x_size\",\n",
    "    \"bid_x_size\",\n",
    "    \"ask_minus_bid\",\n",
    "    \"bid_price_over_ask_price\",\n",
    "    \"reference_price_minus_far_price\",\n",
    "    \"reference_price_times_far_price\",\n",
    "    \"reference_price_times_near_price\",\n",
    "    \"reference_price_minus_ask_price\",\n",
    "    \"reference_price_times_ask_price\",\n",
    "    \"reference_price_ask_price_imb\",\n",
    "    \"reference_price_minus_bid_price\",\n",
    "    \"reference_price_times_bid_price\",\n",
    "    \"reference_price_bid_price_imb\",\n",
    "    \"reference_price_minus_wap\",\n",
    "    \"reference_price_times_wap\",\n",
    "    \"reference_price_wap_imb\",\n",
    "    \"far_price_minus_near_price\",\n",
    "    \"far_price_times_near_price\",\n",
    "    \"far_price_minus_ask_price\",\n",
    "    \"far_price_times_ask_price\",\n",
    "    \"far_price_minus_bid_price\",\n",
    "    \"far_price_times_bid_price\",\n",
    "    \"far_price_times_wap\",\n",
    "    \"far_price_wap_imb\",\n",
    "    \"near_price_minus_ask_price\",\n",
    "    \"near_price_times_ask_price\",\n",
    "    \"near_price_ask_price_imb\",\n",
    "    \"near_price_minus_bid_price\",\n",
    "    \"near_price_times_bid_price\",\n",
    "    \"near_price_bid_price_imb\",\n",
    "    \"near_price_minus_wap\",\n",
    "    \"near_price_wap_imb\",\n",
    "    \"ask_price_minus_bid_price\",\n",
    "    \"ask_price_times_bid_price\",\n",
    "    \"ask_price_minus_wap\",\n",
    "    \"ask_price_times_wap\",\n",
    "    \"ask_price_wap_imb\",\n",
    "    \"bid_price_minus_wap\",\n",
    "    \"bid_price_times_wap\",\n",
    "    \"bid_price_wap_imb\",\n",
    "    \"reference_price_far_price_near_price_imb2\",\n",
    "    \"reference_price_far_price_ask_price_imb2\",\n",
    "    \"reference_price_far_price_bid_price_imb2\",\n",
    "    \"reference_price_far_price_wap_imb2\",\n",
    "    \"reference_price_near_price_ask_price_imb2\",\n",
    "    \"reference_price_near_price_bid_price_imb2\",\n",
    "    \"reference_price_near_price_wap_imb2\",\n",
    "    \"reference_price_ask_price_bid_price_imb2\",\n",
    "    \"reference_price_ask_price_wap_imb2\",\n",
    "    \"reference_price_bid_price_wap_imb2\",\n",
    "    \"far_price_near_price_ask_price_imb2\",\n",
    "    \"far_price_near_price_bid_price_imb2\",\n",
    "    \"far_price_near_price_wap_imb2\",\n",
    "    \"far_price_ask_price_bid_price_imb2\",\n",
    "    \"far_price_ask_price_wap_imb2\",\n",
    "    \"far_price_bid_price_wap_imb2\",\n",
    "    \"near_price_ask_price_bid_price_imb2\",\n",
    "    \"near_price_ask_price_wap_imb2\",\n",
    "    \"near_price_bid_price_wap_imb2\",\n",
    "    \"ask_price_bid_price_wap_imb2\",\n",
    "    \"pca_prices\",\n",
    "]\n",
    "\n",
    "weights = [\n",
    "    0.004,\n",
    "    0.001,\n",
    "    0.002,\n",
    "    0.006,\n",
    "    0.004,\n",
    "    0.004,\n",
    "    0.002,\n",
    "    0.006,\n",
    "    0.006,\n",
    "    0.002,\n",
    "    0.002,\n",
    "    0.008,\n",
    "    0.006,\n",
    "    0.002,\n",
    "    0.008,\n",
    "    0.006,\n",
    "    0.002,\n",
    "    0.006,\n",
    "    0.004,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.001,\n",
    "    0.006,\n",
    "    0.004,\n",
    "    0.002,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.004,\n",
    "    0.001,\n",
    "    0.001,\n",
    "    0.002,\n",
    "    0.002,\n",
    "    0.006,\n",
    "    0.004,\n",
    "    0.004,\n",
    "    0.004,\n",
    "    0.006,\n",
    "    0.002,\n",
    "    0.002,\n",
    "    0.04,\n",
    "    0.002,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.04,\n",
    "    0.002,\n",
    "    0.001,\n",
    "    0.006,\n",
    "    0.004,\n",
    "    0.004,\n",
    "    0.006,\n",
    "    0.001,\n",
    "    0.004,\n",
    "    0.004,\n",
    "    0.002,\n",
    "    0.006,\n",
    "    0.004,\n",
    "    0.006,\n",
    "    0.004,\n",
    "    0.006,\n",
    "    0.004,\n",
    "    0.002,\n",
    "    0.001,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.002,\n",
    "    0.008,\n",
    "    0.004,\n",
    "    0.004,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.006,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.004,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.004,\n",
    "    0.004,\n",
    "    0.001,\n",
    "    0.002,\n",
    "    0.002,\n",
    "    0.008,\n",
    "    0.02,\n",
    "    0.004,\n",
    "    0.006,\n",
    "    0.002,\n",
    "    0.02,\n",
    "    0.002,\n",
    "    0.002,\n",
    "    0.006,\n",
    "    0.004,\n",
    "    0.002,\n",
    "    0.001,\n",
    "    0.02,\n",
    "    0.006,\n",
    "    0.001,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.001,\n",
    "    0.002,\n",
    "    0.006,\n",
    "    0.006,\n",
    "    0.004,\n",
    "    0.006,\n",
    "    0.001,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.006,\n",
    "    0.006,\n",
    "    0.001,\n",
    "    0.04,\n",
    "    0.006,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.002,\n",
    "    0.002,\n",
    "    0.006,\n",
    "    0.002,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.006,\n",
    "    0.006,\n",
    "    0.002,\n",
    "    0.002,\n",
    "    0.008,\n",
    "    0.006,\n",
    "    0.004,\n",
    "    0.002,\n",
    "    0.006,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.006,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.001,\n",
    "    0.004,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.008,\n",
    "    0.006,\n",
    "    0.008,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.002,\n",
    "    0.001,\n",
    "    0.004,\n",
    "    0.004,\n",
    "    0.004,\n",
    "    0.006,\n",
    "    0.008,\n",
    "    0.004,\n",
    "    0.001,\n",
    "    0.001,\n",
    "    0.002,\n",
    "    0.006,\n",
    "    0.004,\n",
    "    0.001,\n",
    "    0.002,\n",
    "    0.006,\n",
    "    0.004,\n",
    "    0.006,\n",
    "    0.008,\n",
    "    0.002,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.002,\n",
    "    0.04,\n",
    "    0.002,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.002,\n",
    "    0.002,\n",
    "    0.006,\n",
    "    0.02,\n",
    "    0.004,\n",
    "    0.002,\n",
    "    0.006,\n",
    "    0.02,\n",
    "    0.001,\n",
    "    0.002,\n",
    "    0.006,\n",
    "    0.004,\n",
    "    0.006,\n",
    "    0.004,\n",
    "    0.004,\n",
    "    0.004,\n",
    "    0.004,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.04,\n",
    "    0.002,\n",
    "    0.008,\n",
    "    0.002,\n",
    "    0.004,\n",
    "    0.001,\n",
    "    0.004,\n",
    "    0.006,\n",
    "    0.004,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_df = pd.DataFrame(\n",
    "    data=list(zip(range(0, 201), weights)), columns=[\"stock_id\", \"index_weight\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(weights_df, on=\"stock_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"wap_calc\"] = (\n",
    "    train[\"bid_price\"] * train[\"ask_size\"] + train[\"ask_price\"] * train[\"bid_size\"]\n",
    ") / (train[\"ask_size\"] + train[\"bid_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['stock_id', 'date_id', 'seconds_in_bucket', 'imbalance_size',\n",
       "       'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n",
       "       'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n",
       "       'ask_size', 'wap', 'target', 'time_id', 'row_id', 'index_weight',\n",
       "       'wap_calc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prev_race(df_in, df_g, rolling_window=10, factor=\"\"):\n",
    "    df = df_in.copy()\n",
    "    original_cols = df_in.columns\n",
    "    df[f\"wap_t-60\"] = df_g[\"wap\"].shift(6)\n",
    "    df[f\"target_t-60\"] = df_g[\"target\"].shift(6)\n",
    "    df[f\"initial_wap\"] = df_g[\"wap_calc\"].transform(\"first\")\n",
    "    df[f\"initial_bid_size\"] = df_g[\"bid_size\"].transform(\"first\")\n",
    "    df[f\"initial_ask_size\"] = df_g[\"ask_size\"].transform(\"first\")\n",
    "    cols = [\"bid_price\", \"ask_price\", \"bid_size\", \"ask_size\", \"wap\"]\n",
    "    for i in cols:\n",
    "        df[f\"{i}_t-60\"] = df_g[i].shift(-6)\n",
    "    for i in cols:\n",
    "        df[f\"{i}_t10\"] = df_g[i].shift(1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_index(df_in, df_g, rolling_window=10, factor=\"\"):\n",
    "    df = df_in.copy()\n",
    "    df[f\"index_wap\"] = df_g[\"wap_weighted\"].transform(\"mean\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_index_2(df_in, df_g, rolling_window=10, factor=\"\"):\n",
    "    df = df_in.copy()\n",
    "    df[f\"index_wap_t-60\"] = df_g[\"index_wap\"].shift(6)\n",
    "    df[f\"index_wap_init\"] = df_g[\"index_wap\"].transform(\"first\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_index_3(df_in, df_g, rolling_window=10, factor=\"\"):\n",
    "    df = df_in.copy()\n",
    "    df[f\"index_wap_t-60\"] = df_g[\"index_wap_move_to_init\"].shift(6)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"wap_weighted\"] = train[\"wap\"] * train[\"index_weight\"]\n",
    "train_g = train.groupby([\"stock_id\", \"date_id\"])\n",
    "train = generate_prev_race(train, train_g)\n",
    "train[\"delta_wap\"] = train[\"wap\"] / train[\"wap_t-60\"]\n",
    "\n",
    "train_g = train.groupby([\"seconds_in_bucket\", \"date_id\"])\n",
    "train = generate_index(train, train_g)\n",
    "\n",
    "\n",
    "train[\"wap_move_to_init\"] = train[\"wap_calc\"] / train[\"initial_wap\"]\n",
    "train_g = train.groupby([\"date_id\"])\n",
    "train = generate_index_2(train, train_g)\n",
    "\n",
    "train[\"index_wap_move_to_init\"] = train[\"index_wap\"] / train[\"index_wap_init\"]\n",
    "train_g = train.groupby([\"date_id\"])\n",
    "train = generate_index_3(train, train_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['stock_id', 'date_id', 'seconds_in_bucket', 'imbalance_size',\n",
       "       'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n",
       "       'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n",
       "       'ask_size', 'wap', 'target', 'time_id', 'row_id', 'index_weight',\n",
       "       'wap_calc', 'wap_weighted', 'wap_t-60', 'target_t-60', 'initial_wap',\n",
       "       'initial_bid_size', 'initial_ask_size', 'bid_price_t-60',\n",
       "       'ask_price_t-60', 'bid_size_t-60', 'ask_size_t-60', 'bid_price_t10',\n",
       "       'ask_price_t10', 'bid_size_t10', 'ask_size_t10', 'wap_t10', 'delta_wap',\n",
       "       'index_wap', 'wap_move_to_init', 'index_wap_t-60', 'index_wap_init',\n",
       "       'index_wap_move_to_init'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"target_calc\"] = (\n",
    "    -(\n",
    "        (train[\"wap_t-60\"] / train[\"wap\"])\n",
    "        - (train[\"index_wap_t-60\"] / train[\"index_wap_move_to_init\"])\n",
    "    )\n",
    "    * 10000\n",
    ")\n",
    "train[\"target_delta\"] = train[\"target_t-60\"] - train[\"target_calc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>imbalance_size</th>\n",
       "      <th>imbalance_buy_sell_flag</th>\n",
       "      <th>reference_price</th>\n",
       "      <th>matched_size</th>\n",
       "      <th>far_price</th>\n",
       "      <th>near_price</th>\n",
       "      <th>bid_price</th>\n",
       "      <th>...</th>\n",
       "      <th>ask_size_t10</th>\n",
       "      <th>wap_t10</th>\n",
       "      <th>delta_wap</th>\n",
       "      <th>index_wap</th>\n",
       "      <th>wap_move_to_init</th>\n",
       "      <th>index_wap_t-60</th>\n",
       "      <th>index_wap_init</th>\n",
       "      <th>index_wap_move_to_init</th>\n",
       "      <th>target_calc</th>\n",
       "      <th>target_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3180602.69</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>13380276.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999483</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1299772.70</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000026</td>\n",
       "      <td>15261106.63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>...</td>\n",
       "      <td>8493.03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999471</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>0.999892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>1.000356</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1299772.70</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999919</td>\n",
       "      <td>15261106.63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>...</td>\n",
       "      <td>23519.16</td>\n",
       "      <td>0.999892</td>\n",
       "      <td>0.999694</td>\n",
       "      <td>0.005034</td>\n",
       "      <td>0.999842</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>1.000525</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1299772.70</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000133</td>\n",
       "      <td>15261106.63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000026</td>\n",
       "      <td>...</td>\n",
       "      <td>12131.60</td>\n",
       "      <td>0.999842</td>\n",
       "      <td>0.999659</td>\n",
       "      <td>0.005034</td>\n",
       "      <td>1.000085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>1.000547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1218204.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000455</td>\n",
       "      <td>15342674.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000241</td>\n",
       "      <td>...</td>\n",
       "      <td>46203.30</td>\n",
       "      <td>1.000085</td>\n",
       "      <td>1.000056</td>\n",
       "      <td>0.005035</td>\n",
       "      <td>1.000317</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>1.000635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1218204.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000455</td>\n",
       "      <td>15342674.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000348</td>\n",
       "      <td>...</td>\n",
       "      <td>26610.45</td>\n",
       "      <td>1.000317</td>\n",
       "      <td>1.000392</td>\n",
       "      <td>0.005035</td>\n",
       "      <td>1.000435</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>1.000668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1218204.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000562</td>\n",
       "      <td>15342674.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000455</td>\n",
       "      <td>...</td>\n",
       "      <td>9897.22</td>\n",
       "      <td>1.000434</td>\n",
       "      <td>1.000622</td>\n",
       "      <td>0.005036</td>\n",
       "      <td>1.000517</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>1.000815</td>\n",
       "      <td>-1.930193</td>\n",
       "      <td>-1.099511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>1264494.89</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000455</td>\n",
       "      <td>15352380.96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000348</td>\n",
       "      <td>...</td>\n",
       "      <td>10085.04</td>\n",
       "      <td>1.000517</td>\n",
       "      <td>1.000459</td>\n",
       "      <td>0.005036</td>\n",
       "      <td>1.000422</td>\n",
       "      <td>1.000356</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>1.000843</td>\n",
       "      <td>-0.279044</td>\n",
       "      <td>0.668858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>1189832.86</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000241</td>\n",
       "      <td>15427043.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000133</td>\n",
       "      <td>...</td>\n",
       "      <td>17366.82</td>\n",
       "      <td>1.000421</td>\n",
       "      <td>1.000322</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>1.000148</td>\n",
       "      <td>1.000525</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>1.000409</td>\n",
       "      <td>4.376209</td>\n",
       "      <td>-0.156200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>1189272.89</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000562</td>\n",
       "      <td>15427602.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000348</td>\n",
       "      <td>...</td>\n",
       "      <td>61984.40</td>\n",
       "      <td>1.000148</td>\n",
       "      <td>1.000639</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>1.000427</td>\n",
       "      <td>1.000547</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>1.000345</td>\n",
       "      <td>8.409505</td>\n",
       "      <td>-2.959257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>1249282.50</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000348</td>\n",
       "      <td>15427602.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000241</td>\n",
       "      <td>...</td>\n",
       "      <td>40433.54</td>\n",
       "      <td>1.000426</td>\n",
       "      <td>1.000666</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>1.000261</td>\n",
       "      <td>1.000635</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>1.000264</td>\n",
       "      <td>10.363349</td>\n",
       "      <td>-7.193574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>1277280.77</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000133</td>\n",
       "      <td>15399604.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000026</td>\n",
       "      <td>...</td>\n",
       "      <td>42572.16</td>\n",
       "      <td>1.000261</td>\n",
       "      <td>1.000548</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>1.000042</td>\n",
       "      <td>1.000668</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>1.000218</td>\n",
       "      <td>9.973232</td>\n",
       "      <td>-9.373609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>1216057.90</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000133</td>\n",
       "      <td>15460827.57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>...</td>\n",
       "      <td>28375.36</td>\n",
       "      <td>1.000042</td>\n",
       "      <td>1.000226</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>1.000815</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>1.000217</td>\n",
       "      <td>8.245111</td>\n",
       "      <td>-8.445382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>1216057.90</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000026</td>\n",
       "      <td>15460827.57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>...</td>\n",
       "      <td>68224.23</td>\n",
       "      <td>0.999895</td>\n",
       "      <td>1.000207</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.000843</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>1.000147</td>\n",
       "      <td>9.028563</td>\n",
       "      <td>-6.618152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>1104904.79</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999919</td>\n",
       "      <td>15571980.68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999705</td>\n",
       "      <td>...</td>\n",
       "      <td>13999.50</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.000533</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>0.999827</td>\n",
       "      <td>1.000409</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>1.000129</td>\n",
       "      <td>8.134313</td>\n",
       "      <td>-8.524127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>1085679.32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>15591206.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999705</td>\n",
       "      <td>...</td>\n",
       "      <td>25196.40</td>\n",
       "      <td>0.999826</td>\n",
       "      <td>1.000556</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>0.999787</td>\n",
       "      <td>1.000345</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>1.000142</td>\n",
       "      <td>7.595631</td>\n",
       "      <td>-11.935445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>1085679.32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999598</td>\n",
       "      <td>15591206.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999491</td>\n",
       "      <td>...</td>\n",
       "      <td>15769.39</td>\n",
       "      <td>0.999787</td>\n",
       "      <td>1.000384</td>\n",
       "      <td>0.005030</td>\n",
       "      <td>0.999596</td>\n",
       "      <td>1.000264</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.999807</td>\n",
       "      <td>8.417362</td>\n",
       "      <td>-10.467165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>1085679.32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999598</td>\n",
       "      <td>15591206.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999383</td>\n",
       "      <td>...</td>\n",
       "      <td>186.58</td>\n",
       "      <td>0.999595</td>\n",
       "      <td>1.000253</td>\n",
       "      <td>0.005030</td>\n",
       "      <td>0.999494</td>\n",
       "      <td>1.000218</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>8.001506</td>\n",
       "      <td>-7.981241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>1445736.98</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999705</td>\n",
       "      <td>15642349.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999598</td>\n",
       "      <td>...</td>\n",
       "      <td>28173.58</td>\n",
       "      <td>0.999494</td>\n",
       "      <td>1.000071</td>\n",
       "      <td>0.005030</td>\n",
       "      <td>0.999670</td>\n",
       "      <td>1.000217</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.999764</td>\n",
       "      <td>5.236943</td>\n",
       "      <td>-2.946932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>1771730.09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>15642349.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999705</td>\n",
       "      <td>...</td>\n",
       "      <td>9330.00</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>1.000446</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.999755</td>\n",
       "      <td>1.000147</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.999842</td>\n",
       "      <td>7.512073</td>\n",
       "      <td>-6.511907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n",
       "0          0        0                  0      3180602.69   \n",
       "1          0        0                 10      1299772.70   \n",
       "2          0        0                 20      1299772.70   \n",
       "3          0        0                 30      1299772.70   \n",
       "4          0        0                 40      1218204.43   \n",
       "5          0        0                 50      1218204.43   \n",
       "6          0        0                 60      1218204.43   \n",
       "7          0        0                 70      1264494.89   \n",
       "8          0        0                 80      1189832.86   \n",
       "9          0        0                 90      1189272.89   \n",
       "10         0        0                100      1249282.50   \n",
       "11         0        0                110      1277280.77   \n",
       "12         0        0                120      1216057.90   \n",
       "13         0        0                130      1216057.90   \n",
       "14         0        0                140      1104904.79   \n",
       "15         0        0                150      1085679.32   \n",
       "16         0        0                160      1085679.32   \n",
       "17         0        0                170      1085679.32   \n",
       "18         0        0                180      1445736.98   \n",
       "19         0        0                190      1771730.09   \n",
       "\n",
       "    imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n",
       "0                         1         0.999812   13380276.64        NaN   \n",
       "1                         1         1.000026   15261106.63        NaN   \n",
       "2                         1         0.999919   15261106.63        NaN   \n",
       "3                         1         1.000133   15261106.63        NaN   \n",
       "4                         1         1.000455   15342674.90        NaN   \n",
       "5                         1         1.000455   15342674.90        NaN   \n",
       "6                         1         1.000562   15342674.90        NaN   \n",
       "7                         1         1.000455   15352380.96        NaN   \n",
       "8                         1         1.000241   15427043.00        NaN   \n",
       "9                         1         1.000562   15427602.97        NaN   \n",
       "10                        1         1.000348   15427602.97        NaN   \n",
       "11                        1         1.000133   15399604.70        NaN   \n",
       "12                        1         1.000133   15460827.57        NaN   \n",
       "13                        1         1.000026   15460827.57        NaN   \n",
       "14                        1         0.999919   15571980.68        NaN   \n",
       "15                        1         0.999812   15591206.15        NaN   \n",
       "16                        1         0.999598   15591206.15        NaN   \n",
       "17                        1         0.999598   15591206.15        NaN   \n",
       "18                        1         0.999705   15642349.64        NaN   \n",
       "19                        1         0.999812   15642349.64        NaN   \n",
       "\n",
       "    near_price  bid_price  ...  ask_size_t10   wap_t10  delta_wap  index_wap  \\\n",
       "0          NaN   0.999812  ...           NaN       NaN   0.999483   0.005031   \n",
       "1          NaN   0.999812  ...       8493.03  1.000000   0.999471   0.005033   \n",
       "2          NaN   0.999812  ...      23519.16  0.999892   0.999694   0.005034   \n",
       "3          NaN   1.000026  ...      12131.60  0.999842   0.999659   0.005034   \n",
       "4          NaN   1.000241  ...      46203.30  1.000085   1.000056   0.005035   \n",
       "5          NaN   1.000348  ...      26610.45  1.000317   1.000392   0.005035   \n",
       "6          NaN   1.000455  ...       9897.22  1.000434   1.000622   0.005036   \n",
       "7          NaN   1.000348  ...      10085.04  1.000517   1.000459   0.005036   \n",
       "8          NaN   1.000133  ...      17366.82  1.000421   1.000322   0.005033   \n",
       "9          NaN   1.000348  ...      61984.40  1.000148   1.000639   0.005033   \n",
       "10         NaN   1.000241  ...      40433.54  1.000426   1.000666   0.005033   \n",
       "11         NaN   1.000026  ...      42572.16  1.000261   1.000548   0.005033   \n",
       "12         NaN   0.999812  ...      28375.36  1.000042   1.000226   0.005033   \n",
       "13         NaN   0.999812  ...      68224.23  0.999895   1.000207   0.005032   \n",
       "14         NaN   0.999705  ...      13999.50  0.999962   1.000533   0.005032   \n",
       "15         NaN   0.999705  ...      25196.40  0.999826   1.000556   0.005032   \n",
       "16         NaN   0.999491  ...      15769.39  0.999787   1.000384   0.005030   \n",
       "17         NaN   0.999383  ...        186.58  0.999595   1.000253   0.005030   \n",
       "18         NaN   0.999598  ...      28173.58  0.999494   1.000071   0.005030   \n",
       "19         NaN   0.999705  ...       9330.00  0.999669   1.000446   0.005031   \n",
       "\n",
       "    wap_move_to_init  index_wap_t-60 index_wap_init  index_wap_move_to_init  \\\n",
       "0           1.000000             NaN       0.005031                1.000000   \n",
       "1           0.999892             NaN       0.005031                1.000356   \n",
       "2           0.999842             NaN       0.005031                1.000525   \n",
       "3           1.000085             NaN       0.005031                1.000547   \n",
       "4           1.000317             NaN       0.005031                1.000635   \n",
       "5           1.000435             NaN       0.005031                1.000668   \n",
       "6           1.000517        1.000000       0.005031                1.000815   \n",
       "7           1.000422        1.000356       0.005031                1.000843   \n",
       "8           1.000148        1.000525       0.005031                1.000409   \n",
       "9           1.000427        1.000547       0.005031                1.000345   \n",
       "10          1.000261        1.000635       0.005031                1.000264   \n",
       "11          1.000042        1.000668       0.005031                1.000218   \n",
       "12          0.999896        1.000815       0.005031                1.000217   \n",
       "13          0.999962        1.000843       0.005031                1.000147   \n",
       "14          0.999827        1.000409       0.005031                1.000129   \n",
       "15          0.999787        1.000345       0.005031                1.000142   \n",
       "16          0.999596        1.000264       0.005031                0.999807   \n",
       "17          0.999494        1.000218       0.005031                0.999672   \n",
       "18          0.999670        1.000217       0.005031                0.999764   \n",
       "19          0.999755        1.000147       0.005031                0.999842   \n",
       "\n",
       "    target_calc  target_delta  \n",
       "0           NaN           NaN  \n",
       "1           NaN           NaN  \n",
       "2           NaN           NaN  \n",
       "3           NaN           NaN  \n",
       "4           NaN           NaN  \n",
       "5           NaN           NaN  \n",
       "6     -1.930193     -1.099511  \n",
       "7     -0.279044      0.668858  \n",
       "8      4.376209     -0.156200  \n",
       "9      8.409505     -2.959257  \n",
       "10    10.363349     -7.193574  \n",
       "11     9.973232     -9.373609  \n",
       "12     8.245111     -8.445382  \n",
       "13     9.028563     -6.618152  \n",
       "14     8.134313     -8.524127  \n",
       "15     7.595631    -11.935445  \n",
       "16     8.417362    -10.467165  \n",
       "17     8.001506     -7.981241  \n",
       "18     5.236943     -2.946932  \n",
       "19     7.512073     -6.511907  \n",
       "\n",
       "[20 rows x 42 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stock_0 = train[train[\"stock_id\"] == 0].dropna(subset=\"bid_size_t-60\").copy()\n",
    "train_stock_0.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seconds_in_bucket                0\n",
       "imbalance_size                 220\n",
       "imbalance_buy_sell_flag          0\n",
       "reference_price                220\n",
       "matched_size                   220\n",
       "far_price                  2894342\n",
       "near_price                 2857180\n",
       "bid_price                      220\n",
       "bid_size                         0\n",
       "ask_price                      220\n",
       "ask_size                         0\n",
       "wap                            220\n",
       "index_weight                     0\n",
       "wap_calc                       220\n",
       "initial_wap                    220\n",
       "wap_weighted                   220\n",
       "index_wap                        0\n",
       "index_wap_init                   0\n",
       "index_wap_move_to_init           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\n",
    "    [\n",
    "        \"seconds_in_bucket\",\n",
    "        \"imbalance_size\",\n",
    "        \"imbalance_buy_sell_flag\",\n",
    "        \"reference_price\",\n",
    "        \"matched_size\",\n",
    "        \"far_price\",\n",
    "        \"near_price\",\n",
    "        \"bid_price\",\n",
    "        \"bid_size\",\n",
    "        \"ask_price\",\n",
    "        \"ask_size\",\n",
    "        \"wap\",\n",
    "        \"index_weight\",\n",
    "        \"wap_calc\",\n",
    "        \"initial_wap\",\n",
    "        \"wap_weighted\",\n",
    "        \"index_wap\",\n",
    "        \"index_wap_init\",\n",
    "        \"index_wap_move_to_init\",\n",
    "    ]\n",
    "].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_stock_0.to_csv('train_with_new_vars_0stock.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_vol = pd.read_csv(\"archive/MedianVolV2.csv\")\n",
    "median_vol.index.name = \"stock_id\"\n",
    "median_vol = median_vol[[\"overall_medvol\", \"first5min_medvol\", \"last5min_medvol\"]]\n",
    "median_sizes = (\n",
    "    train.groupby(\"stock_id\")[\"bid_size\"].median()\n",
    "    + train.groupby(\"stock_id\")[\"ask_size\"].median()\n",
    ")\n",
    "std_sizes = (\n",
    "    train.groupby(\"stock_id\")[\"bid_size\"].median()\n",
    "    + train.groupby(\"stock_id\")[\"ask_size\"].median()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"bid_price_target\"] = train[\"bid_price\"] - train[\"bid_price_t-60\"]\n",
    "train[\"bid_price_t-60\"] = train[\"bid_price_target\"] * 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"wap_target\"] = train[\"wap\"] - train[\"wap_t-60\"]\n",
    "train[\"wap_price_t-60\"] = train[\"wap_target\"] * 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"wap\", \"bid_price\", \"ask_price\"]\n",
    "for i in targets:\n",
    "    train[f\"{i}_prev_move\"] = (train[f\"{i}\"] - train[f\"{i}_t10\"]).fillna(0) * 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"ask_price_target\"] = train[\"ask_price\"] - train[\"ask_price_t-60\"]\n",
    "train[\"ask_price_t-60\"] = train[\"ask_price_target\"] * 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bid_price_t-60</th>\n",
       "      <th>bid_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.43</td>\n",
       "      <td>0.999812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5.36</td>\n",
       "      <td>0.999812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.21</td>\n",
       "      <td>0.999812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.22</td>\n",
       "      <td>1.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.22</td>\n",
       "      <td>1.000348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.43</td>\n",
       "      <td>1.000455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.36</td>\n",
       "      <td>1.000348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.28</td>\n",
       "      <td>1.000133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.43</td>\n",
       "      <td>1.000348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bid_price_t-60  bid_price\n",
       "0           -6.43   0.999812\n",
       "1           -5.36   0.999812\n",
       "2           -3.21   0.999812\n",
       "3           -3.22   1.000026\n",
       "4            0.00   1.000241\n",
       "5            3.22   1.000348\n",
       "6            6.43   1.000455\n",
       "7            5.36   1.000348\n",
       "8            4.28   1.000133\n",
       "9            6.43   1.000348"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[[\"bid_price_t-60\", \"bid_price\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_eng(df):\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\"]]\n",
    "    df = df[cols]\n",
    "    df = df.merge(median_vol, how=\"left\", left_on=\"stock_id\", right_index=True)\n",
    "\n",
    "    df[\"bid_plus_ask_sizes\"] = df[\"bid_size\"] + train[\"ask_size\"]\n",
    "    #     df['median_size'] = df['stock_id'].map(median_sizes.to_dict())\n",
    "    df[\"std_size\"] = df[\"stock_id\"].map(std_sizes.to_dict())\n",
    "    #     df['high_volume'] = np.where(df['bid_plus_ask_sizes'] > df['median_size'], 1, 0)\n",
    "    df[\"imbalance_ratio\"] = df[\"imbalance_size\"] / df[\"matched_size\"]\n",
    "\n",
    "    df[\"imb_s1\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"imb_s2\"] = df.eval(\n",
    "        \"(imbalance_size-matched_size)/(matched_size+imbalance_size)\"\n",
    "    )\n",
    "\n",
    "    df[\"ask_x_size\"] = df.eval(\"ask_size*ask_price\")\n",
    "    df[\"bid_x_size\"] = df.eval(\"bid_size*bid_price\")\n",
    "\n",
    "    df[\"ask_minus_bid\"] = df[\"ask_x_size\"] - df[\"bid_x_size\"]\n",
    "\n",
    "    df[\"bid_size_over_ask_size\"] = df[\"bid_size\"].div(df[\"ask_size\"])\n",
    "    df[\"bid_price_over_ask_price\"] = df[\"bid_price\"].div(df[\"ask_price\"])\n",
    "\n",
    "    prices = [\n",
    "        \"reference_price\",\n",
    "        \"far_price\",\n",
    "        \"near_price\",\n",
    "        \"ask_price\",\n",
    "        \"bid_price\",\n",
    "        \"wap\",\n",
    "    ]\n",
    "\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_minus_{c[1]}\"] = (df[f\"{c[0]}\"] - df[f\"{c[1]}\"]).astype(np.float32)\n",
    "        df[f\"{c[0]}_times_{c[1]}\"] = (df[f\"{c[0]}\"] * df[f\"{c[1]}\"]).astype(np.float32)\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]}-{c[1]})/({c[0]}+{c[1]})\")\n",
    "\n",
    "    for c in combinations(prices, 3):\n",
    "        max_ = df[list(c)].max(axis=1)\n",
    "        min_ = df[list(c)].min(axis=1)\n",
    "        mid_ = df[list(c)].sum(axis=1) - min_ - max_\n",
    "\n",
    "        df[f\"{c[0]}_{c[1]}_{c[2]}_imb2\"] = (max_ - mid_) / (mid_ - min_)\n",
    "\n",
    "    df.drop(\n",
    "        columns=[\n",
    "            # 'date_id',\n",
    "            \"reference_price_far_price_imb\",\n",
    "            \"reference_price_minus_near_price\",\n",
    "            \"reference_price_near_price_imb\",\n",
    "            \"far_price_near_price_imb\",\n",
    "            \"far_price_ask_price_imb\",\n",
    "            \"far_price_bid_price_imb\",\n",
    "            \"far_price_minus_wap\",\n",
    "            \"std_size\",\n",
    "            \"bid_size_over_ask_size\",\n",
    "            \"ask_price_bid_price_imb\",\n",
    "            \"near_price_times_wap\",\n",
    "        ],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # gc.collect()\n",
    "\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reference_price', 'far_price', 'near_price', 'bid_price', 'ask_price', 'bid_price_t10', 'ask_price_t10', 'bid_price_prev_move', 'ask_price_prev_move', 'bid_price_over_ask_price', 'reference_price_minus_far_price', 'reference_price_times_far_price', 'reference_price_times_near_price', 'reference_price_minus_ask_price', 'reference_price_times_ask_price', 'reference_price_ask_price_imb', 'reference_price_minus_bid_price', 'reference_price_times_bid_price', 'reference_price_bid_price_imb', 'reference_price_minus_wap', 'reference_price_times_wap', 'reference_price_wap_imb', 'far_price_minus_near_price', 'far_price_times_near_price', 'far_price_minus_ask_price', 'far_price_times_ask_price', 'far_price_minus_bid_price', 'far_price_times_bid_price', 'far_price_times_wap', 'far_price_wap_imb', 'near_price_minus_ask_price', 'near_price_times_ask_price', 'near_price_ask_price_imb', 'near_price_minus_bid_price', 'near_price_times_bid_price', 'near_price_bid_price_imb', 'near_price_minus_wap', 'near_price_wap_imb', 'ask_price_minus_bid_price', 'ask_price_times_bid_price', 'ask_price_minus_wap', 'ask_price_times_wap', 'ask_price_wap_imb', 'bid_price_minus_wap', 'bid_price_times_wap', 'bid_price_wap_imb', 'reference_price_far_price_near_price_imb2', 'reference_price_far_price_ask_price_imb2', 'reference_price_far_price_bid_price_imb2', 'reference_price_far_price_wap_imb2', 'reference_price_near_price_ask_price_imb2', 'reference_price_near_price_bid_price_imb2', 'reference_price_near_price_wap_imb2', 'reference_price_ask_price_bid_price_imb2', 'reference_price_ask_price_wap_imb2', 'reference_price_bid_price_wap_imb2', 'far_price_near_price_ask_price_imb2', 'far_price_near_price_bid_price_imb2', 'far_price_near_price_wap_imb2', 'far_price_ask_price_bid_price_imb2', 'far_price_ask_price_wap_imb2', 'far_price_bid_price_wap_imb2', 'near_price_ask_price_bid_price_imb2', 'near_price_ask_price_wap_imb2', 'near_price_bid_price_wap_imb2', 'ask_price_bid_price_wap_imb2']\n"
     ]
    }
   ],
   "source": [
    "y = train[\"target\"].values\n",
    "X = feat_eng(train)\n",
    "prices = [\n",
    "    c for c in X.columns if (\"price\" in c) and (\"target\" not in c) and (\"60\" not in c)\n",
    "]\n",
    "print(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = [\n",
    "    c for c in X.columns if (\"price\" in c) and (\"target\" not in c) and (\"60\" not in c)\n",
    "]\n",
    "# prices = [c for c in train.columns if 'price' in c]\n",
    "pca_prices = PCA(n_components=1)\n",
    "X[\"pca_prices\"] = pca_prices.fit_transform(X[prices].fillna(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stock_id                                 int64\n",
       "date_id                                  int64\n",
       "seconds_in_bucket                        int64\n",
       "imbalance_size                         float64\n",
       "imbalance_buy_sell_flag                  int64\n",
       "                                        ...   \n",
       "near_price_ask_price_bid_price_imb2    float64\n",
       "near_price_ask_price_wap_imb2          float64\n",
       "near_price_bid_price_wap_imb2          float64\n",
       "ask_price_bid_price_wap_imb2           float64\n",
       "pca_prices                             float64\n",
       "Length: 115, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = lgb.Booster(model_file=\"data/lgbm_model_new_t60.lgb\")\n",
    "X_train = X[[c for c in X.columns if (\"target\" not in c) and (\"60\" not in c)]].drop(\n",
    "    columns=[\"delta_wap\", \"date_id\"]\n",
    ")\n",
    "lgbm_preds = lgbm.predict(X_train)\n",
    "X[\"lgbm_preds\"] = lgbm_preds\n",
    "\n",
    "del pca_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stock_id',\n",
       " 'seconds_in_bucket',\n",
       " 'imbalance_size',\n",
       " 'imbalance_buy_sell_flag',\n",
       " 'reference_price',\n",
       " 'matched_size',\n",
       " 'far_price',\n",
       " 'near_price',\n",
       " 'bid_price',\n",
       " 'bid_size',\n",
       " 'ask_price',\n",
       " 'ask_size',\n",
       " 'wap',\n",
       " 'index_weight',\n",
       " 'wap_calc',\n",
       " 'wap_weighted',\n",
       " 'initial_wap',\n",
       " 'initial_bid_size',\n",
       " 'initial_ask_size',\n",
       " 'bid_price_t10',\n",
       " 'ask_price_t10',\n",
       " 'bid_size_t10',\n",
       " 'ask_size_t10',\n",
       " 'wap_t10',\n",
       " 'index_wap',\n",
       " 'wap_move_to_init',\n",
       " 'index_wap_init',\n",
       " 'index_wap_move_to_init',\n",
       " 'wap_prev_move',\n",
       " 'bid_price_prev_move',\n",
       " 'ask_price_prev_move',\n",
       " 'overall_medvol',\n",
       " 'first5min_medvol',\n",
       " 'last5min_medvol',\n",
       " 'bid_plus_ask_sizes',\n",
       " 'imbalance_ratio',\n",
       " 'imb_s1',\n",
       " 'imb_s2',\n",
       " 'ask_x_size',\n",
       " 'bid_x_size',\n",
       " 'ask_minus_bid',\n",
       " 'bid_price_over_ask_price',\n",
       " 'reference_price_minus_far_price',\n",
       " 'reference_price_times_far_price',\n",
       " 'reference_price_times_near_price',\n",
       " 'reference_price_minus_ask_price',\n",
       " 'reference_price_times_ask_price',\n",
       " 'reference_price_ask_price_imb',\n",
       " 'reference_price_minus_bid_price',\n",
       " 'reference_price_times_bid_price',\n",
       " 'reference_price_bid_price_imb',\n",
       " 'reference_price_minus_wap',\n",
       " 'reference_price_times_wap',\n",
       " 'reference_price_wap_imb',\n",
       " 'far_price_minus_near_price',\n",
       " 'far_price_times_near_price',\n",
       " 'far_price_minus_ask_price',\n",
       " 'far_price_times_ask_price',\n",
       " 'far_price_minus_bid_price',\n",
       " 'far_price_times_bid_price',\n",
       " 'far_price_times_wap',\n",
       " 'far_price_wap_imb',\n",
       " 'near_price_minus_ask_price',\n",
       " 'near_price_times_ask_price',\n",
       " 'near_price_ask_price_imb',\n",
       " 'near_price_minus_bid_price',\n",
       " 'near_price_times_bid_price',\n",
       " 'near_price_bid_price_imb',\n",
       " 'near_price_minus_wap',\n",
       " 'near_price_wap_imb',\n",
       " 'ask_price_minus_bid_price',\n",
       " 'ask_price_times_bid_price',\n",
       " 'ask_price_minus_wap',\n",
       " 'ask_price_times_wap',\n",
       " 'ask_price_wap_imb',\n",
       " 'bid_price_minus_wap',\n",
       " 'bid_price_times_wap',\n",
       " 'bid_price_wap_imb',\n",
       " 'reference_price_far_price_near_price_imb2',\n",
       " 'reference_price_far_price_ask_price_imb2',\n",
       " 'reference_price_far_price_bid_price_imb2',\n",
       " 'reference_price_far_price_wap_imb2',\n",
       " 'reference_price_near_price_ask_price_imb2',\n",
       " 'reference_price_near_price_bid_price_imb2',\n",
       " 'reference_price_near_price_wap_imb2',\n",
       " 'reference_price_ask_price_bid_price_imb2',\n",
       " 'reference_price_ask_price_wap_imb2',\n",
       " 'reference_price_bid_price_wap_imb2',\n",
       " 'far_price_near_price_ask_price_imb2',\n",
       " 'far_price_near_price_bid_price_imb2',\n",
       " 'far_price_near_price_wap_imb2',\n",
       " 'far_price_ask_price_bid_price_imb2',\n",
       " 'far_price_ask_price_wap_imb2',\n",
       " 'far_price_bid_price_wap_imb2',\n",
       " 'near_price_ask_price_bid_price_imb2',\n",
       " 'near_price_ask_price_wap_imb2',\n",
       " 'near_price_bid_price_wap_imb2',\n",
       " 'ask_price_bid_price_wap_imb2',\n",
       " 'pca_prices']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm.feature_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.join(pca)\n",
    "X = X.dropna(subset=\"wap_t-60\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"stats\"] = np.split(\n",
    "    np.nan_to_num(X[stat_cols].to_numpy(), nan=-1), indices_or_sections=len(X)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.189441833645105"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(X) / (1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pca_prices\n",
       "-1.384228e+11    408262\n",
       "-1.384228e+11    382449\n",
       "-1.384228e+11    119352\n",
       "-1.384228e+11    112550\n",
       "-1.384228e+11    102265\n",
       "-1.384228e+11     94369\n",
       "-1.384228e+11     91487\n",
       "-1.384228e+11     89059\n",
       "-1.384228e+11     79446\n",
       "-1.384228e+11     70583\n",
       "-1.384228e+11     63254\n",
       "-1.384228e+11     57635\n",
       "-1.384228e+11     56887\n",
       "-1.384228e+11     50057\n",
       "-1.384228e+11     44411\n",
       "-1.384228e+11     39646\n",
       "-1.384228e+11     35161\n",
       "-1.384228e+11     31039\n",
       "-1.384228e+11     27870\n",
       "-1.384228e+11     24789\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.pca_prices.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"wap_category\"] = X[\"wap_price_t-60\"].apply(\n",
    "    lambda x: 0\n",
    "    if x < -10\n",
    "    else (\n",
    "        1\n",
    "        if x < -5\n",
    "        else (\n",
    "            2\n",
    "            if x < -1.5\n",
    "            else (3 if x < 1.5 else (4 if x < 5 else (5 if x < 10 else 6)))\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wap_category</th>\n",
       "      <th>wap_price_t-60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-5.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-5.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-3.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>-3.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4666363</th>\n",
       "      <td>6</td>\n",
       "      <td>10.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4666364</th>\n",
       "      <td>4</td>\n",
       "      <td>4.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4666365</th>\n",
       "      <td>5</td>\n",
       "      <td>5.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4666366</th>\n",
       "      <td>2</td>\n",
       "      <td>-2.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4666367</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4666368 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         wap_category  wap_price_t-60\n",
       "0                   1           -5.17\n",
       "1                   1           -5.29\n",
       "2                   2           -3.06\n",
       "3                   2           -3.41\n",
       "4                   3            0.56\n",
       "...               ...             ...\n",
       "4666363             6           10.05\n",
       "4666364             4            4.36\n",
       "4666365             5            5.80\n",
       "4666366             2           -2.66\n",
       "4666367             3           -0.76\n",
       "\n",
       "[4666368 rows x 2 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[[\"wap_category\", \"wap_price_t-60\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [0.0, 3180602.69, 1.0, 0.999812, 13380276.64, ...\n",
      "1    [10.0, 1299772.7, 1.0, 1.000026, 15261106.63, ...\n",
      "2    [20.0, 1299772.7, 1.0, 0.999919, 15261106.63, ...\n",
      "3    [30.0, 1299772.7, 1.0, 1.000133, 15261106.63, ...\n",
      "4    [40.0, 1218204.43, 1.0, 1.000455, 15342674.9, ...\n",
      "5    [50.0, 1218204.43, 1.0, 1.000455, 15342674.9, ...\n",
      "6    [60.0, 1218204.43, 1.0, 1.000562, 15342674.9, ...\n",
      "7    [70.0, 1264494.89, 1.0, 1.000455, 15352380.96,...\n",
      "8    [80.0, 1189832.86, 1.0, 1.000241, 15427043.0, ...\n",
      "9    [90.0, 1189272.89, 1.0, 1.000562, 15427602.97,...\n",
      "Name: stats, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# X[\"stats\"] = X[\"stats\"].apply(lambda x: x.reshape(-1))\n",
    "# print(X[\"stats\"].head(10))\n",
    "# X.to_feather('train_data_with_features.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_feather('train_data_with_features.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"wap_category\"] = pd.qcut(X[\"wap_price_t-60\"], q=5)\n",
    "X[\"target_category\"] = pd.qcut(X[\"target\"], q=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = (X[\"wap_category\"].value_counts(sort=False).reset_index().sort_values(\"wap_category\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8000, 0.7998, 0.8000, 0.8001, 0.8001], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[\"norm_count\"] = 1 - (weights[\"count\"] / weights[\"count\"].sum())\n",
    "weights\n",
    "weight = torch.tensor(weights[\"norm_count\"].to_numpy(), device=\"cuda:0\")\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\AppData\\Local\\Temp\\ipykernel_21744\\340863963.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  means_target = X.groupby('target_category')['target'].median().reset_index().reset_index(names='original_index').rename(columns={'target':'mean_target'})\n",
      "C:\\Users\\Nick\\AppData\\Local\\Temp\\ipykernel_21744\\340863963.py:3: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  means = X.groupby('wap_category')['wap_price_t-60'].median().reset_index().reset_index(names='original_index').rename(columns={'wap_price_t-60':'mean_wap'})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_index</th>\n",
       "      <th>wap_category</th>\n",
       "      <th>mean_wap</th>\n",
       "      <th>wap_cat_name</th>\n",
       "      <th>target_category</th>\n",
       "      <th>mean_target</th>\n",
       "      <th>target_cat_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(-379.691, -6.98]</td>\n",
       "      <td>-11.92</td>\n",
       "      <td>0(-379.691, -6.98]</td>\n",
       "      <td>(-385.291, -5.9]</td>\n",
       "      <td>-9.940267</td>\n",
       "      <td>0(-385.291, -5.9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>(-6.98, -1.79]</td>\n",
       "      <td>-4.02</td>\n",
       "      <td>1(-6.98, -1.79]</td>\n",
       "      <td>(-5.9, -1.67]</td>\n",
       "      <td>-3.499985</td>\n",
       "      <td>1(-5.9, -1.67]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>(-1.79, 1.82]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2(-1.79, 1.82]</td>\n",
       "      <td>(-1.67, 1.56]</td>\n",
       "      <td>-0.050068</td>\n",
       "      <td>2(-1.67, 1.56]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>(1.82, 7.05]</td>\n",
       "      <td>4.08</td>\n",
       "      <td>3(1.82, 7.05]</td>\n",
       "      <td>(1.56, 5.76]</td>\n",
       "      <td>3.390312</td>\n",
       "      <td>3(1.56, 5.76]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>(7.05, 392.99]</td>\n",
       "      <td>11.96</td>\n",
       "      <td>4(7.05, 392.99]</td>\n",
       "      <td>(5.76, 387.779]</td>\n",
       "      <td>9.800196</td>\n",
       "      <td>4(5.76, 387.779]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_index       wap_category  mean_wap        wap_cat_name  \\\n",
       "0               0  (-379.691, -6.98]    -11.92  0(-379.691, -6.98]   \n",
       "1               1     (-6.98, -1.79]     -4.02     1(-6.98, -1.79]   \n",
       "2               2      (-1.79, 1.82]      0.01      2(-1.79, 1.82]   \n",
       "3               3       (1.82, 7.05]      4.08       3(1.82, 7.05]   \n",
       "4               4     (7.05, 392.99]     11.96     4(7.05, 392.99]   \n",
       "\n",
       "    target_category  mean_target    target_cat_name  \n",
       "0  (-385.291, -5.9]    -9.940267  0(-385.291, -5.9]  \n",
       "1     (-5.9, -1.67]    -3.499985     1(-5.9, -1.67]  \n",
       "2     (-1.67, 1.56]    -0.050068     2(-1.67, 1.56]  \n",
       "3      (1.56, 5.76]     3.390312      3(1.56, 5.76]  \n",
       "4   (5.76, 387.779]     9.800196   4(5.76, 387.779]  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means_target = X.groupby('target_category')['target'].median().reset_index().reset_index(names='original_index').rename(columns={'target':'mean_target'})\n",
    "means_target['target_cat_name'] = means_target['original_index'].astype(str)+means_target['target_category'].astype(str)\n",
    "means = X.groupby('wap_category')['wap_price_t-60'].median().reset_index().reset_index(names='original_index').rename(columns={'wap_price_t-60':'mean_wap'})\n",
    "means['wap_cat_name'] = means['original_index'].astype(str)+means['wap_category'].astype(str)\n",
    "means = means.merge(means_target,on='original_index')\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "ohe_out = ohe.fit_transform(\n",
    "    X[\"wap_category\"].to_numpy().reshape(-1, 1),\n",
    ")\n",
    "X[\"wap_target_OHE\"] = [x for x in ohe_out]\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "ohe_out = ohe.fit_transform(\n",
    "    X[\"target_category\"].to_numpy().reshape(-1, 1),\n",
    ")\n",
    "X[\"target_OHE\"] = [x for x in ohe_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[\"target_OHE\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             one_hot_encoded                             blurred_encoded\n",
      "0  [0.0, 1.0, 0.0, 0.0, 0.0]   [0.2, 0.8, 0.2, 0.1, 0.06666666666666667]\n",
      "1  [1.0, 0.0, 0.0, 0.0, 0.0]  [0.8, 0.2, 0.1, 0.06666666666666667, 0.05]\n",
      "2  [0.0, 0.0, 0.0, 1.0, 0.0]   [0.06666666666666667, 0.1, 0.2, 0.8, 0.2]\n",
      "3  [0.0, 0.0, 1.0, 0.0, 0.0]                   [0.1, 0.2, 0.8, 0.2, 0.1]\n",
      "4  [0.0, 0.0, 0.0, 0.0, 1.0]  [0.05, 0.06666666666666667, 0.1, 0.2, 0.8]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'one_hot_encoded': [\n",
    "    np.array([0., 1., 0., 0., 0.]),\n",
    "    np.array([1., 0., 0., 0., 0.]),\n",
    "    np.array([0., 0., 0., 1., 0.]),\n",
    "    np.array([0., 0., 1., 0., 0.]),\n",
    "    np.array([0., 0., 0., 0., 1.])\n",
    "]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to blur the one-hot encoded vectors\n",
    "def blur_vector(vector, blur_factor=0.2):\n",
    "    idx = np.argmax(vector)  # Get the index of the one-hot class\n",
    "    blurred_vector = np.zeros_like(vector)\n",
    "    \n",
    "    blurred_vector[idx] += (1 - blur_factor)  # Highest probability to the original class\n",
    "    \n",
    "    # Distribute the blur_factor to the neighbors without wrapping around\n",
    "    for i in range(1, len(vector)):\n",
    "        left = idx - i\n",
    "        right = idx + i\n",
    "        \n",
    "        if left >= 0:\n",
    "            blurred_vector[left] += blur_factor / i\n",
    "        if right < len(vector):\n",
    "            blurred_vector[right] += blur_factor / i\n",
    "        if left < 0 and right >= len(vector):\n",
    "            break\n",
    "    \n",
    "    return blurred_vector\n",
    "\n",
    "# Apply the blur_vector function to the 'one_hot_encoded' column\n",
    "df['blurred_encoded'] = df['one_hot_encoded'].apply(blur_vector)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"target_OHE\"] = X[\"target_OHE\"].apply(blur_vector)\n",
    "X[\"wap_target_OHE\"] = X[\"wap_target_OHE\"].apply(blur_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/95232 [00:00<?, ?it/s]c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\utils\\torch_classes.py:54: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  self.stocksDict[stock_id].wap_daily_ohe[day] = torch.tensor(stock_daily_data['wap_target_OHE'].to_list(), requires_grad=False, device='cuda:0')\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 24089/95232 [00:47<02:08, 553.86it/s]"
     ]
    }
   ],
   "source": [
    "importlib.reload(utils.torch_classes)\n",
    "trading_data = utils.torch_classes.TradingData(X,means)\n",
    "hidden_size = 64\n",
    "# trading_data.generate_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train: 385, Length of test 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 385/385 [00:00<00:00, 387.98it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 456.26it/s]\n"
     ]
    }
   ],
   "source": [
    "trading_data.generate_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i,stocks in enumerate(trading_data.stocksDict.values()):\n",
    "#     if i==0:\n",
    "#         continue\n",
    "#     else:\n",
    "#         stocks.data_daily = []\n",
    "# trading_data.train_batches = []\n",
    "# del train\n",
    "# del X\n",
    "# X = []\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# del pca_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del pca, pca_prices_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.training_testing' from 'c:\\\\Users\\\\Nick\\\\Documents\\\\GitHub\\\\OptiverKaggle\\\\utils\\\\training_testing.py'>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(utils.torch_classes)\n",
    "importlib.reload(utils.training_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_dict = {\n",
    "    \"RMSProp\": optim.RMSprop,\n",
    "    \"Adam\": optim.Adam,\n",
    "    \"RAdam\": optim.RAdam,\n",
    "    \"NAdam\": optim.NAdam,\n",
    "    \"AdamW\": optim.AdamW,\n",
    "    \"SGD\": optim.SGD,\n",
    "    \"Rprop\": optim.Rprop,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(\n",
    "    trading_df=trading_data, config=None, prev_model_file=None, prev_model_version=450\n",
    "):\n",
    "    trading_df = trading_data\n",
    "    with wandb.init(project=\"Optviver_new\", config=config, save_code=True):\n",
    "        wandb.define_metric(\"val_epoch_loss_l1\", summary=\"min\")\n",
    "        wandb.define_metric(\"epoch_l1_loss\", summary=\"min\")\n",
    "        wandb.define_metric(\"Accuracy\", summary=\"max\")\n",
    "        config['prev_model_file'] = prev_model_file\n",
    "        config = wandb.config\n",
    "\n",
    "        input_size = len(trading_df.stocksDict[0].data_daily[0][0])\n",
    "\n",
    "        target_size_ohe = len(trading_df.stocksDict[0].target_daily_ohe[0][0])\n",
    "\n",
    "        print(target_size_ohe)\n",
    "        \n",
    "        model = utils.torch_classes.GRUNetV4(\n",
    "            input_size,\n",
    "            config[\"hidden_size\"],\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            fc0_size=config[\"fc0_size\"],\n",
    "            target_size=target_size_ohe\n",
    "        ).to(\"cuda:0\")\n",
    "\n",
    "        wandb.watch(model, log='all') \n",
    "        optimizer = optim_dict[config['optim']](model.parameters(), lr=config['learning_rate'], weight_decay=0.01)\n",
    "        \n",
    "        config = wandb.config\n",
    "        print(config)\n",
    "        print(config['ohe_targets'])\n",
    "        if prev_model_file != None:\n",
    "            model_name = prev_model_file\n",
    "            config['prev_model_file'] = prev_model_file            \n",
    "            model_loc = f\"models/{model_name}/{model_name}_{prev_model_version}.pt\"\n",
    "            model_data = torch.load(model_loc, map_location=torch.device(\"cuda:0\"))\n",
    "            print(model_data[\"model_state_dict\"].keys())\n",
    "            print(model_data.keys())\n",
    "\n",
    "            # del_keys = ['fc_final.weight', 'fc_final.bias', 'fc_wap0.weight']\n",
    "            # [model_data['model_state_dict'].pop(k) for k in del_keys]\n",
    "            model.load_state_dict(model_data[\"model_state_dict\"], strict=False)\n",
    "            # optimizer.load_state_dict(model_data[\"optim\"])\n",
    "\n",
    "        print(model)\n",
    "        trading_df.reset_hidden(\n",
    "            hidden_size=config[\"hidden_size\"], num_layers=config[\"num_layers\"]\n",
    "        )\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        print(criterion)\n",
    "        print(optimizer)\n",
    "        output = utils.training_testing_double.train_model(\n",
    "            trading_df, model, config, optimizer, criterion\n",
    "        )\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# arroios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.training_testing_double' from 'c:\\\\Users\\\\Nick\\\\Documents\\\\GitHub\\\\OptiverKaggle\\\\utils\\\\training_testing_double.py'>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(utils.torch_classes)\n",
    "importlib.reload(utils.training_testing)\n",
    "importlib.reload(utils.training_testing_double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_static = {\n",
    "    \"learning_rate\": 0.00005,\n",
    "    \"hidden_size\": 256,\n",
    "    \"num_layers\": 2,\n",
    "    \"batch_norm\": 1,\n",
    "    \"epochs\": 5000,\n",
    "    \"mini_batches\": 20,\n",
    "    \"fc0_size\": 256,\n",
    "    \"note\": \"GRUNetV4, no detaching, wap, 20 mini batch, upping hidden size\",\n",
    "    'optim': 'RMSProp',\n",
    "    'ohe_targets':means,\n",
    "}\n",
    "config = config_static\n",
    "torch.cuda.empty_cache()\n",
    "trading_data.detach_hidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in trading_data.stocksDict.values():\n",
    "    s.hidden_out = torch.zeros(49,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m49\u001b[39m,\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m      2\u001b[0m x\u001b[38;5;241m.\u001b[39mview((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m200\u001b[39m,\u001b[38;5;241m5\u001b[39m))\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "x = torch.rand(49,1000)\n",
    "x.view((-1,200,5)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231216_101106-o4p3qklc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/Optviver_new/runs/o4p3qklc' target=\"_blank\">dark-thunder-344</a></strong> to <a href='https://wandb.ai/nickojelly/Optviver_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/Optviver_new' target=\"_blank\">https://wandb.ai/nickojelly/Optviver_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/Optviver_new/runs/o4p3qklc' target=\"_blank\">https://wandb.ai/nickojelly/Optviver_new/runs/o4p3qklc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "{'learning_rate': 5e-05, 'hidden_size': 256, 'num_layers': 2, 'batch_norm': 1, 'epochs': 5000, 'mini_batches': 20, 'fc0_size': 256, 'note': 'GRUNetV4, no detaching, wap, 20 mini batch, upping hidden size', 'optim': 'RMSProp', 'ohe_targets': '   original_index       wap_category  mean_wap        wap_cat_name  \\\\\\n0               0  (-379.691, -6.98]    -11.92  0(-379.691, -6.98]   \\n1               1     (-6.98, -1.79]     -4.02     1(-6.98, -1.79]   \\n2               2      (-1.79, 1.82]      0.01      2(-1.79, 1.82]   \\n3               3       (1.82, 7.05]      4.08       3(1.82, 7.05]   \\n4               4     (7.05, 392.99]     11.96     4(7.05, 392.99]   \\n\\n    target_category  mean_target    target_cat_name  \\n0  (-385.291, -5.9]    -9.940267  0(-385.291, -5.9]  \\n1     (-5.9, -1.67]    -3.499985     1(-5.9, -1.67]  \\n2     (-1.67, 1.56]    -0.050068     2(-1.67, 1.56]  \\n3      (1.56, 5.76]     3.390312      3(1.56, 5.76]  \\n4   (5.76, 387.779]     9.800196   4(5.76, 387.779]  '}\n",
      "   original_index       wap_category  mean_wap        wap_cat_name  \\\n",
      "0               0  (-379.691, -6.98]    -11.92  0(-379.691, -6.98]   \n",
      "1               1     (-6.98, -1.79]     -4.02     1(-6.98, -1.79]   \n",
      "2               2      (-1.79, 1.82]      0.01      2(-1.79, 1.82]   \n",
      "3               3       (1.82, 7.05]      4.08       3(1.82, 7.05]   \n",
      "4               4     (7.05, 392.99]     11.96     4(7.05, 392.99]   \n",
      "\n",
      "    target_category  mean_target    target_cat_name  \n",
      "0  (-385.291, -5.9]    -9.940267  0(-385.291, -5.9]  \n",
      "1     (-5.9, -1.67]    -3.499985     1(-5.9, -1.67]  \n",
      "2     (-1.67, 1.56]    -0.050068     2(-1.67, 1.56]  \n",
      "3      (1.56, 5.76]     3.390312      3(1.56, 5.76]  \n",
      "4   (5.76, 387.779]     9.800196   4(5.76, 387.779]  \n",
      "GRUNetV4(\n",
      "  (gru): GRU(256, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (drop_1): Dropout(p=0.5, inplace=False)\n",
      "  (fc0): Linear(in_features=22, out_features=256, bias=True)\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc_final): Linear(in_features=256, out_features=5, bias=True)\n",
      "  (fc_reg0): Linear(in_features=10, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f732454e6fa14e23afb6c97ec862c8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Temp\\ipykernel_18904\\241991133.py\", line 52, in model_pipeline\n",
      "    output = utils.training_testing_double.train_model(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\utils\\training_testing_double.py\", line 109, in train_model\n",
      "    output_wap_ohe, output_wap, hidden, _, x_h = model(new_x, hidden_in)\n",
      "                                                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1568, in _call_impl\n",
      "    result = forward_call(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\utils\\torch_classes.py\", line 481, in forward\n",
      "    x = self.layer_norm(x_h)\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py\", line 196, in forward\n",
      "    return F.layer_norm(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\functional.py\", line 2543, in layer_norm\n",
      "    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8880b6beaa4746ac1442ac84b032cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='57.480 MB of 57.480 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>â–‚â–â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡</td></tr><tr><td>L1_loss_wap_epoch</td><td>â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–†â–‡â–†â–…â–…</td></tr><tr><td>epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch_loss</td><td>â–…â–ˆâ–†â–…â–â–„â–‡â–…â–‡â–…â–…â–â–„â–â–…â–‡â–…â–„â–â–„â–‚â–…â–ˆâ–…â–…â–â–…â–â–…â–‡â–„â–„â–‚â–…â–‚â–ƒâ–†â–…â–‚â–ƒ</td></tr><tr><td>loss_1</td><td>â–ˆâ–â–ˆâ–„â–…â–…â–…â–‡â–„â–…â–†â–„â–…â–…â–…â–„â–…â–„â–„â–„â–…â–…â–„â–…â–†â–‚â–„â–…â–†â–ƒâ–‚â–ƒâ–†â–†â–†â–ƒâ–†â–†â–‚â–‚</td></tr><tr><td>losst_to_zero</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>output_sd</td><td>â–ˆâ–‡â–‚â–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–â–ƒâ–„â–ƒâ–â–â–â–‚â–â–â–ƒâ–ƒ</td></tr><tr><td>relu_sum</td><td>â–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–</td></tr><tr><td>target_calc_loss</td><td>â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†</td></tr><tr><td>train_class_tgt_loss</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train_class_wap_loss</td><td>â–ˆâ–†â–„â–…â–…â–„â–ƒâ–„â–„â–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–…â–„â–„â–„â–„â–ƒâ–„â–„â–„â–„â–„â–„â–ƒâ–‚â–â–†â–ƒâ–â–„</td></tr><tr><td>train_target_loss</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train_wap_loss</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–</td></tr><tr><td>val_epoch_loss_wap</td><td>â–ˆâ–ˆâ–†â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–…â–…â–‡â–‡â–ˆâ–†â–ˆâ–ˆâ–‡â–‡</td></tr><tr><td>val_loss_wap_ohe</td><td>â–ˆâ–‡â–†â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–‚â–â–â–â–‚â–‚â–‚â–â–‚â–‚â–â–</td></tr><tr><td>wap_pred_loss</td><td>â–„â–â–…â–ƒâ–„â–„â–„â–„â–„â–„â–…â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–†â–†â–‡â–‡â–†â–†â–†â–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–†â–†</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.26455</td></tr><tr><td>L1_loss_wap_epoch</td><td>6.18971</td></tr><tr><td>epoch</td><td>67</td></tr><tr><td>epoch_loss</td><td>11.54975</td></tr><tr><td>loss_1</td><td>12.04104</td></tr><tr><td>losst_to_zero</td><td>6.17443</td></tr><tr><td>output_sd</td><td>0.32217</td></tr><tr><td>relu_sum</td><td>756514.875</td></tr><tr><td>target_calc_loss</td><td>6.47479</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>6.04278</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.39624</td></tr><tr><td>val_epoch_loss_wap</td><td>2.08626</td></tr><tr><td>val_loss_wap_ohe</td><td>2.09884</td></tr><tr><td>wap_pred_loss</td><td>7.57019</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dark-thunder-344</strong> at: <a href='https://wandb.ai/nickojelly/Optviver_new/runs/o4p3qklc' target=\"_blank\">https://wandb.ai/nickojelly/Optviver_new/runs/o4p3qklc</a><br/>Synced 6 W&B file(s), 21 media file(s), 21 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231216_101106-o4p3qklc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m CUDA_LAUNCH_BLOCKING \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# output = model_pipeline(trading_data, config_static,)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrading_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_static\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[56], line 52\u001b[0m, in \u001b[0;36mmodel_pipeline\u001b[1;34m(trading_df, config, prev_model_file, prev_model_version)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(criterion)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(optimizer)\n\u001b[1;32m---> 52\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_testing_double\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrading_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\utils\\training_testing_double.py:109\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(trading_df, model, config, optimizer, criterion)\u001b[0m\n\u001b[0;32m    103\u001b[0m criterion\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m daily_weights\n\u001b[0;32m    105\u001b[0m hidden_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([x\u001b[38;5;241m.\u001b[39mhidden \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m stocks])\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 109\u001b[0m output_wap_ohe, output_wap, hidden, _, x_h \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m output_wap_ohe \u001b[38;5;241m=\u001b[39m output_wap_ohe\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m    112\u001b[0m hidden \u001b[38;5;241m=\u001b[39m hidden\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1565\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1573\u001b[0m     ):\n\u001b[0;32m   1574\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\utils\\torch_classes.py:481\u001b[0m, in \u001b[0;36mGRUNetV4.forward\u001b[1;34m(self, x, h, test)\u001b[0m\n\u001b[0;32m    478\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m    479\u001b[0m x_h,hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru(x,h\u001b[38;5;241m.\u001b[39mcontiguous())\n\u001b[1;32m--> 481\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_h\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    482\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m    483\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(x)\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:196\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\functional.py:2543\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2541\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[0;32m   2542\u001b[0m     )\n\u001b[1;32m-> 2543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CUDA_LAUNCH_BLOCKING = 1\n",
    "# output = model_pipeline(trading_data, config_static,)\n",
    "output = model_pipeline(trading_data, config_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output_all,Y_ohe_target,stock_ids \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "output_all,Y_ohe_target,stock_ids = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = [output.grad_fn.next_functions[0][0]]\n",
    "outs = []\n",
    "while start:\n",
    "    fn = start.pop(0)\n",
    "    print(fn)\n",
    "    if fn:\n",
    "        outs.append(fn)\n",
    "        next_fns = fn.next_functions\n",
    "        start.extend([x[0] for x in next_fns])\n",
    "        print(start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in outs:\n",
    "    print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49, 25600])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([s.hidden_out for s in trading_data.stocksDict.values()],dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = utils.torch_classes.GRUNetV3(\n",
    "    22,\n",
    "    config[\"hidden_size\"],\n",
    "    num_layers=config[\"num_layers\"],\n",
    "    fc0_size=config[\"fc0_size\"],\n",
    "    target_size=5\n",
    ").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.named_parameters at 0x0000026BB89ACD40>\n"
     ]
    }
   ],
   "source": [
    "print(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[ 3.7291e-01, -4.4458e-01, -4.3755e-02,  3.4391e-01, -7.2737e-02],\n",
       "           [ 1.2280e-01,  1.3427e-01,  2.1452e-01, -3.4860e-01,  1.2834e-01],\n",
       "           [ 3.8206e-01, -4.4528e-01, -1.8777e-01, -2.8338e-02, -2.6217e-01],\n",
       "           [ 2.9334e-01,  1.8038e-01,  1.3834e-01,  1.9175e-01,  2.0782e-01],\n",
       "           [-8.5392e-02,  3.2079e-01, -1.3422e-01, -4.0403e-01, -2.3458e-01],\n",
       "           [ 3.2337e-01,  3.2368e-01, -1.1684e-01,  3.6930e-01, -3.0831e-01],\n",
       "           [ 2.1491e-01,  1.5638e-01, -2.3402e-01, -1.5434e-02, -2.3061e-01],\n",
       "           [ 2.8756e-01, -4.8865e-02,  8.7331e-02, -1.9059e-01,  2.7535e-02],\n",
       "           [ 3.6005e-01, -3.6371e-01, -2.7036e-02,  5.9889e-03, -2.7516e-01],\n",
       "           [-3.1839e-01,  3.7880e-01,  1.8474e-01, -2.0867e-01,  5.1393e-03],\n",
       "           [-4.0443e-01, -1.5904e-01,  2.0367e-01, -3.6570e-01,  2.4082e-01],\n",
       "           [ 1.7567e-01, -2.0604e-01, -2.8393e-01,  2.9547e-01, -2.7353e-01],\n",
       "           [-1.4293e-03,  8.7977e-03,  1.9564e-01,  2.1823e-02,  3.8455e-01],\n",
       "           [ 2.8605e-01, -3.3175e-01,  4.1654e-01, -3.6800e-02,  3.3588e-01],\n",
       "           [ 1.2070e-01, -1.7248e-01, -4.0787e-01, -3.4206e-01, -2.2069e-01],\n",
       "           [ 2.8356e-01,  1.6401e-01, -6.7495e-02, -4.4268e-01,  1.7382e-01],\n",
       "           [-2.4022e-01,  1.0718e-01, -3.3688e-01, -2.6927e-01,  3.8410e-01],\n",
       "           [-2.7880e-01,  1.2576e-01,  2.8680e-01,  2.4634e-01,  2.8932e-01],\n",
       "           [ 4.3397e-01, -9.9730e-02,  4.4028e-01,  1.7958e-01,  1.5654e-01],\n",
       "           [-2.0298e-02,  1.0889e-01, -3.5532e-01,  1.6622e-01,  3.0522e-01],\n",
       "           [-3.6187e-01,  4.2697e-01,  8.9681e-02, -2.7104e-02, -6.0476e-02],\n",
       "           [ 3.3302e-01, -2.0032e-01, -3.8729e-01, -1.4650e-01, -1.7519e-01],\n",
       "           [-2.9324e-01,  1.7847e-01, -4.3591e-01,  4.4121e-01, -1.1596e-01],\n",
       "           [ 1.8019e-01, -5.6562e-03,  2.0691e-01, -2.9030e-01,  1.0257e-01],\n",
       "           [ 2.8376e-01, -4.2092e-02, -1.2323e-01,  3.1840e-01, -4.2351e-01],\n",
       "           [-7.9669e-02, -3.7906e-01,  8.3048e-02,  1.3747e-01,  6.8477e-02],\n",
       "           [ 2.7384e-01,  1.7524e-01, -4.3679e-01,  2.4078e-01,  2.9960e-01],\n",
       "           [ 7.4053e-02,  2.4995e-01, -2.1894e-01, -1.9736e-01, -1.3114e-01],\n",
       "           [ 3.4375e-03,  3.1310e-01, -8.2568e-02, -4.3768e-01, -4.3239e-01],\n",
       "           [ 2.2245e-01,  1.8071e-01,  1.2654e-01,  4.0215e-01, -2.0267e-01],\n",
       "           [ 1.3022e-01, -1.5012e-01, -8.2055e-02, -1.4047e-01, -3.2619e-01],\n",
       "           [-4.3110e-01,  1.4812e-01, -2.4419e-01,  2.9401e-01, -2.3358e-01],\n",
       "           [-1.6161e-01, -3.3501e-01,  4.1033e-01,  1.5708e-01, -2.1953e-01],\n",
       "           [-3.6908e-01,  3.5234e-01, -2.7561e-01, -2.6572e-01,  1.9263e-01],\n",
       "           [ 4.2984e-01,  2.9374e-01, -1.6163e-01,  1.1074e-01,  2.3491e-01],\n",
       "           [ 3.3456e-01,  1.4604e-01,  2.8307e-01, -3.9282e-01, -1.2599e-01],\n",
       "           [ 3.9209e-01, -3.7297e-01,  2.5487e-01,  2.3355e-01,  3.2738e-01],\n",
       "           [ 1.1464e-01, -1.5043e-01,  3.3672e-01, -3.0894e-01, -2.3543e-01],\n",
       "           [-2.7643e-01,  4.3302e-01, -3.3287e-01, -3.0617e-02, -2.6098e-01],\n",
       "           [-3.9904e-01, -2.5945e-01,  3.2524e-01, -1.3226e-01, -3.3350e-01],\n",
       "           [-3.7794e-01,  3.6564e-01, -4.4340e-01, -4.3960e-01, -1.4475e-01],\n",
       "           [-4.0960e-02, -2.3681e-01, -6.7275e-02, -4.0695e-01,  2.3194e-01],\n",
       "           [ 1.1167e-01, -1.6381e-01, -2.8667e-01, -4.4396e-01, -1.4770e-01],\n",
       "           [-2.8798e-01,  1.6181e-01,  3.4398e-01,  9.4028e-02,  2.8457e-01],\n",
       "           [ 1.9626e-01, -1.8294e-01,  2.7389e-01,  2.8068e-01, -1.3082e-03],\n",
       "           [-1.8142e-01,  2.6001e-02,  4.0116e-01,  4.4241e-01,  1.2477e-01],\n",
       "           [-4.0546e-02,  4.2194e-01, -3.3876e-01,  2.2864e-01, -8.7954e-02],\n",
       "           [-7.0878e-03, -1.2493e-01,  2.1404e-01, -2.0196e-03,  6.1481e-02],\n",
       "           [ 3.4021e-01,  2.6638e-01,  1.0739e-01, -5.7877e-02,  7.4487e-02],\n",
       "           [-2.4413e-01,  2.8013e-02, -2.7924e-01, -2.0549e-01,  1.7820e-01],\n",
       "           [-4.0621e-01,  4.4464e-01,  2.0355e-02, -1.8073e-02, -4.3637e-02],\n",
       "           [ 1.5938e-01, -1.5365e-01, -4.8246e-03, -4.4372e-01, -2.9577e-01],\n",
       "           [ 3.4620e-01, -4.5570e-02,  2.3212e-01, -2.6832e-01, -3.9874e-01],\n",
       "           [ 3.8151e-01, -3.5470e-01, -6.0381e-02,  2.5510e-01,  3.3190e-01],\n",
       "           [-1.4059e-01, -6.7851e-02,  8.6485e-02, -2.2402e-01,  1.7561e-01],\n",
       "           [ 2.4195e-01, -2.8198e-01, -1.9871e-02, -3.6600e-01,  1.6173e-01],\n",
       "           [ 8.4395e-02,  1.6604e-01,  3.3794e-01,  1.1379e-01,  3.5397e-01],\n",
       "           [-1.9148e-01,  2.4666e-03, -4.2477e-02,  1.6738e-01, -4.2284e-01],\n",
       "           [ 3.5203e-01, -3.9709e-01, -1.3448e-01,  3.9049e-01,  6.0177e-02],\n",
       "           [ 2.4684e-01,  1.7093e-01,  4.2211e-01, -3.6970e-01, -4.3258e-01],\n",
       "           [-1.0886e-01, -3.5314e-01, -2.2381e-01,  3.0486e-01, -2.1124e-01],\n",
       "           [-3.1366e-01, -4.2458e-01, -4.1652e-01, -2.1869e-01, -2.6015e-01],\n",
       "           [-2.4176e-01, -1.4407e-01, -1.6266e-01,  4.1141e-02, -1.9175e-01],\n",
       "           [ 2.0158e-01,  1.7432e-02,  2.7129e-04,  4.1541e-02,  4.3120e-01]],\n",
       "          device='cuda:0', requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([ 0.2863, -0.0557, -0.2709,  0.0936, -0.2758,  0.1966,  0.1503,  0.1481,\n",
       "            0.1558,  0.0041, -0.0472, -0.3124, -0.0429,  0.2989,  0.3552, -0.4053,\n",
       "            0.0334,  0.2676,  0.0320,  0.2306,  0.0143, -0.1212,  0.4049,  0.1176,\n",
       "           -0.3597, -0.3387,  0.3581,  0.0168,  0.1622,  0.0700, -0.0982,  0.0541,\n",
       "            0.3878,  0.3184,  0.4298,  0.3505,  0.1642,  0.1008, -0.0647,  0.1819,\n",
       "           -0.1931,  0.0528, -0.3447, -0.0865, -0.2100, -0.3699, -0.0924,  0.2305,\n",
       "           -0.4171, -0.2427, -0.3885, -0.3266, -0.2623, -0.0437,  0.2277,  0.1079,\n",
       "            0.1469, -0.1378, -0.3172,  0.1079, -0.4472, -0.1901, -0.1822, -0.0229],\n",
       "          device='cuda:0', requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([[-6.9247e-02,  7.9935e-02,  6.6473e-02,  1.1274e-01, -1.0338e-01,\n",
       "            -1.2042e-01, -6.8781e-02, -6.3894e-02, -1.2315e-01, -5.4256e-02,\n",
       "            -9.8623e-02, -5.0802e-02,  7.6958e-02, -4.2316e-02, -1.0783e-01,\n",
       "             8.7500e-02, -8.1571e-02, -1.7378e-02, -4.8078e-02,  1.0309e-01,\n",
       "            -7.3725e-03,  6.9163e-02,  1.1989e-02, -5.7481e-02,  1.9799e-02,\n",
       "             6.5290e-02,  7.2944e-03,  5.2027e-02,  6.8104e-02,  1.1403e-01,\n",
       "             1.0549e-01, -2.9668e-02,  1.4978e-02,  4.0969e-02,  3.3632e-02,\n",
       "             1.3625e-02,  5.0860e-02, -1.1626e-02,  1.1755e-01,  7.6026e-02,\n",
       "            -8.7822e-02,  1.1083e-01,  3.5489e-02,  7.5153e-02, -6.4239e-02,\n",
       "             6.6995e-02,  1.0785e-01,  1.1574e-01, -1.0972e-01,  5.0364e-03,\n",
       "            -1.8461e-02, -7.6211e-02,  2.8519e-02, -5.6491e-02, -8.2304e-02,\n",
       "             1.1599e-01, -3.7982e-02,  2.1612e-02,  8.6779e-03,  1.1130e-04,\n",
       "            -4.5270e-02, -7.7041e-02, -9.7771e-03, -6.0915e-02]], device='cuda:0',\n",
       "          requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([-0.0779], device='cuda:0', requires_grad=True)],\n",
       "  'lr': 0.001,\n",
       "  'momentum': 0,\n",
       "  'dampening': 0,\n",
       "  'weight_decay': 0,\n",
       "  'nesterov': False,\n",
       "  'maximize': False,\n",
       "  'foreach': None,\n",
       "  'differentiable': False}]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('gru.weight_ih_l0', Parameter containing:\n",
      "tensor([[ 0.0692,  0.0354,  0.0277,  ...,  0.0724,  0.0817, -0.0029],\n",
      "        [ 0.0584, -0.0322, -0.0534,  ..., -0.0696, -0.0301, -0.0261],\n",
      "        [ 0.0235,  0.0013, -0.0771,  ...,  0.0026, -0.0435,  0.0021],\n",
      "        ...,\n",
      "        [-0.0506,  0.0410, -0.0296,  ...,  0.0301, -0.0777,  0.0147],\n",
      "        [ 0.0097, -0.0110, -0.0794,  ...,  0.0062, -0.0580,  0.0042],\n",
      "        [-0.0327,  0.0709,  0.0339,  ...,  0.0064,  0.0483,  0.0750]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('gru.weight_hh_l0', Parameter containing:\n",
      "tensor([[-0.0731,  0.0470,  0.0003,  ...,  0.0390, -0.0785, -0.0132],\n",
      "        [-0.0131,  0.0352, -0.0494,  ..., -0.0209, -0.0540,  0.0480],\n",
      "        [-0.0019,  0.0267,  0.0391,  ...,  0.0851, -0.0083, -0.0700],\n",
      "        ...,\n",
      "        [ 0.0197, -0.0308, -0.0673,  ...,  0.0164,  0.0050,  0.0471],\n",
      "        [-0.0039, -0.0608, -0.0284,  ..., -0.0233,  0.0607,  0.0054],\n",
      "        [ 0.0729, -0.0014, -0.0598,  ...,  0.0774,  0.0224, -0.0415]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('gru.bias_ih_l0', Parameter containing:\n",
      "tensor([ 3.2352e-02,  2.8606e-02, -4.7658e-03,  7.8730e-02, -4.5272e-02,\n",
      "        -8.6763e-02, -9.6180e-03,  1.3138e-02, -8.3245e-02,  7.8489e-02,\n",
      "        -3.2669e-02,  1.4921e-02, -5.2034e-02, -8.4022e-02, -7.9749e-02,\n",
      "        -2.9451e-02,  8.1366e-02,  1.4829e-02,  5.5929e-03,  7.7531e-02,\n",
      "         8.5620e-02,  5.9278e-03,  8.8070e-02, -2.4641e-02, -2.1937e-02,\n",
      "        -7.8088e-02, -5.9218e-02, -3.2675e-02,  9.3461e-03, -8.2567e-02,\n",
      "        -6.9219e-02, -7.0475e-02, -7.4227e-02, -2.3913e-02, -4.3667e-02,\n",
      "         5.1925e-02, -3.3680e-02, -4.2036e-03, -6.3011e-02,  1.6498e-03,\n",
      "         5.6388e-02,  8.6906e-02, -4.4211e-02,  3.9389e-02, -2.2483e-02,\n",
      "        -5.8527e-02, -6.5131e-02,  7.7347e-02,  5.8719e-02,  6.7123e-02,\n",
      "         8.4859e-02, -7.5611e-02, -7.2967e-02,  1.4158e-02,  4.6462e-02,\n",
      "         3.6848e-02, -1.4437e-02, -2.2927e-02, -4.0055e-02,  5.5179e-02,\n",
      "        -5.6635e-02,  2.1534e-02,  5.7829e-02, -7.4736e-02, -3.6754e-02,\n",
      "        -7.1353e-02, -2.3349e-02,  6.6132e-02, -2.6137e-02, -1.3481e-02,\n",
      "        -7.1410e-02, -2.9397e-02, -2.4546e-02,  2.5939e-02, -2.8565e-02,\n",
      "         5.6575e-02,  3.7524e-02, -4.2184e-03, -2.9805e-02,  2.4337e-02,\n",
      "        -6.0663e-02, -4.7396e-02,  2.0043e-02, -6.9237e-02, -2.4360e-02,\n",
      "         2.5654e-02, -4.3854e-02, -7.5108e-02, -2.2705e-02, -2.5901e-02,\n",
      "         5.5513e-02,  5.5064e-02,  3.4694e-03, -8.5122e-02,  7.8547e-02,\n",
      "         3.5804e-02,  7.1156e-02, -4.5557e-03,  1.2937e-02, -1.7185e-03,\n",
      "         5.7410e-02,  3.2799e-02,  5.9856e-02,  5.6282e-03,  6.8510e-02,\n",
      "         5.8937e-02, -8.0812e-02,  5.4592e-02, -7.4775e-02, -3.9988e-02,\n",
      "        -4.1521e-02,  1.2184e-02, -4.8338e-02,  6.4696e-02, -4.5655e-02,\n",
      "         1.4907e-02,  4.6836e-02,  3.7830e-02,  8.5388e-02, -6.4941e-02,\n",
      "         7.9762e-02,  7.9420e-02, -2.4160e-02, -6.7244e-02,  2.2816e-02,\n",
      "         1.0827e-02,  2.6795e-02,  6.3825e-02, -6.8621e-02, -4.9027e-02,\n",
      "        -5.0764e-02, -5.8139e-03,  2.9339e-02, -5.2818e-02, -2.8005e-02,\n",
      "         7.6278e-02,  5.0871e-02, -4.7209e-02,  7.7496e-02, -3.5201e-02,\n",
      "         1.2877e-02, -6.5407e-02, -1.3398e-02,  8.0715e-02,  2.3442e-02,\n",
      "        -1.6557e-02,  8.3851e-02,  7.1176e-02,  4.7076e-02,  2.6756e-02,\n",
      "        -6.6852e-02,  5.2298e-02, -2.9636e-02,  8.3121e-02,  1.6381e-02,\n",
      "         1.8580e-02,  4.9770e-02, -1.7648e-02, -1.3862e-02,  6.7856e-03,\n",
      "         4.0722e-02,  4.1150e-02,  6.1365e-02, -4.7053e-02, -7.7826e-02,\n",
      "         6.8993e-03,  1.4894e-02,  1.4779e-03,  3.9662e-02, -6.5876e-02,\n",
      "        -2.8449e-03,  3.5701e-02,  6.2333e-02,  8.1293e-02,  5.8975e-02,\n",
      "         6.0871e-06,  7.8952e-02, -5.7839e-03,  5.9865e-03,  6.4785e-02,\n",
      "        -7.2760e-02,  8.3059e-02,  5.8022e-02, -4.9053e-03,  8.4776e-02,\n",
      "        -6.9815e-02, -3.7235e-02, -2.2027e-02,  2.3220e-02,  3.4994e-02,\n",
      "         7.2588e-02, -3.2387e-02, -9.0243e-03, -8.0950e-02, -2.8350e-02,\n",
      "        -4.7087e-02,  1.4714e-02, -2.5422e-02,  7.2241e-02, -9.1795e-03,\n",
      "        -7.7906e-02,  7.9889e-02,  2.7256e-02, -1.4403e-02,  5.7888e-02,\n",
      "        -2.4085e-03, -8.3673e-02, -6.7059e-02, -4.7060e-02,  2.3003e-02,\n",
      "        -5.7638e-03, -3.1571e-02, -6.5455e-02,  6.7595e-02, -4.1928e-02,\n",
      "        -2.3065e-02, -5.3478e-02, -7.9144e-02, -1.8669e-02,  4.8180e-02,\n",
      "        -6.1318e-02, -7.3139e-02, -3.2671e-02, -3.3404e-02, -2.2468e-02,\n",
      "        -3.1763e-02, -7.7847e-02,  3.3434e-02,  6.6730e-02, -7.9974e-02,\n",
      "        -7.3391e-02, -8.6526e-02, -8.7819e-03, -2.1221e-02,  5.5496e-02,\n",
      "        -4.1122e-02,  4.4156e-02, -4.3858e-02,  1.8920e-02,  2.5012e-02,\n",
      "        -3.2027e-02,  6.2518e-02, -4.8412e-02, -8.4571e-02,  1.4445e-02,\n",
      "        -8.3935e-02,  3.1603e-02, -1.6694e-02,  4.1874e-02,  8.4512e-02,\n",
      "        -2.1383e-02,  3.5336e-02, -1.3658e-02, -6.7992e-02, -1.5681e-02,\n",
      "         4.2689e-02,  7.1021e-03, -5.3597e-04, -6.8482e-02, -6.9595e-02,\n",
      "         3.0038e-02,  4.1171e-03,  6.5411e-02,  1.0046e-02,  5.2440e-02,\n",
      "         1.9273e-02,  4.3311e-02, -8.5786e-02, -4.5199e-02, -4.1249e-02,\n",
      "         7.3250e-02, -2.1068e-02, -9.6244e-03,  1.9204e-02,  5.0837e-02,\n",
      "        -8.0018e-02,  1.3806e-02,  6.6410e-02,  7.5086e-02, -8.5725e-02,\n",
      "        -8.6158e-03, -1.3236e-02,  6.6849e-02, -8.6268e-02, -4.9681e-02,\n",
      "         8.7781e-03, -3.0885e-02,  4.3062e-02, -5.6949e-02, -4.1285e-02,\n",
      "        -1.6184e-02, -2.7088e-02,  4.6255e-02,  4.7342e-03, -3.1257e-02,\n",
      "        -8.3949e-02,  1.5887e-02, -7.4760e-03, -8.5859e-02,  1.6207e-02,\n",
      "        -2.7675e-02,  7.2071e-02, -3.3304e-02, -6.7746e-02, -3.2733e-02,\n",
      "        -8.7969e-03, -1.2112e-02, -5.9353e-02,  8.1948e-02,  4.8306e-02,\n",
      "        -4.6288e-02, -1.4444e-02, -6.0014e-05, -3.8495e-02, -3.9850e-02,\n",
      "         4.6740e-02,  8.7409e-03,  5.0170e-02,  5.2063e-02,  1.5862e-02,\n",
      "         9.1630e-03, -7.6679e-03, -1.0765e-02,  8.5382e-02,  6.9096e-02,\n",
      "         8.5050e-02, -5.3795e-02,  3.0935e-02, -4.6199e-02,  5.7074e-02,\n",
      "         5.4308e-02,  8.2586e-02, -4.7462e-02,  4.7524e-02,  2.3624e-02,\n",
      "         3.0213e-03, -6.9447e-05, -2.3673e-02, -2.8790e-03, -5.3539e-02,\n",
      "        -2.8003e-02,  1.8911e-02, -2.1003e-03,  3.4973e-02,  5.9834e-03,\n",
      "         5.6757e-02, -4.9402e-02, -6.2981e-02,  2.8649e-02, -1.6152e-02,\n",
      "        -5.0786e-02, -6.3883e-02,  4.4124e-03, -2.0051e-02, -6.2891e-02,\n",
      "         6.5777e-02,  3.2828e-02, -2.1469e-02,  6.5784e-02,  7.3678e-02,\n",
      "        -8.1914e-02, -8.2368e-02, -2.1957e-02,  3.0031e-02,  3.0768e-02,\n",
      "        -5.1793e-02, -7.6958e-02, -3.5808e-02,  4.0411e-02,  1.5377e-02,\n",
      "        -3.3233e-02,  7.8377e-02,  5.0599e-02, -3.2973e-02,  3.6357e-02,\n",
      "         6.2867e-02, -7.8880e-02,  2.1640e-02, -3.7159e-02,  5.8094e-02,\n",
      "         8.6487e-02,  1.1344e-02, -5.0131e-02, -3.3188e-02], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "('gru.bias_hh_l0', Parameter containing:\n",
      "tensor([ 0.0181, -0.0721,  0.0399, -0.0601,  0.0867,  0.0109,  0.0066,  0.0531,\n",
      "        -0.0098, -0.0851,  0.0844,  0.0109,  0.0565, -0.0597,  0.0368, -0.0772,\n",
      "        -0.0243,  0.0530,  0.0745,  0.0798,  0.0543,  0.0817,  0.0165, -0.0619,\n",
      "        -0.0586,  0.0494,  0.0496,  0.0329, -0.0304,  0.0842, -0.0464,  0.0162,\n",
      "         0.0729, -0.0264, -0.0270, -0.0089, -0.0664, -0.0655,  0.0476, -0.0063,\n",
      "         0.0499, -0.0174, -0.0764,  0.0195, -0.0709, -0.0741, -0.0717,  0.0019,\n",
      "        -0.0453,  0.0201,  0.0077,  0.0134,  0.0484,  0.0450, -0.0459, -0.0374,\n",
      "         0.0033,  0.0362, -0.0128, -0.0856,  0.0479,  0.0382, -0.0358, -0.0134,\n",
      "        -0.0288,  0.0269,  0.0389,  0.0492, -0.0245, -0.0509,  0.0180, -0.0674,\n",
      "         0.0213,  0.0822,  0.0135, -0.0553, -0.0643, -0.0266,  0.0129,  0.0186,\n",
      "        -0.0250,  0.0468,  0.0514, -0.0583,  0.0095,  0.0526, -0.0728, -0.0834,\n",
      "         0.0620,  0.0545,  0.0008, -0.0833,  0.0296, -0.0544, -0.0100, -0.0112,\n",
      "        -0.0153, -0.0739, -0.0095,  0.0616,  0.0807, -0.0029,  0.0353,  0.0014,\n",
      "        -0.0252,  0.0506,  0.0425, -0.0875, -0.0192, -0.0363,  0.0140,  0.0623,\n",
      "        -0.0652, -0.0243,  0.0561, -0.0362,  0.0092,  0.0196, -0.0868,  0.0453,\n",
      "         0.0184, -0.0251, -0.0565,  0.0839, -0.0114,  0.0803, -0.0489,  0.0363,\n",
      "         0.0347, -0.0554,  0.0726, -0.0686,  0.0552,  0.0783, -0.0364,  0.0778,\n",
      "         0.0131,  0.0254,  0.0278,  0.0628,  0.0855, -0.0401,  0.0213, -0.0704,\n",
      "        -0.0882,  0.0705, -0.0076, -0.0748,  0.0117, -0.0709,  0.0562, -0.0152,\n",
      "         0.0493,  0.0764, -0.0422,  0.0282, -0.0176, -0.0227,  0.0619,  0.0445,\n",
      "        -0.0602, -0.0564,  0.0630, -0.0260, -0.0460,  0.0648, -0.0826,  0.0721,\n",
      "         0.0633, -0.0855,  0.0100, -0.0268,  0.0627, -0.0603, -0.0289, -0.0691,\n",
      "         0.0569,  0.0768, -0.0497,  0.0288, -0.0541,  0.0136, -0.0704,  0.0581,\n",
      "         0.0567,  0.0445,  0.0587,  0.0075, -0.0364,  0.0405, -0.0514,  0.0630,\n",
      "        -0.0539, -0.0449,  0.0518,  0.0773, -0.0551,  0.0217,  0.0387,  0.0308,\n",
      "         0.0745, -0.0819, -0.0738, -0.0771,  0.0044, -0.0568,  0.0475,  0.0466,\n",
      "         0.0162,  0.0367, -0.0171,  0.0520, -0.0685,  0.0582,  0.0369,  0.0760,\n",
      "         0.0826, -0.0410, -0.0655, -0.0563,  0.0846,  0.0195,  0.0076, -0.0695,\n",
      "         0.0432,  0.0192,  0.0332,  0.0124, -0.0191,  0.0523,  0.0508, -0.0205,\n",
      "        -0.0676,  0.0790, -0.0813,  0.0631,  0.0510, -0.0407,  0.0173,  0.0287,\n",
      "        -0.0514, -0.0232,  0.0636, -0.0592,  0.0772, -0.0760, -0.0293,  0.0135,\n",
      "        -0.0564,  0.0390,  0.0864, -0.0103, -0.0419,  0.0809,  0.0008, -0.0453,\n",
      "         0.0837,  0.0428, -0.0394, -0.0214,  0.0650, -0.0637,  0.0733,  0.0882,\n",
      "        -0.0099,  0.0002, -0.0859, -0.0146,  0.0853, -0.0085, -0.0565,  0.0479,\n",
      "         0.0841, -0.0862, -0.0369, -0.0123, -0.0840, -0.0021,  0.0590, -0.0739,\n",
      "         0.0758,  0.0369,  0.0865,  0.0822,  0.0541,  0.0018,  0.0756, -0.0673,\n",
      "        -0.0614, -0.0698,  0.0521, -0.0060,  0.0258,  0.0092,  0.0377, -0.0218,\n",
      "        -0.0274, -0.0233, -0.0038, -0.0057,  0.0796,  0.0502, -0.0005, -0.0508,\n",
      "         0.0597,  0.0521, -0.0630,  0.0623,  0.0170,  0.0097, -0.0270, -0.0424,\n",
      "        -0.0851, -0.0009,  0.0815, -0.0297, -0.0386,  0.0006,  0.0563,  0.0510,\n",
      "         0.0268, -0.0578, -0.0523, -0.0345, -0.0402, -0.0099, -0.0730, -0.0815,\n",
      "        -0.0284,  0.0522,  0.0380, -0.0654, -0.0438, -0.0441, -0.0495,  0.0391,\n",
      "         0.0562,  0.0694, -0.0589, -0.0549,  0.0755, -0.0692, -0.0832,  0.0476,\n",
      "         0.0129,  0.0516,  0.0306, -0.0583,  0.0310,  0.0689, -0.0217, -0.0720,\n",
      "         0.0409,  0.0138, -0.0680, -0.0604,  0.0432,  0.0877, -0.0592, -0.0727,\n",
      "        -0.0848, -0.0513, -0.0437, -0.0545,  0.0243,  0.0735,  0.0240,  0.0850,\n",
      "        -0.0020,  0.0622,  0.0710,  0.0523, -0.0816, -0.0327,  0.0703, -0.0706,\n",
      "        -0.0822,  0.0762, -0.0232, -0.0209, -0.0222, -0.0365, -0.0216, -0.0520],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('gru.weight_ih_l1', Parameter containing:\n",
      "tensor([[-0.0006, -0.0097, -0.0712,  ..., -0.0365, -0.0343, -0.0217],\n",
      "        [ 0.0651, -0.0528,  0.0784,  ..., -0.0624,  0.0779, -0.0404],\n",
      "        [-0.0659,  0.0716,  0.0413,  ...,  0.0457,  0.0396, -0.0692],\n",
      "        ...,\n",
      "        [ 0.0363,  0.0538,  0.0332,  ...,  0.0386, -0.0428,  0.0306],\n",
      "        [ 0.0320, -0.0480, -0.0574,  ..., -0.0849, -0.0103, -0.0019],\n",
      "        [-0.0330, -0.0259, -0.0114,  ...,  0.0334,  0.0566,  0.0111]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('gru.weight_hh_l1', Parameter containing:\n",
      "tensor([[-1.6318e-02,  3.0334e-02,  5.1026e-02,  ...,  8.7144e-02,\n",
      "         -7.9040e-03, -4.7503e-02],\n",
      "        [ 9.5844e-05, -4.6579e-02, -3.1971e-02,  ...,  3.5711e-02,\n",
      "          2.5303e-02, -3.9342e-02],\n",
      "        [-4.7148e-02, -7.5938e-02,  2.1575e-02,  ..., -6.5718e-02,\n",
      "          6.3779e-02, -7.1327e-02],\n",
      "        ...,\n",
      "        [-1.6405e-02,  4.7217e-04, -7.4032e-02,  ..., -2.4903e-04,\n",
      "          8.0813e-02,  6.2280e-02],\n",
      "        [-5.1162e-02,  2.7428e-02, -3.9343e-02,  ...,  2.4824e-02,\n",
      "         -6.0552e-03,  2.1349e-02],\n",
      "        [-3.8645e-02, -3.2217e-02,  1.4342e-03,  ...,  6.8634e-02,\n",
      "          2.3081e-02, -5.5289e-03]], device='cuda:0', requires_grad=True))\n",
      "('gru.bias_ih_l1', Parameter containing:\n",
      "tensor([ 0.0303,  0.0181, -0.0767, -0.0438,  0.0555,  0.0256, -0.0337,  0.0120,\n",
      "        -0.0009, -0.0633, -0.0468, -0.0806, -0.0417,  0.0330, -0.0278, -0.0085,\n",
      "         0.0874, -0.0796,  0.0714,  0.0393, -0.0171,  0.0089,  0.0676,  0.0309,\n",
      "         0.0440, -0.0592,  0.0385, -0.0303, -0.0189,  0.0778, -0.0136,  0.0487,\n",
      "        -0.0836,  0.0432, -0.0223, -0.0777, -0.0693,  0.0198, -0.0691,  0.0540,\n",
      "         0.0669,  0.0524, -0.0514,  0.0452, -0.0142,  0.0721,  0.0448,  0.0854,\n",
      "         0.0333, -0.0262, -0.0244, -0.0045, -0.0660, -0.0039,  0.0112, -0.0232,\n",
      "         0.0207,  0.0417,  0.0208,  0.0808,  0.0864,  0.0162, -0.0778,  0.0457,\n",
      "         0.0796, -0.0316, -0.0514, -0.0043, -0.0401, -0.0854,  0.0754, -0.0628,\n",
      "         0.0325, -0.0315, -0.0323,  0.0191,  0.0419, -0.0861, -0.0227,  0.0156,\n",
      "         0.0384, -0.0091,  0.0439, -0.0761,  0.0708,  0.0615,  0.0388,  0.0200,\n",
      "         0.0406,  0.0781,  0.0809, -0.0527,  0.0010, -0.0841, -0.0830,  0.0640,\n",
      "         0.0799, -0.0843,  0.0475, -0.0534,  0.0476, -0.0179,  0.0862,  0.0038,\n",
      "        -0.0378, -0.0106, -0.0841,  0.0199, -0.0154, -0.0602,  0.0800, -0.0139,\n",
      "        -0.0336, -0.0345, -0.0831, -0.0262, -0.0364,  0.0810, -0.0374,  0.0629,\n",
      "         0.0657,  0.0288,  0.0568, -0.0525,  0.0718,  0.0191, -0.0741, -0.0618,\n",
      "        -0.0864, -0.0356, -0.0631, -0.0008, -0.0506,  0.0731, -0.0483, -0.0604,\n",
      "         0.0676,  0.0335,  0.0228, -0.0589, -0.0496,  0.0611, -0.0715,  0.0472,\n",
      "         0.0870,  0.0124, -0.0433,  0.0587,  0.0854,  0.0509, -0.0809, -0.0576,\n",
      "         0.0774, -0.0553, -0.0641,  0.0533,  0.0255, -0.0524,  0.0053,  0.0172,\n",
      "        -0.0143, -0.0307,  0.0152,  0.0581,  0.0108, -0.0708, -0.0206,  0.0645,\n",
      "        -0.0400, -0.0576,  0.0290, -0.0382, -0.0251, -0.0288,  0.0695,  0.0487,\n",
      "        -0.0668,  0.0575,  0.0818,  0.0186, -0.0776,  0.0424,  0.0874,  0.0442,\n",
      "        -0.0553,  0.0430,  0.0630,  0.0742,  0.0760,  0.0529,  0.0690, -0.0205,\n",
      "        -0.0044, -0.0672, -0.0379,  0.0396,  0.0344,  0.0039,  0.0271,  0.0370,\n",
      "        -0.0233,  0.0003, -0.0260,  0.0218,  0.0499,  0.0575, -0.0699, -0.0017,\n",
      "        -0.0385,  0.0621, -0.0866, -0.0615, -0.0475,  0.0191,  0.0328, -0.0699,\n",
      "         0.0236, -0.0186,  0.0844,  0.0437,  0.0226, -0.0439, -0.0416, -0.0402,\n",
      "         0.0221, -0.0548, -0.0719, -0.0097,  0.0544,  0.0165,  0.0071, -0.0763,\n",
      "         0.0535,  0.0501, -0.0457,  0.0759,  0.0057,  0.0481,  0.0570,  0.0200,\n",
      "         0.0807,  0.0447, -0.0020,  0.0806, -0.0623,  0.0402,  0.0143, -0.0495,\n",
      "         0.0474, -0.0685,  0.0423,  0.0061,  0.0030, -0.0853, -0.0404, -0.0095,\n",
      "        -0.0500, -0.0374,  0.0199, -0.0854, -0.0879,  0.0392, -0.0333, -0.0820,\n",
      "         0.0698, -0.0143,  0.0129,  0.0157,  0.0809,  0.0082, -0.0103, -0.0573,\n",
      "         0.0190,  0.0343, -0.0135,  0.0238,  0.0420,  0.0435, -0.0699, -0.0161,\n",
      "        -0.0771,  0.0706, -0.0470,  0.0040,  0.0652,  0.0709,  0.0333,  0.0494,\n",
      "         0.0640,  0.0466, -0.0284, -0.0802, -0.0345, -0.0562,  0.0267, -0.0148,\n",
      "        -0.0591, -0.0029, -0.0865,  0.0177,  0.0221, -0.0306, -0.0510, -0.0330,\n",
      "         0.0425, -0.0714, -0.0879, -0.0096,  0.0049, -0.0531,  0.0364,  0.0353,\n",
      "        -0.0532, -0.0572, -0.0745,  0.0614, -0.0447,  0.0368,  0.0528,  0.0644,\n",
      "        -0.0773,  0.0289,  0.0822, -0.0475,  0.0663, -0.0636, -0.0359,  0.0818,\n",
      "        -0.0037,  0.0737,  0.0631, -0.0553,  0.0260,  0.0176,  0.0875, -0.0152,\n",
      "         0.0263, -0.0668, -0.0216,  0.0569, -0.0375,  0.0690,  0.0517,  0.0395,\n",
      "        -0.0307, -0.0450,  0.0663, -0.0817,  0.0639,  0.0242,  0.0207, -0.0254,\n",
      "         0.0043,  0.0083, -0.0660, -0.0751, -0.0405, -0.0144,  0.0805, -0.0301,\n",
      "        -0.0421,  0.0178, -0.0135, -0.0373,  0.0141,  0.0869,  0.0349, -0.0587,\n",
      "        -0.0336,  0.0611,  0.0591,  0.0300, -0.0863,  0.0698,  0.0168, -0.0300,\n",
      "        -0.0072, -0.0293, -0.0720,  0.0557, -0.0616,  0.0140, -0.0307, -0.0757],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('gru.bias_hh_l1', Parameter containing:\n",
      "tensor([-8.7583e-02,  6.8298e-02, -4.5547e-02, -4.7524e-04,  5.3149e-02,\n",
      "        -6.3352e-04, -4.8351e-02, -5.6423e-02,  6.1267e-02,  4.7081e-03,\n",
      "        -2.3819e-03,  1.8914e-02, -3.3898e-02,  7.7569e-02,  8.3880e-02,\n",
      "        -8.1973e-02,  3.9652e-02,  4.0335e-02, -6.9310e-03, -5.3797e-02,\n",
      "        -6.2650e-02, -6.6882e-02,  4.6011e-02, -7.9633e-02,  5.2425e-02,\n",
      "         5.0870e-02, -7.8778e-02, -4.9497e-02,  6.9742e-02,  2.9723e-02,\n",
      "        -3.7987e-02,  4.2975e-02, -6.6959e-02,  5.7792e-02, -6.0567e-02,\n",
      "         8.2952e-02,  2.2749e-02,  8.4302e-02, -5.2363e-02, -7.8111e-02,\n",
      "         1.6107e-02,  4.4559e-02,  1.5962e-02, -1.7309e-02, -1.8644e-02,\n",
      "        -5.8653e-02,  5.9388e-02,  4.0878e-02, -4.0496e-02, -7.0499e-03,\n",
      "         2.2402e-03,  2.4120e-02,  2.0119e-02, -7.6404e-02,  3.5880e-02,\n",
      "         3.8410e-02, -1.9366e-02, -3.4253e-02,  7.0693e-02, -5.7105e-02,\n",
      "        -5.9536e-02,  3.4864e-02, -8.5301e-02, -2.8823e-02,  3.9178e-02,\n",
      "         1.8501e-02,  5.6711e-02,  3.3602e-02,  1.5603e-02,  5.4854e-02,\n",
      "        -7.3450e-02, -4.4402e-02, -3.7218e-02, -2.9268e-02, -1.5940e-02,\n",
      "        -1.7417e-02, -2.4349e-02,  8.0844e-02, -6.4920e-02, -8.6538e-02,\n",
      "        -2.0204e-02,  8.1037e-02,  3.0570e-02,  1.1841e-02,  6.3576e-02,\n",
      "         7.3798e-02, -2.4726e-02,  2.1935e-02,  7.6298e-02, -4.1799e-02,\n",
      "        -5.9259e-02,  6.8787e-02,  1.8994e-02,  5.5680e-02,  4.3107e-02,\n",
      "         2.3006e-02,  8.8296e-02, -6.2381e-02, -7.4663e-03, -1.6716e-02,\n",
      "        -3.9516e-02, -8.4177e-02,  5.3632e-02, -5.2304e-02, -7.9343e-02,\n",
      "         2.8396e-02, -3.2864e-03,  6.3207e-02, -8.3818e-02,  5.7010e-02,\n",
      "        -6.7289e-02, -1.5834e-02, -4.0855e-02,  8.4623e-02,  3.6005e-03,\n",
      "         7.7250e-02, -7.6003e-02, -1.7008e-03, -6.4849e-02,  2.2446e-02,\n",
      "         5.1011e-02, -8.3572e-02, -1.5446e-02,  2.6906e-03,  5.2854e-02,\n",
      "         5.1837e-02, -6.2693e-02, -2.5235e-02,  2.2281e-03, -6.3024e-02,\n",
      "        -4.3843e-02, -1.8681e-02,  5.4326e-02, -7.1065e-03,  1.2637e-03,\n",
      "         3.4508e-02,  1.9968e-02, -9.1798e-03, -5.3776e-02,  4.0230e-02,\n",
      "         7.4505e-02,  5.6338e-02,  1.5320e-02, -1.2434e-02, -3.8859e-02,\n",
      "         3.9347e-02,  2.1592e-02,  3.8185e-02,  3.6015e-02,  2.4856e-02,\n",
      "        -5.4064e-02, -3.4272e-02, -1.3256e-04, -8.6220e-02, -7.1327e-02,\n",
      "        -2.9344e-02, -7.4041e-02,  1.2557e-02,  8.2289e-02,  3.8668e-02,\n",
      "        -3.1998e-02,  6.5545e-02, -2.6908e-02, -3.1375e-02,  3.4789e-02,\n",
      "        -7.1401e-02, -8.4923e-02, -7.6983e-02, -3.3641e-02,  3.1186e-02,\n",
      "        -3.0819e-02, -7.9250e-02,  7.0130e-02, -1.7388e-03, -6.7261e-02,\n",
      "         8.5770e-03, -5.1852e-02,  6.2488e-02,  5.8898e-02,  6.3536e-02,\n",
      "         6.0416e-02, -4.6160e-02,  1.4748e-02,  6.9876e-02,  4.3403e-02,\n",
      "         6.6109e-02, -7.2987e-02,  2.9390e-03,  1.6055e-02, -5.5802e-02,\n",
      "        -8.3243e-02,  1.4838e-02,  9.4873e-03,  1.2208e-02, -8.0570e-02,\n",
      "         5.1042e-02, -7.5253e-02,  8.4995e-02, -3.2685e-02,  6.9283e-02,\n",
      "        -6.5283e-02,  2.2427e-02,  6.5302e-02,  6.8938e-02,  4.3185e-02,\n",
      "         5.5607e-02, -1.9177e-03,  6.4082e-02, -3.5848e-02,  8.7200e-02,\n",
      "        -1.3360e-02, -7.0678e-02, -2.4787e-02, -5.0854e-02, -5.3641e-02,\n",
      "        -2.7221e-02,  1.7288e-02, -4.1205e-02, -9.7473e-04,  6.0935e-02,\n",
      "         2.9795e-02, -3.5831e-02,  8.5336e-02, -7.0514e-02, -4.8488e-02,\n",
      "         7.0544e-03, -5.7365e-02,  6.1140e-02,  8.5985e-02,  6.7059e-02,\n",
      "        -5.8399e-02,  3.4341e-02, -1.7760e-02,  3.7748e-02,  1.4443e-02,\n",
      "         5.9770e-03, -8.0128e-02, -2.1525e-05, -5.8416e-02,  4.8361e-02,\n",
      "        -4.5941e-02, -6.8472e-03,  9.6198e-03,  8.2539e-02, -3.1291e-02,\n",
      "         7.6242e-02,  1.7183e-02,  3.3378e-02, -6.3966e-02, -3.3737e-02,\n",
      "        -2.1442e-02, -4.1345e-02,  7.0193e-03, -3.8316e-02,  8.7054e-02,\n",
      "        -7.6008e-02, -1.8204e-02,  3.0705e-02,  1.0319e-02, -7.7783e-02,\n",
      "         6.9540e-02,  8.1186e-02, -7.2763e-03, -5.0613e-02,  4.7807e-02,\n",
      "         2.6472e-02,  1.3150e-02,  4.1741e-03,  5.9755e-02, -4.4772e-02,\n",
      "        -6.2844e-02, -5.9583e-03,  3.3241e-02, -4.8225e-02,  1.3827e-02,\n",
      "         8.3706e-02, -2.7515e-02, -7.6389e-02,  4.0768e-02, -2.4297e-02,\n",
      "        -7.0537e-02, -7.9977e-02, -4.6557e-02,  8.6785e-02, -1.1863e-02,\n",
      "         2.1070e-02, -1.9918e-03,  6.9643e-02, -1.6113e-02, -4.8052e-02,\n",
      "        -3.1240e-02,  2.1693e-02, -2.3793e-03,  8.4243e-02, -5.6279e-02,\n",
      "         2.8515e-02, -6.1705e-02,  7.9168e-03,  5.9094e-04, -8.6663e-02,\n",
      "         1.0633e-02,  3.7232e-02,  2.2567e-02, -4.2471e-02, -4.9080e-02,\n",
      "        -4.1713e-02, -3.1286e-02, -8.2593e-02,  7.0490e-02,  7.6379e-02,\n",
      "         7.2013e-02, -1.9187e-02,  7.7711e-02,  1.1853e-02, -6.9703e-02,\n",
      "        -6.5089e-02, -4.6024e-02, -1.6384e-02,  1.3104e-02, -3.1672e-02,\n",
      "         1.2900e-02,  1.9088e-02,  4.7814e-02, -1.0426e-02,  5.0363e-03,\n",
      "        -3.7761e-02,  2.4084e-02,  3.8684e-02,  1.5150e-02, -1.2633e-02,\n",
      "         9.5961e-03, -6.7981e-02, -3.2838e-02, -4.0057e-02,  1.0377e-02,\n",
      "        -2.2362e-02, -3.8351e-02,  2.8693e-02,  1.1890e-02,  4.1963e-02,\n",
      "         5.0541e-02, -6.4481e-02, -1.6874e-02, -1.0849e-02,  6.8739e-02,\n",
      "         9.2683e-03, -4.5972e-02, -4.4145e-02,  1.0728e-02, -7.6567e-02,\n",
      "         3.2990e-02,  7.2382e-02, -1.8740e-02, -8.8732e-03,  5.7989e-02,\n",
      "         5.4440e-02,  7.2936e-02,  1.0129e-02, -5.7591e-02, -1.3610e-04,\n",
      "        -1.5575e-02, -3.5897e-02, -8.6240e-02,  4.6653e-04,  7.4259e-02,\n",
      "        -6.2486e-02, -8.7523e-02, -8.7193e-02, -8.4678e-02, -3.2526e-03,\n",
      "         4.5760e-02, -2.2706e-02, -4.4873e-03,  7.0304e-03,  5.3684e-02,\n",
      "        -3.9260e-02,  5.9556e-02, -7.5228e-02,  5.1734e-02,  7.2074e-02,\n",
      "        -3.2005e-02,  2.0028e-02, -5.5290e-03, -8.3281e-02], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "('batch_norm.weight', Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.], device='cuda:0', requires_grad=True))\n",
      "('batch_norm.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('layer_norm.weight', Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', requires_grad=True))\n",
      "('layer_norm.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))\n",
      "('layer_norm2.weight', Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.], device='cuda:0', requires_grad=True))\n",
      "('layer_norm2.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('fc0.weight', Parameter containing:\n",
      "tensor([[-0.0054,  0.0174, -0.0845,  ...,  0.1921, -0.1613, -0.1604],\n",
      "        [-0.0830, -0.1741, -0.0901,  ..., -0.1759, -0.1939,  0.1631],\n",
      "        [ 0.1263,  0.1174, -0.1029,  ...,  0.1446,  0.0542,  0.0931],\n",
      "        ...,\n",
      "        [ 0.1236,  0.0454,  0.1385,  ...,  0.1587, -0.0715, -0.0359],\n",
      "        [ 0.1193,  0.0055, -0.0102,  ...,  0.0616, -0.1953, -0.0338],\n",
      "        [ 0.0084,  0.1213, -0.2003,  ...,  0.1366, -0.1335, -0.1488]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('fc0.bias', Parameter containing:\n",
      "tensor([-0.0707,  0.1014, -0.0070, -0.1936, -0.1469,  0.1611,  0.0900,  0.0936,\n",
      "         0.0297, -0.0338, -0.1218,  0.1850,  0.2020, -0.0300, -0.0011,  0.0263,\n",
      "        -0.0736, -0.1156, -0.1885, -0.1508, -0.0187,  0.1812,  0.1620, -0.0645,\n",
      "        -0.0223, -0.1617, -0.1477,  0.2101, -0.1352, -0.1617, -0.1182, -0.1639,\n",
      "         0.1195, -0.0969, -0.0983,  0.1268,  0.0708, -0.1266, -0.1753, -0.0461,\n",
      "        -0.2054, -0.0361,  0.1829,  0.1192,  0.0656,  0.0786, -0.0196, -0.0641,\n",
      "        -0.2013, -0.1545, -0.0953,  0.0466,  0.1715,  0.1051, -0.0866,  0.1612,\n",
      "        -0.1635,  0.0706, -0.1261,  0.0773,  0.1865,  0.1001, -0.0669,  0.2013,\n",
      "        -0.0280, -0.0553, -0.0996,  0.1412, -0.1892, -0.1312,  0.0626, -0.0122,\n",
      "        -0.1495,  0.0012,  0.0932,  0.2057,  0.1599, -0.1794, -0.0795,  0.1974,\n",
      "         0.0104, -0.1892,  0.0224, -0.0842,  0.1142,  0.0849,  0.1778, -0.1689,\n",
      "        -0.0043,  0.0467,  0.2055, -0.0471,  0.1099,  0.0633, -0.1458, -0.0847,\n",
      "         0.0914,  0.1664,  0.0432,  0.0972, -0.0791,  0.1460,  0.1161,  0.0076,\n",
      "        -0.0480, -0.0650,  0.1695,  0.0669,  0.0632,  0.0706, -0.1900, -0.1128,\n",
      "        -0.1618, -0.0286, -0.0553,  0.1895,  0.1979, -0.1303, -0.0219, -0.1503,\n",
      "        -0.0045,  0.0308,  0.0134, -0.0919,  0.1688,  0.1519,  0.0637, -0.1798],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('fc1.weight', Parameter containing:\n",
      "tensor([[ 0.0745,  0.0053,  0.0005,  ...,  0.0679, -0.0247, -0.0413],\n",
      "        [-0.0495, -0.0713,  0.0828,  ...,  0.0780, -0.0766, -0.0752],\n",
      "        [ 0.0207, -0.0105,  0.0705,  ..., -0.0662, -0.0805,  0.0509],\n",
      "        ...,\n",
      "        [-0.0566, -0.0696, -0.0506,  ...,  0.0101,  0.0047,  0.0855],\n",
      "        [-0.0710, -0.0711, -0.0340,  ..., -0.0596,  0.0405,  0.0518],\n",
      "        [ 0.0633, -0.0544,  0.0553,  ...,  0.0105, -0.0699,  0.0102]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('fc1.bias', Parameter containing:\n",
      "tensor([-0.0374,  0.0459, -0.0279, -0.0557,  0.0472,  0.0669, -0.0270, -0.0359,\n",
      "         0.0713,  0.0331, -0.0615, -0.0098, -0.0856, -0.0513, -0.0858,  0.0460,\n",
      "         0.0328,  0.0117,  0.0278, -0.0559,  0.0357, -0.0832,  0.0042, -0.0438,\n",
      "         0.0497,  0.0643,  0.0554,  0.0717, -0.0309,  0.0548,  0.0692,  0.0720,\n",
      "         0.0035,  0.0711, -0.0805,  0.0059, -0.0833, -0.0157, -0.0623,  0.0672,\n",
      "        -0.0789,  0.0739,  0.0349, -0.0551,  0.0612,  0.0731,  0.0646,  0.0167,\n",
      "        -0.0523, -0.0229, -0.0025,  0.0734,  0.0452,  0.0410, -0.0155,  0.0371,\n",
      "        -0.0500, -0.0058, -0.0153,  0.0119,  0.0170,  0.0670,  0.0230,  0.0716,\n",
      "        -0.0082, -0.0393,  0.0016,  0.0882, -0.0729,  0.0144, -0.0593, -0.0324,\n",
      "         0.0767, -0.0829, -0.0797,  0.0837, -0.0008,  0.0864, -0.0194,  0.0763,\n",
      "         0.0005, -0.0123,  0.0790,  0.0678,  0.0228,  0.0854, -0.0864, -0.0606,\n",
      "         0.0736,  0.0189, -0.0470,  0.0345, -0.0133, -0.0204, -0.0234,  0.0079,\n",
      "         0.0217, -0.0870,  0.0551, -0.0643,  0.0150,  0.0228,  0.0567,  0.0205,\n",
      "         0.0034, -0.0083,  0.0020,  0.0210, -0.0086,  0.0342, -0.0189, -0.0291,\n",
      "        -0.0580,  0.0086, -0.0183,  0.0123, -0.0103,  0.0415, -0.0326, -0.0183,\n",
      "        -0.0052, -0.0161,  0.0718,  0.0560, -0.0007, -0.0822, -0.0167,  0.0088,\n",
      "         0.0727,  0.0188,  0.0099, -0.0232,  0.0081,  0.0252, -0.0183,  0.0736,\n",
      "        -0.0875, -0.0705,  0.0450,  0.0529, -0.0058, -0.0365,  0.0198,  0.0042,\n",
      "        -0.0547,  0.0230,  0.0496, -0.0183, -0.0666, -0.0689, -0.0687, -0.0141,\n",
      "         0.0112,  0.0134,  0.0396,  0.0429, -0.0596,  0.0008, -0.0856,  0.0398,\n",
      "        -0.0011,  0.0708, -0.0216,  0.0135,  0.0234,  0.0227,  0.0103,  0.0552,\n",
      "        -0.0833, -0.0465,  0.0169,  0.0564,  0.0454, -0.0097, -0.0138,  0.0875,\n",
      "         0.0128, -0.0703, -0.0765,  0.0022, -0.0700,  0.0051,  0.0013, -0.0154,\n",
      "         0.0814,  0.0762, -0.0818,  0.0221,  0.0063,  0.0830, -0.0669, -0.0324,\n",
      "        -0.0342, -0.0486, -0.0123, -0.0113,  0.0083, -0.0664, -0.0852, -0.0286,\n",
      "         0.0400,  0.0015, -0.0533, -0.0100, -0.0593,  0.0279,  0.0465, -0.0810,\n",
      "        -0.0789, -0.0439, -0.0711, -0.0828, -0.0387,  0.0550, -0.0743, -0.0058,\n",
      "         0.0822, -0.0119, -0.0198, -0.0735,  0.0128, -0.0212, -0.0582, -0.0622,\n",
      "        -0.0126, -0.0378,  0.0431,  0.0090,  0.0276,  0.0700,  0.0708, -0.0548,\n",
      "         0.0418, -0.0162,  0.0642,  0.0828, -0.0759, -0.0820, -0.0731,  0.0121,\n",
      "         0.0588,  0.0709,  0.0210, -0.0540, -0.0867, -0.0053,  0.0166, -0.0477,\n",
      "        -0.0112,  0.0051, -0.0271, -0.0578, -0.0073, -0.0177,  0.0134, -0.0004],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('fc_final.weight', Parameter containing:\n",
      "tensor([[ 0.0472, -0.0302, -0.0085,  ..., -0.0487, -0.0430,  0.0449],\n",
      "        [ 0.0202,  0.0576, -0.0268,  ..., -0.0276, -0.0334,  0.0359],\n",
      "        [-0.0524,  0.0177,  0.0248,  ..., -0.0173, -0.0161, -0.0017],\n",
      "        [-0.0404, -0.0579, -0.0229,  ..., -0.0201,  0.0420,  0.0043],\n",
      "        [ 0.0184,  0.0301, -0.0071,  ..., -0.0600,  0.0555,  0.0117]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('fc_final.bias', Parameter containing:\n",
      "tensor([ 0.0618, -0.0605, -0.0597, -0.0192, -0.0010], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "('fc_wap0.weight', Parameter containing:\n",
      "tensor([[ 3.7291e-01, -4.4458e-01, -4.3755e-02,  3.4391e-01, -7.2737e-02],\n",
      "        [ 1.2280e-01,  1.3427e-01,  2.1452e-01, -3.4860e-01,  1.2834e-01],\n",
      "        [ 3.8206e-01, -4.4528e-01, -1.8777e-01, -2.8338e-02, -2.6217e-01],\n",
      "        [ 2.9334e-01,  1.8038e-01,  1.3834e-01,  1.9175e-01,  2.0782e-01],\n",
      "        [-8.5392e-02,  3.2079e-01, -1.3422e-01, -4.0403e-01, -2.3458e-01],\n",
      "        [ 3.2337e-01,  3.2368e-01, -1.1684e-01,  3.6930e-01, -3.0831e-01],\n",
      "        [ 2.1491e-01,  1.5638e-01, -2.3402e-01, -1.5434e-02, -2.3061e-01],\n",
      "        [ 2.8756e-01, -4.8865e-02,  8.7331e-02, -1.9059e-01,  2.7535e-02],\n",
      "        [ 3.6005e-01, -3.6371e-01, -2.7036e-02,  5.9889e-03, -2.7516e-01],\n",
      "        [-3.1839e-01,  3.7880e-01,  1.8474e-01, -2.0867e-01,  5.1393e-03],\n",
      "        [-4.0443e-01, -1.5904e-01,  2.0367e-01, -3.6570e-01,  2.4082e-01],\n",
      "        [ 1.7567e-01, -2.0604e-01, -2.8393e-01,  2.9547e-01, -2.7353e-01],\n",
      "        [-1.4293e-03,  8.7977e-03,  1.9564e-01,  2.1823e-02,  3.8455e-01],\n",
      "        [ 2.8605e-01, -3.3175e-01,  4.1654e-01, -3.6800e-02,  3.3588e-01],\n",
      "        [ 1.2070e-01, -1.7248e-01, -4.0787e-01, -3.4206e-01, -2.2069e-01],\n",
      "        [ 2.8356e-01,  1.6401e-01, -6.7495e-02, -4.4268e-01,  1.7382e-01],\n",
      "        [-2.4022e-01,  1.0718e-01, -3.3688e-01, -2.6927e-01,  3.8410e-01],\n",
      "        [-2.7880e-01,  1.2576e-01,  2.8680e-01,  2.4634e-01,  2.8932e-01],\n",
      "        [ 4.3397e-01, -9.9730e-02,  4.4028e-01,  1.7958e-01,  1.5654e-01],\n",
      "        [-2.0298e-02,  1.0889e-01, -3.5532e-01,  1.6622e-01,  3.0522e-01],\n",
      "        [-3.6187e-01,  4.2697e-01,  8.9681e-02, -2.7104e-02, -6.0476e-02],\n",
      "        [ 3.3302e-01, -2.0032e-01, -3.8729e-01, -1.4650e-01, -1.7519e-01],\n",
      "        [-2.9324e-01,  1.7847e-01, -4.3591e-01,  4.4121e-01, -1.1596e-01],\n",
      "        [ 1.8019e-01, -5.6562e-03,  2.0691e-01, -2.9030e-01,  1.0257e-01],\n",
      "        [ 2.8376e-01, -4.2092e-02, -1.2323e-01,  3.1840e-01, -4.2351e-01],\n",
      "        [-7.9669e-02, -3.7906e-01,  8.3048e-02,  1.3747e-01,  6.8477e-02],\n",
      "        [ 2.7384e-01,  1.7524e-01, -4.3679e-01,  2.4078e-01,  2.9960e-01],\n",
      "        [ 7.4053e-02,  2.4995e-01, -2.1894e-01, -1.9736e-01, -1.3114e-01],\n",
      "        [ 3.4375e-03,  3.1310e-01, -8.2568e-02, -4.3768e-01, -4.3239e-01],\n",
      "        [ 2.2245e-01,  1.8071e-01,  1.2654e-01,  4.0215e-01, -2.0267e-01],\n",
      "        [ 1.3022e-01, -1.5012e-01, -8.2055e-02, -1.4047e-01, -3.2619e-01],\n",
      "        [-4.3110e-01,  1.4812e-01, -2.4419e-01,  2.9401e-01, -2.3358e-01],\n",
      "        [-1.6161e-01, -3.3501e-01,  4.1033e-01,  1.5708e-01, -2.1953e-01],\n",
      "        [-3.6908e-01,  3.5234e-01, -2.7561e-01, -2.6572e-01,  1.9263e-01],\n",
      "        [ 4.2984e-01,  2.9374e-01, -1.6163e-01,  1.1074e-01,  2.3491e-01],\n",
      "        [ 3.3456e-01,  1.4604e-01,  2.8307e-01, -3.9282e-01, -1.2599e-01],\n",
      "        [ 3.9209e-01, -3.7297e-01,  2.5487e-01,  2.3355e-01,  3.2738e-01],\n",
      "        [ 1.1464e-01, -1.5043e-01,  3.3672e-01, -3.0894e-01, -2.3543e-01],\n",
      "        [-2.7643e-01,  4.3302e-01, -3.3287e-01, -3.0617e-02, -2.6098e-01],\n",
      "        [-3.9904e-01, -2.5945e-01,  3.2524e-01, -1.3226e-01, -3.3350e-01],\n",
      "        [-3.7794e-01,  3.6564e-01, -4.4340e-01, -4.3960e-01, -1.4475e-01],\n",
      "        [-4.0960e-02, -2.3681e-01, -6.7275e-02, -4.0695e-01,  2.3194e-01],\n",
      "        [ 1.1167e-01, -1.6381e-01, -2.8667e-01, -4.4396e-01, -1.4770e-01],\n",
      "        [-2.8798e-01,  1.6181e-01,  3.4398e-01,  9.4028e-02,  2.8457e-01],\n",
      "        [ 1.9626e-01, -1.8294e-01,  2.7389e-01,  2.8068e-01, -1.3082e-03],\n",
      "        [-1.8142e-01,  2.6001e-02,  4.0116e-01,  4.4241e-01,  1.2477e-01],\n",
      "        [-4.0546e-02,  4.2194e-01, -3.3876e-01,  2.2864e-01, -8.7954e-02],\n",
      "        [-7.0878e-03, -1.2493e-01,  2.1404e-01, -2.0196e-03,  6.1481e-02],\n",
      "        [ 3.4021e-01,  2.6638e-01,  1.0739e-01, -5.7877e-02,  7.4487e-02],\n",
      "        [-2.4413e-01,  2.8013e-02, -2.7924e-01, -2.0549e-01,  1.7820e-01],\n",
      "        [-4.0621e-01,  4.4464e-01,  2.0355e-02, -1.8073e-02, -4.3637e-02],\n",
      "        [ 1.5938e-01, -1.5365e-01, -4.8246e-03, -4.4372e-01, -2.9577e-01],\n",
      "        [ 3.4620e-01, -4.5570e-02,  2.3212e-01, -2.6832e-01, -3.9874e-01],\n",
      "        [ 3.8151e-01, -3.5470e-01, -6.0381e-02,  2.5510e-01,  3.3190e-01],\n",
      "        [-1.4059e-01, -6.7851e-02,  8.6485e-02, -2.2402e-01,  1.7561e-01],\n",
      "        [ 2.4195e-01, -2.8198e-01, -1.9871e-02, -3.6600e-01,  1.6173e-01],\n",
      "        [ 8.4395e-02,  1.6604e-01,  3.3794e-01,  1.1379e-01,  3.5397e-01],\n",
      "        [-1.9148e-01,  2.4666e-03, -4.2477e-02,  1.6738e-01, -4.2284e-01],\n",
      "        [ 3.5203e-01, -3.9709e-01, -1.3448e-01,  3.9049e-01,  6.0177e-02],\n",
      "        [ 2.4684e-01,  1.7093e-01,  4.2211e-01, -3.6970e-01, -4.3258e-01],\n",
      "        [-1.0886e-01, -3.5314e-01, -2.2381e-01,  3.0486e-01, -2.1124e-01],\n",
      "        [-3.1366e-01, -4.2458e-01, -4.1652e-01, -2.1869e-01, -2.6015e-01],\n",
      "        [-2.4176e-01, -1.4407e-01, -1.6266e-01,  4.1141e-02, -1.9175e-01],\n",
      "        [ 2.0158e-01,  1.7432e-02,  2.7129e-04,  4.1541e-02,  4.3120e-01]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('fc_wap0.bias', Parameter containing:\n",
      "tensor([ 0.2863, -0.0557, -0.2709,  0.0936, -0.2758,  0.1966,  0.1503,  0.1481,\n",
      "         0.1558,  0.0041, -0.0472, -0.3124, -0.0429,  0.2989,  0.3552, -0.4053,\n",
      "         0.0334,  0.2676,  0.0320,  0.2306,  0.0143, -0.1212,  0.4049,  0.1176,\n",
      "        -0.3597, -0.3387,  0.3581,  0.0168,  0.1622,  0.0700, -0.0982,  0.0541,\n",
      "         0.3878,  0.3184,  0.4298,  0.3505,  0.1642,  0.1008, -0.0647,  0.1819,\n",
      "        -0.1931,  0.0528, -0.3447, -0.0865, -0.2100, -0.3699, -0.0924,  0.2305,\n",
      "        -0.4171, -0.2427, -0.3885, -0.3266, -0.2623, -0.0437,  0.2277,  0.1079,\n",
      "         0.1469, -0.1378, -0.3172,  0.1079, -0.4472, -0.1901, -0.1822, -0.0229],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('fc_wap1.weight', Parameter containing:\n",
      "tensor([[-6.9247e-02,  7.9935e-02,  6.6473e-02,  1.1274e-01, -1.0338e-01,\n",
      "         -1.2042e-01, -6.8781e-02, -6.3894e-02, -1.2315e-01, -5.4256e-02,\n",
      "         -9.8623e-02, -5.0802e-02,  7.6958e-02, -4.2316e-02, -1.0783e-01,\n",
      "          8.7500e-02, -8.1571e-02, -1.7378e-02, -4.8078e-02,  1.0309e-01,\n",
      "         -7.3725e-03,  6.9163e-02,  1.1989e-02, -5.7481e-02,  1.9799e-02,\n",
      "          6.5290e-02,  7.2944e-03,  5.2027e-02,  6.8104e-02,  1.1403e-01,\n",
      "          1.0549e-01, -2.9668e-02,  1.4978e-02,  4.0969e-02,  3.3632e-02,\n",
      "          1.3625e-02,  5.0860e-02, -1.1626e-02,  1.1755e-01,  7.6026e-02,\n",
      "         -8.7822e-02,  1.1083e-01,  3.5489e-02,  7.5153e-02, -6.4239e-02,\n",
      "          6.6995e-02,  1.0785e-01,  1.1574e-01, -1.0972e-01,  5.0364e-03,\n",
      "         -1.8461e-02, -7.6211e-02,  2.8519e-02, -5.6491e-02, -8.2304e-02,\n",
      "          1.1599e-01, -3.7982e-02,  2.1612e-02,  8.6779e-03,  1.1130e-04,\n",
      "         -4.5270e-02, -7.7041e-02, -9.7771e-03, -6.0915e-02]], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "('fc_wap1.bias', Parameter containing:\n",
      "tensor([-0.0779], device='cuda:0', requires_grad=True))\n",
      "('fc_target0.weight', Parameter containing:\n",
      "tensor([[-0.3429, -0.3553,  0.1312, -0.2693, -0.0735],\n",
      "        [-0.1893, -0.2266,  0.1636, -0.0730, -0.1335],\n",
      "        [ 0.2773,  0.2190, -0.0771, -0.2437, -0.1269],\n",
      "        [-0.3205, -0.0205,  0.2087,  0.0489,  0.2314],\n",
      "        [-0.2462,  0.4161, -0.2998, -0.1012,  0.2875],\n",
      "        [-0.0166,  0.1792, -0.2122, -0.1728, -0.3904],\n",
      "        [ 0.1400,  0.2321, -0.2388,  0.0137, -0.3188],\n",
      "        [-0.2546,  0.2256,  0.3889, -0.3925, -0.2041],\n",
      "        [ 0.1991,  0.2829,  0.3240,  0.3637, -0.3389],\n",
      "        [ 0.2874, -0.0808, -0.4241,  0.4030, -0.0542],\n",
      "        [ 0.1170,  0.2609, -0.3899, -0.4238, -0.1883],\n",
      "        [-0.2882,  0.0162,  0.0358,  0.0391, -0.3239],\n",
      "        [-0.1118,  0.4091, -0.4044, -0.1094, -0.4287],\n",
      "        [-0.3687,  0.4049,  0.1700,  0.1365,  0.2062],\n",
      "        [-0.4468, -0.1087, -0.0599, -0.1044, -0.0511],\n",
      "        [-0.1457,  0.4030, -0.0155,  0.3955, -0.3915],\n",
      "        [ 0.2943,  0.3970, -0.3240,  0.2677,  0.0168],\n",
      "        [-0.2666,  0.1829, -0.2849,  0.1839,  0.3844],\n",
      "        [-0.3777,  0.3869, -0.3484, -0.1766, -0.1092],\n",
      "        [ 0.3852, -0.1385,  0.3994,  0.1366, -0.3239],\n",
      "        [ 0.0690, -0.4443,  0.2237, -0.1072, -0.2358],\n",
      "        [-0.3406,  0.2989, -0.0834, -0.2834, -0.1499],\n",
      "        [ 0.0629, -0.1203, -0.0155, -0.0893, -0.2692],\n",
      "        [ 0.0524,  0.1994,  0.2248,  0.3729,  0.2953],\n",
      "        [-0.3238,  0.3933, -0.1392, -0.3287,  0.0379],\n",
      "        [ 0.0552, -0.1000,  0.1309,  0.0613,  0.3866],\n",
      "        [-0.2643, -0.3612, -0.0644, -0.0821, -0.2000],\n",
      "        [ 0.3530, -0.0422,  0.2071, -0.0158,  0.0134],\n",
      "        [-0.1826, -0.3631,  0.4255,  0.1674, -0.3513],\n",
      "        [ 0.0933,  0.0488, -0.0361,  0.3091, -0.0108],\n",
      "        [-0.3441, -0.2956, -0.2236,  0.0453, -0.0470],\n",
      "        [ 0.0484, -0.0372, -0.2713, -0.1290,  0.3681],\n",
      "        [ 0.2082,  0.3944,  0.2081,  0.2585, -0.3272],\n",
      "        [ 0.1221,  0.1926, -0.3116, -0.1245, -0.1187],\n",
      "        [-0.0390,  0.2229,  0.0070, -0.2552, -0.4466],\n",
      "        [ 0.2055,  0.0178, -0.4248, -0.3977,  0.0346],\n",
      "        [ 0.3201,  0.1474, -0.4420, -0.4149,  0.1371],\n",
      "        [ 0.1487, -0.1244,  0.4317,  0.3737, -0.2766],\n",
      "        [ 0.0990,  0.4177,  0.1688, -0.0364, -0.1758],\n",
      "        [ 0.4451,  0.1621,  0.3516,  0.0420,  0.0220],\n",
      "        [-0.4440,  0.3755,  0.0102,  0.2494, -0.0015],\n",
      "        [-0.0079,  0.3080, -0.3699,  0.3833,  0.2741],\n",
      "        [ 0.1731,  0.0744,  0.2103, -0.0613, -0.0128],\n",
      "        [ 0.0556,  0.4383, -0.3920,  0.3914, -0.1274],\n",
      "        [-0.1051,  0.4235,  0.3033,  0.4188, -0.1010],\n",
      "        [ 0.1192,  0.2229,  0.1336, -0.2376,  0.1082],\n",
      "        [ 0.3193, -0.0707,  0.1176, -0.1146, -0.3662],\n",
      "        [-0.0592, -0.1642,  0.0581, -0.2546, -0.4420],\n",
      "        [ 0.1050, -0.2276,  0.1334, -0.3474,  0.3594],\n",
      "        [-0.2664, -0.3942,  0.0931, -0.1186,  0.4380],\n",
      "        [ 0.1243,  0.1766,  0.2702,  0.0822,  0.0301],\n",
      "        [ 0.4059,  0.1139,  0.3295, -0.1504, -0.1812],\n",
      "        [-0.1214, -0.0253,  0.4126, -0.0856, -0.4371],\n",
      "        [ 0.0550,  0.4249,  0.2544,  0.1470, -0.1614],\n",
      "        [ 0.2165,  0.4095,  0.0308, -0.4115,  0.0388],\n",
      "        [ 0.2755,  0.3947,  0.3680,  0.0882, -0.0640],\n",
      "        [ 0.2465,  0.0135,  0.2621,  0.3432,  0.3353],\n",
      "        [ 0.4339,  0.2383, -0.2620, -0.3467,  0.2038],\n",
      "        [ 0.0604, -0.4287, -0.3190,  0.4217,  0.2911],\n",
      "        [-0.3788,  0.2042, -0.2981, -0.0156,  0.2447],\n",
      "        [ 0.4303, -0.2622, -0.3964, -0.0151, -0.0305],\n",
      "        [-0.3564, -0.2373,  0.1876, -0.3960,  0.1604],\n",
      "        [-0.2494, -0.4040, -0.3658, -0.2705, -0.3126],\n",
      "        [ 0.1269, -0.1379,  0.3091,  0.3079,  0.2704]], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "('fc_target0.bias', Parameter containing:\n",
      "tensor([-0.2313,  0.4232,  0.2028, -0.0667, -0.1953,  0.0955,  0.2952,  0.2184,\n",
      "        -0.4240,  0.3091,  0.0861,  0.1509,  0.3919, -0.0126,  0.2563,  0.0441,\n",
      "        -0.2986,  0.0259, -0.2265, -0.1535,  0.1961,  0.2400,  0.1408, -0.0987,\n",
      "         0.1619,  0.2702, -0.1277,  0.0983,  0.3990, -0.4258, -0.2355,  0.2329,\n",
      "         0.2673, -0.0213, -0.3121,  0.3522, -0.0797,  0.1465,  0.0880,  0.2575,\n",
      "         0.1038,  0.1334, -0.2330, -0.2793, -0.3564, -0.2805,  0.3030, -0.0977,\n",
      "        -0.2473,  0.1658, -0.2341,  0.2312, -0.4460,  0.1446,  0.2358,  0.0213,\n",
      "         0.2591,  0.0555, -0.3333, -0.1183,  0.4346, -0.3699, -0.3318,  0.1153],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('fc_target1.weight', Parameter containing:\n",
      "tensor([[-0.0179,  0.1163,  0.0906, -0.0345, -0.0636,  0.0777, -0.0582,  0.0578,\n",
      "         -0.1102, -0.0378,  0.0445, -0.0146,  0.0010,  0.0239,  0.0269,  0.0884,\n",
      "          0.1114,  0.0870, -0.1212, -0.0762,  0.0582, -0.0947,  0.0584,  0.1142,\n",
      "         -0.0941,  0.0151, -0.0108, -0.0813,  0.0734,  0.0289, -0.1030,  0.1055,\n",
      "          0.0503, -0.0159, -0.0270, -0.0286, -0.0220,  0.0395, -0.0145,  0.0615,\n",
      "          0.0579, -0.0380, -0.0320, -0.0583, -0.0407, -0.0529, -0.0639, -0.0459,\n",
      "          0.1022,  0.0932,  0.0147,  0.0403,  0.0945, -0.0100, -0.0020,  0.0137,\n",
      "         -0.1172,  0.1057,  0.0186,  0.1027,  0.1232, -0.0452, -0.0373,  0.1129]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('fc_target1.bias', Parameter containing:\n",
      "tensor([-0.0215], device='cuda:0', requires_grad=True))\n",
      "('all_fc0.weight', Parameter containing:\n",
      "tensor([[-0.0041, -0.0018,  0.0010,  ..., -0.0037,  0.0060,  0.0044],\n",
      "        [-0.0053, -0.0015, -0.0025,  ..., -0.0047, -0.0033,  0.0037],\n",
      "        [ 0.0019,  0.0044, -0.0052,  ..., -0.0009, -0.0026, -0.0053],\n",
      "        ...,\n",
      "        [-0.0016, -0.0048,  0.0049,  ...,  0.0029, -0.0008, -0.0034],\n",
      "        [-0.0019, -0.0019,  0.0010,  ...,  0.0025,  0.0045, -0.0010],\n",
      "        [-0.0049,  0.0008, -0.0041,  ...,  0.0003,  0.0011,  0.0006]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('all_fc0.bias', Parameter containing:\n",
      "tensor([-0.0040, -0.0035, -0.0040,  ..., -0.0040,  0.0007,  0.0025],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('all_fc1.weight', Parameter containing:\n",
      "tensor([[-0.0203,  0.0199,  0.0134,  ...,  0.0019, -0.0063, -0.0128],\n",
      "        [-0.0090, -0.0111, -0.0120,  ...,  0.0078,  0.0032,  0.0134],\n",
      "        [-0.0085, -0.0009,  0.0130,  ...,  0.0037,  0.0163,  0.0013],\n",
      "        ...,\n",
      "        [-0.0171, -0.0193,  0.0005,  ...,  0.0215,  0.0047,  0.0111],\n",
      "        [-0.0126, -0.0157,  0.0014,  ...,  0.0201, -0.0050, -0.0068],\n",
      "        [-0.0203, -0.0058, -0.0191,  ...,  0.0087, -0.0086,  0.0099]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('all_fc1.bias', Parameter containing:\n",
      "tensor([ 0.0182, -0.0033,  0.0090,  ...,  0.0193,  0.0049, -0.0084],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('all_fc2.weight', Parameter containing:\n",
      "tensor([[-2.2945e-03, -2.8117e-02, -2.7870e-02,  ...,  1.8958e-02,\n",
      "          2.4396e-02, -4.0125e-03],\n",
      "        [-2.9241e-02, -1.9040e-02, -2.4125e-05,  ..., -2.1926e-02,\n",
      "         -2.5800e-02, -1.2121e-03],\n",
      "        [ 9.0915e-03, -2.9386e-02,  1.7627e-02,  ..., -1.1094e-03,\n",
      "         -2.7613e-02,  2.9800e-02],\n",
      "        ...,\n",
      "        [ 1.3459e-02, -1.6167e-02,  2.7709e-02,  ...,  1.3266e-02,\n",
      "         -2.2509e-03,  1.9356e-02],\n",
      "        [ 4.5961e-04, -8.8989e-03, -3.0090e-02,  ..., -1.7264e-02,\n",
      "          1.8719e-02,  2.3535e-02],\n",
      "        [-1.2348e-02, -2.3137e-02, -2.1194e-02,  ..., -6.3838e-03,\n",
      "         -1.6210e-02, -1.1664e-02]], device='cuda:0', requires_grad=True))\n",
      "('all_fc2.bias', Parameter containing:\n",
      "tensor([ 1.3096e-02, -1.3603e-02, -7.8303e-03,  1.7085e-02, -1.5244e-02,\n",
      "        -2.8541e-02, -3.1246e-02,  1.5298e-02,  3.0017e-02, -5.9749e-03,\n",
      "         1.2899e-02, -1.8693e-02, -7.9882e-03, -8.3916e-03, -2.2877e-02,\n",
      "        -1.3782e-02,  2.6487e-02, -1.7026e-02, -9.8711e-03, -3.0198e-02,\n",
      "        -2.4629e-02,  3.0715e-03, -2.5112e-02, -9.1862e-03, -2.5813e-03,\n",
      "         1.5813e-02, -4.7524e-03,  8.1708e-03,  2.1888e-04, -1.4715e-02,\n",
      "        -8.7417e-03, -2.3215e-02, -2.9754e-02, -2.0173e-02,  2.3956e-02,\n",
      "         2.4731e-02, -2.8714e-02,  1.6407e-02, -7.8503e-03, -8.2513e-03,\n",
      "         1.7054e-02, -2.8495e-02, -2.2510e-02,  1.1750e-02, -2.3790e-02,\n",
      "         7.7944e-05, -2.3213e-02, -1.9541e-02, -2.5799e-03,  2.5246e-03,\n",
      "         1.5895e-02,  2.9715e-02, -1.3054e-02, -1.9009e-02,  1.7209e-02,\n",
      "         1.9634e-02,  2.7022e-02, -1.0808e-02,  2.0904e-02, -1.1147e-04,\n",
      "        -2.4293e-02, -2.2804e-02, -2.3710e-03, -2.6939e-02, -2.1062e-02,\n",
      "        -2.9702e-02, -9.2311e-04, -6.7735e-03,  2.2677e-02, -1.7798e-02,\n",
      "        -1.7647e-02,  1.5020e-03,  2.8759e-02, -2.4111e-02,  3.0993e-02,\n",
      "         1.5097e-02, -2.1595e-02, -1.9241e-02, -5.3856e-03, -2.4062e-03,\n",
      "        -6.6386e-03,  1.5024e-03,  3.0822e-02,  3.7668e-03,  4.7921e-03,\n",
      "         1.4232e-02,  2.7436e-02,  1.2042e-04, -1.8085e-02,  2.1706e-02,\n",
      "         1.2554e-02, -2.5930e-02,  1.1788e-02,  2.2840e-02, -7.0439e-03,\n",
      "         1.2994e-02,  2.0374e-02, -5.1890e-03, -2.5867e-02,  2.5178e-02,\n",
      "        -6.5045e-03,  4.6165e-04, -9.8410e-04, -1.0356e-02,  1.2242e-02,\n",
      "        -1.9155e-02,  1.6150e-02, -1.0063e-02,  1.4722e-02,  9.1785e-03,\n",
      "        -9.1405e-03,  2.0237e-02, -1.5083e-02, -1.3064e-02, -2.7006e-03,\n",
      "         8.2195e-03,  2.2815e-02, -2.0223e-02, -2.7228e-02, -2.1559e-02,\n",
      "        -7.8805e-03,  2.0660e-02, -2.5181e-02,  2.5755e-03,  7.5811e-03,\n",
      "         1.0620e-02,  1.8221e-02,  2.8932e-02, -1.7851e-02,  1.7159e-02,\n",
      "        -3.0199e-02, -2.5260e-02, -1.3247e-02, -2.7856e-02, -5.7517e-03,\n",
      "         1.1777e-02,  4.6476e-03, -3.0804e-03, -2.9575e-02,  1.3021e-02,\n",
      "        -8.4165e-03,  2.1493e-04,  1.8421e-02,  2.5461e-04, -9.3361e-03,\n",
      "        -2.9092e-03, -2.5458e-02,  1.6287e-02,  1.4739e-03, -2.4302e-02,\n",
      "        -2.0439e-02,  2.9718e-02,  1.8169e-02, -8.1006e-04,  2.7548e-02,\n",
      "        -1.4653e-02, -5.7379e-03,  5.7440e-03,  2.2525e-02,  1.6691e-02,\n",
      "        -1.9825e-02, -6.7963e-03, -2.8684e-02,  2.7451e-03, -5.9743e-03,\n",
      "         2.5321e-02, -3.6766e-03, -2.3379e-02, -1.6045e-02, -2.4167e-03,\n",
      "         2.1936e-02,  2.1676e-02,  5.7245e-03,  1.4895e-02, -2.0225e-02,\n",
      "         2.1432e-02, -1.8196e-02, -2.1920e-02, -9.8421e-03, -8.1263e-03,\n",
      "         1.7469e-03, -7.0334e-03,  7.4401e-03, -2.4565e-02, -5.3433e-03,\n",
      "         2.8231e-02, -1.7773e-02, -2.6687e-03,  2.6778e-02, -1.8137e-02,\n",
      "        -1.4791e-02, -2.8560e-02,  2.2427e-02,  1.9168e-02, -6.5493e-03,\n",
      "        -2.1568e-02, -1.6137e-02,  2.3659e-03,  1.3328e-02, -2.3103e-02,\n",
      "        -5.6386e-03, -1.7744e-02, -2.3132e-02,  9.1456e-03, -1.7239e-02,\n",
      "        -1.5096e-02, -2.1077e-02,  1.2652e-02,  2.2270e-02,  2.2189e-02,\n",
      "        -1.3351e-02, -2.6627e-02, -7.8035e-03,  4.3504e-05, -1.2126e-02,\n",
      "        -2.4703e-02,  1.1561e-02,  2.0358e-02, -1.6537e-02, -4.6748e-03,\n",
      "        -1.8299e-02,  2.1174e-02,  2.7528e-02, -9.8450e-03, -3.9148e-03,\n",
      "         1.3647e-02,  5.2550e-03,  1.5968e-02,  2.0204e-02, -4.7223e-03,\n",
      "        -3.0639e-02, -1.6963e-02, -1.9696e-02, -1.8864e-02, -1.4520e-02,\n",
      "         4.9834e-04, -7.8325e-03, -2.2966e-02, -2.8917e-02,  3.0314e-02,\n",
      "        -4.6007e-03,  1.6898e-02,  1.7493e-02, -2.3308e-02, -6.9913e-03,\n",
      "        -2.6140e-02, -4.3472e-03,  3.1026e-02,  1.2584e-02,  2.3380e-02,\n",
      "        -2.3603e-02, -2.9736e-02,  3.3168e-03, -1.1491e-02, -2.4828e-02,\n",
      "         2.3869e-02,  2.8094e-02,  2.3804e-02,  1.1371e-02, -2.7522e-02,\n",
      "         2.9375e-02,  1.5045e-03, -1.6601e-02,  1.7818e-02, -1.3265e-02,\n",
      "        -2.1149e-02, -2.0246e-02,  1.9087e-02,  2.7219e-02, -9.0773e-03,\n",
      "        -1.1751e-02,  2.1774e-02,  1.1570e-04, -2.3215e-02,  2.1541e-02,\n",
      "        -5.8258e-03, -2.6993e-02,  2.2301e-02, -2.5220e-03,  1.0524e-03,\n",
      "         8.4231e-03,  2.9814e-02, -5.4326e-03,  1.6801e-02,  1.4745e-02,\n",
      "         1.4912e-02,  2.6938e-02, -1.3253e-02,  9.2570e-03,  1.7148e-02,\n",
      "         2.3285e-03, -2.5871e-02, -1.9901e-02, -2.6145e-02,  2.2198e-02,\n",
      "        -4.3274e-03, -2.7984e-02, -2.2151e-02, -2.1805e-02,  1.1715e-02,\n",
      "        -2.3618e-02,  4.8084e-03,  1.5567e-02, -4.7021e-03, -4.5668e-03,\n",
      "        -2.7316e-02, -2.3496e-02, -1.0246e-03,  2.5060e-02,  2.0619e-02,\n",
      "        -9.4327e-03,  2.3358e-02,  1.8184e-02, -7.0935e-03, -9.6379e-03,\n",
      "        -1.2699e-02, -2.1666e-03,  2.7523e-02,  2.1696e-02, -9.0763e-05,\n",
      "         8.3191e-03,  8.7623e-03,  1.8103e-02,  2.5200e-02,  2.8234e-02,\n",
      "        -8.5841e-03,  2.0059e-02,  1.0051e-02,  1.1559e-02,  2.1765e-02,\n",
      "        -8.6207e-03, -2.0553e-02, -7.0208e-03,  8.7109e-03, -2.5662e-02,\n",
      "         1.8051e-02, -7.9903e-03,  1.6693e-02,  2.8634e-02, -2.0561e-02,\n",
      "        -1.6655e-02,  7.9650e-03,  1.8326e-02, -2.9373e-02, -2.7745e-02,\n",
      "        -1.9258e-02,  6.9457e-03,  1.7655e-02, -2.9124e-02,  9.7865e-03,\n",
      "         1.4554e-02,  3.0569e-02,  5.4675e-03,  2.3887e-02,  5.1705e-03,\n",
      "        -9.3381e-03, -2.5673e-02, -3.0596e-02, -2.0999e-02,  2.3161e-02,\n",
      "         1.8869e-03, -2.6463e-02,  1.3204e-02, -1.8894e-02,  3.5125e-03,\n",
      "         1.2741e-02, -2.8430e-02, -4.7383e-03, -1.2466e-02,  2.3392e-02,\n",
      "         2.2339e-02, -9.9521e-03,  6.1569e-03,  2.9075e-02,  1.7925e-02,\n",
      "        -7.5353e-03,  1.1296e-02, -3.1222e-02,  2.9084e-02, -1.0658e-03,\n",
      "         1.7387e-02,  3.0149e-02,  2.5375e-02, -1.5613e-02,  1.2336e-02,\n",
      "         1.0143e-02, -2.3306e-02, -2.6711e-02,  8.5702e-03, -3.0625e-02,\n",
      "        -2.1561e-02,  1.3237e-03,  1.8686e-02,  2.3565e-02, -1.7782e-02,\n",
      "         1.9600e-02,  6.0891e-03, -8.1278e-04,  1.3587e-02, -1.5030e-03,\n",
      "        -1.2212e-02, -1.6496e-02,  6.4028e-04, -1.2282e-02,  1.3156e-02,\n",
      "         5.0101e-03,  1.9868e-02, -1.2802e-02,  8.7903e-03,  1.6787e-02,\n",
      "        -2.5133e-03, -1.2502e-02,  1.9480e-02,  3.0087e-02, -2.8116e-02,\n",
      "        -1.7493e-02,  2.5278e-02,  1.1214e-02,  7.3135e-03,  2.6566e-02,\n",
      "         3.0912e-02,  1.1041e-02,  1.9949e-02,  2.3785e-02, -8.4294e-03,\n",
      "         1.5592e-03,  4.9690e-03, -2.9564e-02,  1.7304e-02, -1.7938e-02,\n",
      "         1.7418e-02,  1.2082e-02, -2.6764e-02,  1.5999e-02,  2.5024e-02,\n",
      "        -2.0712e-03, -2.3322e-02,  1.6551e-02,  8.9609e-03,  1.1822e-02,\n",
      "        -1.8749e-02, -4.8255e-03, -2.3888e-02,  1.2260e-02,  2.1809e-02,\n",
      "        -1.5081e-02,  8.8751e-03, -2.1659e-02,  5.8671e-03,  2.4469e-02,\n",
      "        -7.3500e-03,  2.5662e-02, -2.8913e-02,  1.8287e-02, -2.2565e-02,\n",
      "        -5.7627e-03, -1.6569e-02,  9.4424e-03,  3.1100e-02,  2.4225e-02,\n",
      "        -5.0527e-03, -6.7376e-03, -1.8880e-02,  3.4804e-03, -1.8840e-02,\n",
      "         5.3297e-04, -1.3779e-02,  1.6645e-02, -6.1473e-03, -1.6558e-02,\n",
      "         8.6949e-03, -2.7793e-02, -2.4685e-02, -1.7876e-02, -1.5566e-02,\n",
      "        -2.3505e-02, -2.1416e-02, -1.8942e-02, -7.0715e-04,  2.9365e-02,\n",
      "         1.8649e-02, -2.9711e-02, -2.7743e-02, -1.2458e-02, -1.2716e-02,\n",
      "         1.0527e-02, -2.5553e-02, -5.6499e-03, -1.9621e-02, -2.7147e-02,\n",
      "         1.3358e-02, -2.2175e-02,  2.4589e-02,  2.5208e-02, -1.3555e-02,\n",
      "        -2.5448e-02, -7.2744e-03,  1.4406e-02,  1.2990e-02, -5.1459e-03,\n",
      "         2.7220e-02, -6.2614e-03, -3.0394e-03,  5.9165e-03,  2.3937e-02,\n",
      "        -1.8721e-02,  1.0463e-02,  2.6823e-02, -2.5073e-02, -2.2857e-02,\n",
      "         2.0557e-03, -5.3956e-03, -2.5266e-02, -1.2786e-02,  2.3168e-02,\n",
      "         2.7718e-02, -2.5675e-03, -1.9837e-02, -3.0725e-02,  3.8764e-03,\n",
      "        -6.9511e-03,  2.3085e-02, -2.2230e-02,  2.1926e-02, -1.5802e-02,\n",
      "        -2.5036e-03,  1.6820e-02,  2.5344e-02,  1.5023e-02,  2.1353e-03,\n",
      "        -7.3287e-03,  2.1199e-02, -7.1626e-03,  2.7772e-02,  2.8055e-02,\n",
      "         1.7427e-02, -2.5893e-02, -1.1342e-02,  9.8661e-03,  2.9639e-02,\n",
      "        -1.3296e-02,  2.3489e-02,  1.5611e-03, -7.6272e-03, -2.2302e-02,\n",
      "        -1.5003e-02, -3.0235e-02,  1.6802e-02,  2.8129e-02, -7.3743e-03,\n",
      "        -2.2497e-02,  1.9871e-02,  5.3129e-03, -7.5127e-03,  1.7311e-02,\n",
      "        -1.5928e-02,  2.9036e-02, -1.8997e-02, -2.3060e-02,  1.6971e-03,\n",
      "         1.6670e-02,  9.8038e-03, -1.9956e-02,  4.2449e-03,  4.2365e-04,\n",
      "         1.2802e-02, -2.0942e-02,  1.7705e-02,  8.6304e-03, -1.2133e-03,\n",
      "         1.9606e-02, -2.3813e-02,  2.7091e-02, -2.8578e-02,  2.9652e-02,\n",
      "        -2.3984e-02,  2.5426e-02, -1.0232e-02, -3.0590e-02, -2.4564e-02,\n",
      "         2.8172e-02,  2.6351e-02,  2.6998e-02,  3.0285e-02,  2.8887e-02,\n",
      "         3.1156e-02,  2.7807e-02,  2.8491e-02, -7.7719e-03, -1.9336e-02,\n",
      "         2.0975e-02, -8.6022e-03,  7.9474e-03, -6.6900e-03, -1.7041e-02,\n",
      "        -2.0847e-02, -1.0704e-03, -1.8110e-02, -2.5190e-02,  8.4292e-03,\n",
      "         2.6352e-02, -2.8337e-02,  3.7527e-03,  1.1177e-03,  2.4508e-02,\n",
      "        -1.9116e-02,  3.1186e-02, -6.4178e-03, -7.4466e-03, -7.1395e-03,\n",
      "         2.2173e-02,  1.6805e-02, -1.5677e-02, -2.5305e-02, -3.4091e-03,\n",
      "         2.8808e-02,  2.9271e-02, -1.5280e-03,  1.6604e-02, -2.6905e-02,\n",
      "        -2.4584e-02, -1.6944e-02,  2.9151e-02, -1.5654e-02,  2.2253e-02,\n",
      "        -1.0633e-02, -2.0802e-02, -2.8162e-02, -4.7718e-04, -8.0441e-03,\n",
      "         2.5808e-02,  2.7425e-03,  1.5195e-02, -4.7822e-03, -1.5700e-02,\n",
      "         2.0681e-02,  9.7763e-03,  1.0391e-02,  1.0665e-02,  2.9455e-02,\n",
      "        -2.5998e-02, -6.8592e-03,  2.3838e-02,  8.5978e-03,  1.2501e-02,\n",
      "         1.1757e-02,  9.3100e-03, -5.8500e-03,  5.1148e-03,  2.1490e-02,\n",
      "         2.9571e-02,  9.5673e-04, -2.9770e-02, -2.5672e-02, -2.9340e-02,\n",
      "        -2.7520e-02, -9.2971e-04, -1.7520e-02,  2.2967e-02,  1.9407e-02,\n",
      "        -2.2225e-02,  2.8352e-02, -1.4803e-02,  1.8305e-02, -2.4636e-02,\n",
      "        -1.6019e-02,  2.7711e-02, -1.5927e-02,  2.1517e-02,  2.2947e-03,\n",
      "         1.6810e-03,  2.8810e-02,  2.5654e-02, -2.8283e-02,  6.0531e-03,\n",
      "         2.1995e-02, -9.7959e-03,  2.2774e-03, -2.3991e-02,  1.1095e-02,\n",
      "        -1.6995e-02, -2.3753e-02,  4.5120e-03, -2.6029e-02,  5.9383e-03,\n",
      "         1.7632e-02,  1.1933e-02,  1.8403e-02, -2.8260e-02,  2.6072e-02,\n",
      "         1.3319e-02,  6.0562e-03, -5.9788e-03, -6.4668e-03,  1.6170e-02,\n",
      "         7.5593e-03, -2.1637e-02,  9.7266e-03,  1.1211e-02, -2.0860e-02,\n",
      "        -8.8051e-03, -3.0003e-02,  2.5705e-03,  2.6669e-02, -2.4672e-02,\n",
      "         2.4737e-02,  1.5474e-02, -1.1084e-02, -2.7276e-02,  1.3258e-02,\n",
      "         1.8316e-03,  6.3334e-03, -2.5351e-02,  1.3037e-02, -8.2945e-03,\n",
      "         2.1951e-02, -1.4525e-02,  1.0635e-02, -1.1857e-02, -1.7997e-02,\n",
      "        -2.4434e-02,  1.4763e-02, -2.3544e-02, -1.0874e-02, -7.4620e-03,\n",
      "        -2.6316e-02, -1.3274e-02,  6.4465e-03, -1.9505e-02, -1.6125e-02,\n",
      "         1.8260e-02,  2.3276e-02,  2.4883e-02, -2.6341e-02,  1.6726e-02,\n",
      "         2.4784e-04,  2.7731e-03,  5.6347e-03, -1.2266e-02,  2.3719e-02,\n",
      "         3.0216e-02,  3.0717e-02,  2.7660e-02,  7.3700e-03, -2.4573e-02,\n",
      "         5.4279e-03,  1.9829e-03, -1.3974e-02, -1.6141e-02,  1.9246e-02,\n",
      "         2.2570e-02, -1.0708e-02,  2.1532e-02,  2.6901e-02,  2.9776e-02,\n",
      "         2.6015e-02, -7.1471e-03,  2.0003e-02, -2.6854e-02, -2.0342e-02,\n",
      "         8.1622e-03,  2.9373e-02,  1.9032e-02,  2.3831e-02,  1.9787e-02,\n",
      "         1.0986e-02, -9.2237e-03,  1.2145e-02, -7.0281e-03,  1.2608e-02,\n",
      "         1.9279e-02, -1.8980e-02, -1.5014e-02, -2.5037e-02, -3.1394e-03,\n",
      "         1.4142e-02,  1.7637e-02,  6.5656e-03,  1.1376e-02, -2.6005e-02,\n",
      "         1.0177e-03, -2.5081e-02,  2.8781e-02,  2.0969e-02, -1.6311e-02,\n",
      "        -1.2322e-02,  3.0013e-02,  2.3053e-02,  2.2284e-02, -1.7223e-03,\n",
      "        -2.3816e-02,  9.8257e-04, -1.4934e-02, -2.0872e-02,  2.7397e-02,\n",
      "         1.2210e-02, -2.5716e-02, -5.8839e-03, -2.2491e-02,  2.7315e-02,\n",
      "         1.3131e-02,  1.0080e-02, -9.3259e-03, -1.5826e-02,  1.6378e-02,\n",
      "         2.8532e-02,  2.0718e-02, -1.1232e-02,  1.2999e-02, -1.3117e-02,\n",
      "        -5.3172e-03, -1.4970e-02,  7.8195e-03, -2.3010e-02,  1.1750e-02,\n",
      "         2.5123e-03,  1.9739e-02,  2.1642e-02,  3.9146e-04, -3.6540e-03,\n",
      "         2.2265e-02, -6.1662e-03, -1.5480e-02,  3.6339e-03, -1.6141e-02,\n",
      "        -1.3283e-02, -1.7261e-03, -1.2461e-02, -2.2879e-02, -6.0486e-03,\n",
      "        -6.7570e-03,  1.3085e-03,  1.1638e-02,  2.8530e-02,  6.2874e-03,\n",
      "        -3.7415e-03, -3.3459e-03,  6.0960e-03, -1.2016e-02, -1.1960e-02,\n",
      "        -2.0325e-04,  2.1155e-02, -4.8352e-03,  1.0106e-02,  3.0131e-02,\n",
      "         2.7597e-02,  3.0330e-02, -1.3447e-02,  1.5827e-02,  2.9424e-02,\n",
      "         2.4084e-02, -2.1154e-02, -2.4243e-02, -2.7992e-02,  2.4482e-02,\n",
      "         1.5720e-02,  3.0322e-02,  1.1682e-03, -1.2553e-02,  1.8616e-02,\n",
      "         2.6136e-02, -1.1781e-02,  1.2078e-02, -2.3627e-02, -5.2703e-03,\n",
      "         1.8798e-02, -5.0598e-03, -2.8067e-02, -2.1371e-02, -1.5854e-02,\n",
      "        -2.7051e-02,  1.2217e-02, -1.9685e-02, -4.9692e-03,  2.3816e-02,\n",
      "        -1.0199e-02, -2.2326e-02, -1.6752e-02,  2.6119e-02, -2.0441e-02,\n",
      "        -1.7377e-02,  1.6487e-02,  2.6717e-02,  1.5113e-02, -1.2012e-02,\n",
      "        -2.1397e-02, -1.2321e-02, -3.1008e-02,  1.8821e-02, -1.9359e-02,\n",
      "        -6.1315e-03,  2.4822e-02,  7.6316e-03, -1.8705e-02, -2.5087e-02,\n",
      "         7.4300e-03, -1.3999e-03, -1.6714e-02, -6.8367e-03, -2.9369e-02,\n",
      "         1.0592e-02,  2.8806e-02,  2.7331e-02,  1.8448e-03, -9.9686e-03,\n",
      "        -2.4524e-02, -4.1493e-03,  2.3353e-02,  1.9171e-02,  1.6691e-02,\n",
      "         1.1969e-02,  5.4380e-04,  2.5317e-02,  1.9585e-02, -1.7736e-03,\n",
      "         2.9316e-02,  2.6366e-02,  1.6979e-02, -2.5170e-02,  6.5609e-03,\n",
      "        -2.4062e-02,  3.9143e-03, -1.1955e-02, -1.3615e-02,  4.1764e-03,\n",
      "         2.1154e-03, -1.1050e-02, -1.7453e-02, -4.5822e-03, -1.6309e-02,\n",
      "        -1.4599e-02, -2.7348e-02,  2.1787e-02, -3.0677e-02,  2.0199e-03,\n",
      "         2.1226e-02,  2.0179e-04,  8.5666e-03,  2.2114e-02,  1.4571e-02,\n",
      "         1.3718e-02,  1.0060e-02,  9.6706e-04, -1.3822e-03, -4.1326e-03,\n",
      "         1.8451e-02,  8.5313e-03,  4.4627e-03,  2.6416e-02,  2.9422e-02,\n",
      "        -2.3890e-02,  1.1857e-02,  2.8606e-02, -3.1127e-02,  2.5601e-03,\n",
      "         1.8742e-03,  2.8866e-02,  2.5789e-03, -6.4073e-03, -2.6617e-02,\n",
      "         2.7733e-02, -1.2619e-02, -2.5109e-02, -1.4632e-04, -7.0573e-03,\n",
      "        -3.0447e-02,  1.7594e-02,  1.7018e-02,  1.6330e-02, -1.9969e-02,\n",
      "         2.2613e-03, -1.8938e-02, -1.5160e-02,  1.0095e-02, -2.7600e-02,\n",
      "         1.5467e-02, -7.6550e-04,  7.9489e-03,  2.6115e-02,  9.3241e-03,\n",
      "         2.1779e-02, -8.5010e-03,  2.1704e-03,  1.2552e-02,  2.7681e-02,\n",
      "         1.7664e-02, -2.5196e-03,  4.7318e-03,  3.0541e-02,  2.4664e-02,\n",
      "        -1.1753e-02,  1.0446e-02, -2.1323e-02,  2.5788e-02, -2.3034e-02,\n",
      "        -1.1156e-02, -6.7554e-04, -2.4129e-02, -1.7572e-02,  4.2001e-03],\n",
      "       device='cuda:0', requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for x in model.named_parameters():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = trading_data.train_batches[0]\n",
    "stocks = [\n",
    "    trading_data.stocksDict[x] for x in trading_data.stock_batches[0]\n",
    "]  # Stocks for the Day\n",
    "hidden_in = torch.stack([x.hidden for x in stocks]).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, output_wap,output_target, hidden, _,x_h  = model(new_x, hidden_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.3343e-01,  8.8778e-01, -4.2058e-01, -6.4730e-01,  8.7384e-01],\n",
       "        [-6.2712e-02,  6.1765e-01,  1.7410e-02, -3.4411e-01,  2.0902e-01],\n",
       "        [-6.1046e-01, -1.9967e-01,  6.1882e-02,  4.8858e-01,  3.1453e-01],\n",
       "        [-1.6122e-01,  2.8558e-01,  1.4333e-01,  7.7996e-01,  1.7925e-01],\n",
       "        [-1.0556e+00, -2.4617e-02,  1.4076e+00,  3.1731e-01, -3.9524e-01],\n",
       "        [-7.3603e-02,  3.2032e-01,  8.8867e-02, -4.2863e-01, -1.4289e-01],\n",
       "        [ 3.0614e-01,  1.5331e-02, -5.9832e-01, -4.5656e-01, -3.0634e-01],\n",
       "        [ 5.7669e-01,  2.6611e-01,  3.3720e-01, -1.9185e-01, -2.4626e-01],\n",
       "        [-5.8149e-01,  1.2545e-01, -4.5483e-01, -1.1380e+00,  1.2437e-01],\n",
       "        [ 3.2330e-01,  4.1436e-01, -1.2951e-01,  2.2822e-01,  2.1477e-01],\n",
       "        [-5.2429e-01,  1.6436e-01, -4.9772e-02, -2.0800e-01,  2.0340e-01],\n",
       "        [ 5.5443e-02, -3.1750e-03, -5.7174e-02,  1.7509e-01, -2.1420e-02],\n",
       "        [ 1.3214e-01,  1.8137e-01,  6.0655e-01,  1.6703e-01, -1.2482e-01],\n",
       "        [-2.4990e-01,  2.2987e-01,  1.0968e+00,  1.8924e-01,  3.3690e-01],\n",
       "        [-2.6713e-01,  2.9137e-01, -4.7012e-01,  7.6616e-02,  3.2921e-01],\n",
       "        [-4.1230e-01,  5.3270e-01, -3.6011e-01,  4.6816e-02,  1.0396e-01],\n",
       "        [ 1.2124e-03,  2.8237e-01,  1.3758e-01, -7.3923e-01,  3.4919e-01],\n",
       "        [ 3.2199e-01, -9.3270e-01, -8.0500e-01, -2.4073e-01, -9.4253e-01],\n",
       "        [-4.9141e-01,  7.8852e-03, -7.5872e-01,  1.8279e-01, -1.6243e-01],\n",
       "        [ 6.5837e-01,  1.4073e-01, -5.3009e-01, -7.6125e-01,  1.7690e-01],\n",
       "        [ 1.2068e-01,  6.9732e-02, -5.8514e-01, -8.0626e-01, -1.1928e-01],\n",
       "        [ 1.8734e-02, -2.9010e-01,  2.4130e-01,  7.4008e-01,  3.1338e-02],\n",
       "        [ 2.8578e-01, -1.6483e-01, -1.8517e-01,  8.2304e-03,  5.4719e-01],\n",
       "        [ 4.0549e-01,  4.6772e-02, -1.3028e-01, -7.5527e-01, -8.1141e-02],\n",
       "        [-1.5511e-01,  3.2411e-01, -2.1539e-01,  7.7946e-01, -1.8476e-01],\n",
       "        [-4.8311e-01, -5.6298e-01, -4.9307e-01, -4.1586e-01,  4.5858e-01],\n",
       "        [ 4.6888e-01,  8.1807e-01,  5.5649e-01,  6.6821e-01, -1.6715e-01],\n",
       "        [ 5.1946e-01, -2.1591e-01, -3.3540e-01,  2.7165e-01, -2.9059e-01],\n",
       "        [ 1.8311e-01,  1.1099e+00, -2.2768e-01,  9.3685e-02, -6.5294e-01],\n",
       "        [ 4.4979e-01,  1.5278e-01,  9.1865e-01,  8.1201e-02,  1.3109e-01],\n",
       "        [ 9.2642e-01,  4.8474e-01, -2.2902e-01,  4.5810e-01, -3.6451e-01],\n",
       "        [-2.9940e-01,  9.2949e-02, -3.8453e-01, -8.9826e-02, -5.0146e-01],\n",
       "        [ 9.2438e-01,  3.7745e-01,  3.8230e-01, -1.1202e-01,  8.2310e-01],\n",
       "        [-3.7542e-01, -1.8934e-01,  1.7159e+00,  2.9751e-01,  6.1827e-02],\n",
       "        [-9.2577e-01, -4.1939e-01,  2.8254e-01,  5.5213e-01, -4.6282e-02],\n",
       "        [ 1.6068e-01,  2.0284e-01,  1.0034e+00,  2.9418e-01, -4.4835e-01],\n",
       "        [-4.9898e-01, -1.9890e-01,  6.4051e-01,  4.4057e-01,  2.5229e-01],\n",
       "        [ 4.6602e-01,  7.3899e-02, -1.7750e-01,  1.7915e-01,  4.1441e-02],\n",
       "        [-3.6582e-01, -1.8863e-01,  7.8170e-01,  6.0251e-02,  1.9602e-01],\n",
       "        [-2.8370e-01,  6.6682e-01,  1.9989e-01,  2.1518e-01, -4.0994e-01],\n",
       "        [-9.7541e-02, -1.1835e-01,  5.2288e-02, -4.7608e-02, -6.2903e-01],\n",
       "        [ 7.1779e-02, -3.8524e-01,  2.4309e-01,  6.2336e-01, -7.4641e-01],\n",
       "        [-4.6859e-01, -5.1763e-01,  2.1585e-01, -5.3040e-01, -2.6376e-01],\n",
       "        [-3.5737e-01,  2.9909e-01, -4.8277e-01,  1.4558e-01, -5.6604e-01],\n",
       "        [-8.8069e-01,  1.2834e-01, -3.8700e-01, -7.4733e-01,  2.2999e-01],\n",
       "        [-2.2137e-01,  2.2598e-01,  7.5328e-01, -4.4584e-01, -1.2462e-01],\n",
       "        [-6.5854e-01,  9.7632e-01, -4.6918e-02,  2.5002e-01, -8.9823e-02],\n",
       "        [-5.4935e-01,  1.6292e-01,  3.7170e-01, -1.5772e-01, -5.3931e-01],\n",
       "        [-6.1812e-01,  5.6799e-01, -4.5439e-01, -9.9396e-01, -5.5362e-01]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49, 128])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_h[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 191, 128])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0715, -0.3007, -0.3952,  ...,  0.1994,  0.2760,  0.6914],\n",
       "        [-0.5203, -0.3244,  0.1136,  ...,  0.2139, -0.2839, -0.0521],\n",
       "        [-0.1306, -0.4088, -0.4979,  ...,  0.1097,  0.1139,  0.3685],\n",
       "        ...,\n",
       "        [ 0.2355, -0.4255, -0.3770,  ...,  0.0821,  0.4428,  0.7968],\n",
       "        [ 0.3527, -0.0424, -0.3906,  ...,  0.2032,  0.5162,  0.7078],\n",
       "        [ 0.1563, -0.3637, -0.5328,  ...,  0.2227,  0.5381,  0.7269]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_h[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49, 24448])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_h.transpose(0,1).view(49,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9971, 0.1248, 0.6241, 0.2072, 0.3945, 0.6816, 0.8236, 0.6424],\n",
       "         [0.5264, 0.4756, 0.9043, 0.3493, 0.4849, 0.1512, 0.5880, 0.7806],\n",
       "         [0.9071, 0.7058, 0.7836, 0.3517, 0.7901, 0.9152, 0.0402, 0.9141],\n",
       "         [0.3636, 0.0574, 0.8023, 0.4773, 0.7985, 0.3932, 0.4654, 0.4337],\n",
       "         [0.3068, 0.9018, 0.9335, 0.4926, 0.0184, 0.2860, 0.7080, 0.1708]],\n",
       "\n",
       "        [[0.7209, 0.0880, 0.2006, 0.6237, 0.2939, 0.5592, 0.2364, 0.0083],\n",
       "         [0.9159, 0.3644, 0.0816, 0.0738, 0.0552, 0.1786, 0.9659, 0.3292],\n",
       "         [0.2573, 0.9850, 0.7967, 0.9886, 0.5799, 0.7047, 0.8740, 0.2954],\n",
       "         [0.3082, 0.5769, 0.9567, 0.2306, 0.0250, 0.8337, 0.5929, 0.0734],\n",
       "         [0.7718, 0.4955, 0.1319, 0.7638, 0.3982, 0.7447, 0.3115, 0.8503]],\n",
       "\n",
       "        [[0.6962, 0.2655, 0.0194, 0.1229, 0.9939, 0.5296, 0.1092, 0.2704],\n",
       "         [0.0443, 0.9956, 0.8027, 0.8369, 0.6567, 0.2214, 0.7674, 0.7787],\n",
       "         [0.5406, 0.5198, 0.1325, 0.6316, 0.1649, 0.2789, 0.7535, 0.1267],\n",
       "         [0.3095, 0.5799, 0.1040, 0.5866, 0.8216, 0.1769, 0.2234, 0.9153],\n",
       "         [0.1079, 0.0340, 0.8048, 0.9187, 0.8484, 0.4402, 0.0054, 0.6345]],\n",
       "\n",
       "        [[0.7649, 0.5629, 0.0060, 0.5607, 0.1823, 0.8384, 0.7839, 0.7994],\n",
       "         [0.5683, 0.0966, 0.9478, 0.0184, 0.0397, 0.2665, 0.7539, 0.4741],\n",
       "         [0.6184, 0.1730, 0.1334, 0.4190, 0.8018, 0.8775, 0.8534, 0.5983],\n",
       "         [0.5724, 0.3269, 0.5955, 0.2013, 0.8879, 0.1401, 0.2085, 0.8486],\n",
       "         [0.1870, 0.0671, 0.6050, 0.3444, 0.2892, 0.4204, 0.5678, 0.3916]],\n",
       "\n",
       "        [[0.5819, 0.8299, 0.5122, 0.4393, 0.9424, 0.3924, 0.2382, 0.8192],\n",
       "         [0.8839, 0.7861, 0.4892, 0.5981, 0.1525, 0.2736, 0.8597, 0.8774],\n",
       "         [0.1547, 0.4455, 0.9739, 0.7498, 0.1568, 0.9187, 0.8507, 0.3908],\n",
       "         [0.2570, 0.1857, 0.4095, 0.5653, 0.4497, 0.5769, 0.6133, 0.5313],\n",
       "         [0.1630, 0.5079, 0.4130, 0.7273, 0.2356, 0.6153, 0.9798, 0.5722]],\n",
       "\n",
       "        [[0.9183, 0.3582, 0.5947, 0.2193, 0.3878, 0.0769, 0.0901, 0.7007],\n",
       "         [0.8222, 0.0655, 0.1642, 0.6826, 0.3753, 0.6011, 0.0867, 0.9048],\n",
       "         [0.0938, 0.9620, 0.1838, 0.7238, 0.3321, 0.5504, 0.2548, 0.3978],\n",
       "         [0.9838, 0.8669, 0.2601, 0.1265, 0.8441, 0.9585, 0.1032, 0.7731],\n",
       "         [0.0930, 0.5029, 0.3960, 0.9393, 0.0733, 0.3876, 0.7446, 0.6860]],\n",
       "\n",
       "        [[0.9879, 0.1685, 0.9008, 0.5366, 0.3259, 0.1680, 0.4694, 0.8939],\n",
       "         [0.8225, 0.7217, 0.2077, 0.4013, 0.4607, 0.1462, 0.7583, 0.1152],\n",
       "         [0.9944, 0.5492, 0.7425, 0.5426, 0.6049, 0.8980, 0.7422, 0.6000],\n",
       "         [0.8476, 0.2344, 0.2006, 0.7503, 0.1528, 0.3629, 0.5118, 0.8208],\n",
       "         [0.5880, 0.7379, 0.2227, 0.2515, 0.1198, 0.7919, 0.8013, 0.3235]],\n",
       "\n",
       "        [[0.9315, 0.4870, 0.4023, 0.7973, 0.8637, 0.0342, 0.7653, 0.2355],\n",
       "         [0.2322, 0.1019, 0.9702, 0.2120, 0.3604, 0.6824, 0.4320, 0.4111],\n",
       "         [0.5230, 0.7083, 0.1788, 0.3423, 0.0375, 0.3650, 0.1651, 0.9002],\n",
       "         [0.2971, 0.2060, 0.5570, 0.1758, 0.9304, 0.2914, 0.0995, 0.5416],\n",
       "         [0.4058, 0.4313, 0.6586, 0.2591, 0.9420, 0.2189, 0.7652, 0.0651]],\n",
       "\n",
       "        [[0.2494, 0.7771, 0.4415, 0.3413, 0.5383, 0.6422, 0.1188, 0.8422],\n",
       "         [0.0032, 0.0711, 0.2413, 0.2288, 0.1284, 0.2859, 0.2957, 0.3746],\n",
       "         [0.6411, 0.9961, 0.2605, 0.5079, 0.1397, 0.5313, 0.3013, 0.1422],\n",
       "         [0.6109, 0.2915, 0.6578, 0.9179, 0.6367, 0.3163, 0.2258, 0.2916],\n",
       "         [0.6376, 0.2886, 0.8415, 0.2016, 0.3176, 0.0093, 0.8147, 0.2378]],\n",
       "\n",
       "        [[0.2287, 0.7353, 0.8100, 0.9965, 0.3683, 0.5774, 0.8625, 0.4009],\n",
       "         [0.9651, 0.6782, 0.4317, 0.1142, 0.1207, 0.2878, 0.4554, 0.5310],\n",
       "         [0.2684, 0.9500, 0.9739, 0.7555, 0.9792, 0.8595, 0.0190, 0.6462],\n",
       "         [0.7553, 0.9404, 0.9443, 0.6873, 0.0496, 0.1382, 0.7497, 0.2643],\n",
       "         [0.6148, 0.4071, 0.1821, 0.3341, 0.6124, 0.3653, 0.1989, 0.6945]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((10,5,8))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9971, 0.1248, 0.6241, 0.2072, 0.3945, 0.6816, 0.8236, 0.6424, 0.7209,\n",
       "         0.0880, 0.2006, 0.6237, 0.2939, 0.5592, 0.2364, 0.0083, 0.6962, 0.2655,\n",
       "         0.0194, 0.1229, 0.9939, 0.5296, 0.1092, 0.2704, 0.7649, 0.5629, 0.0060,\n",
       "         0.5607, 0.1823, 0.8384, 0.7839, 0.7994, 0.5819, 0.8299, 0.5122, 0.4393,\n",
       "         0.9424, 0.3924, 0.2382, 0.8192, 0.9183, 0.3582, 0.5947, 0.2193, 0.3878,\n",
       "         0.0769, 0.0901, 0.7007, 0.9879, 0.1685, 0.9008, 0.5366, 0.3259, 0.1680,\n",
       "         0.4694, 0.8939, 0.9315, 0.4870, 0.4023, 0.7973, 0.8637, 0.0342, 0.7653,\n",
       "         0.2355, 0.2494, 0.7771, 0.4415, 0.3413, 0.5383, 0.6422, 0.1188, 0.8422,\n",
       "         0.2287, 0.7353, 0.8100, 0.9965, 0.3683, 0.5774, 0.8625, 0.4009],\n",
       "        [0.5264, 0.4756, 0.9043, 0.3493, 0.4849, 0.1512, 0.5880, 0.7806, 0.9159,\n",
       "         0.3644, 0.0816, 0.0738, 0.0552, 0.1786, 0.9659, 0.3292, 0.0443, 0.9956,\n",
       "         0.8027, 0.8369, 0.6567, 0.2214, 0.7674, 0.7787, 0.5683, 0.0966, 0.9478,\n",
       "         0.0184, 0.0397, 0.2665, 0.7539, 0.4741, 0.8839, 0.7861, 0.4892, 0.5981,\n",
       "         0.1525, 0.2736, 0.8597, 0.8774, 0.8222, 0.0655, 0.1642, 0.6826, 0.3753,\n",
       "         0.6011, 0.0867, 0.9048, 0.8225, 0.7217, 0.2077, 0.4013, 0.4607, 0.1462,\n",
       "         0.7583, 0.1152, 0.2322, 0.1019, 0.9702, 0.2120, 0.3604, 0.6824, 0.4320,\n",
       "         0.4111, 0.0032, 0.0711, 0.2413, 0.2288, 0.1284, 0.2859, 0.2957, 0.3746,\n",
       "         0.9651, 0.6782, 0.4317, 0.1142, 0.1207, 0.2878, 0.4554, 0.5310],\n",
       "        [0.9071, 0.7058, 0.7836, 0.3517, 0.7901, 0.9152, 0.0402, 0.9141, 0.2573,\n",
       "         0.9850, 0.7967, 0.9886, 0.5799, 0.7047, 0.8740, 0.2954, 0.5406, 0.5198,\n",
       "         0.1325, 0.6316, 0.1649, 0.2789, 0.7535, 0.1267, 0.6184, 0.1730, 0.1334,\n",
       "         0.4190, 0.8018, 0.8775, 0.8534, 0.5983, 0.1547, 0.4455, 0.9739, 0.7498,\n",
       "         0.1568, 0.9187, 0.8507, 0.3908, 0.0938, 0.9620, 0.1838, 0.7238, 0.3321,\n",
       "         0.5504, 0.2548, 0.3978, 0.9944, 0.5492, 0.7425, 0.5426, 0.6049, 0.8980,\n",
       "         0.7422, 0.6000, 0.5230, 0.7083, 0.1788, 0.3423, 0.0375, 0.3650, 0.1651,\n",
       "         0.9002, 0.6411, 0.9961, 0.2605, 0.5079, 0.1397, 0.5313, 0.3013, 0.1422,\n",
       "         0.2684, 0.9500, 0.9739, 0.7555, 0.9792, 0.8595, 0.0190, 0.6462],\n",
       "        [0.3636, 0.0574, 0.8023, 0.4773, 0.7985, 0.3932, 0.4654, 0.4337, 0.3082,\n",
       "         0.5769, 0.9567, 0.2306, 0.0250, 0.8337, 0.5929, 0.0734, 0.3095, 0.5799,\n",
       "         0.1040, 0.5866, 0.8216, 0.1769, 0.2234, 0.9153, 0.5724, 0.3269, 0.5955,\n",
       "         0.2013, 0.8879, 0.1401, 0.2085, 0.8486, 0.2570, 0.1857, 0.4095, 0.5653,\n",
       "         0.4497, 0.5769, 0.6133, 0.5313, 0.9838, 0.8669, 0.2601, 0.1265, 0.8441,\n",
       "         0.9585, 0.1032, 0.7731, 0.8476, 0.2344, 0.2006, 0.7503, 0.1528, 0.3629,\n",
       "         0.5118, 0.8208, 0.2971, 0.2060, 0.5570, 0.1758, 0.9304, 0.2914, 0.0995,\n",
       "         0.5416, 0.6109, 0.2915, 0.6578, 0.9179, 0.6367, 0.3163, 0.2258, 0.2916,\n",
       "         0.7553, 0.9404, 0.9443, 0.6873, 0.0496, 0.1382, 0.7497, 0.2643],\n",
       "        [0.3068, 0.9018, 0.9335, 0.4926, 0.0184, 0.2860, 0.7080, 0.1708, 0.7718,\n",
       "         0.4955, 0.1319, 0.7638, 0.3982, 0.7447, 0.3115, 0.8503, 0.1079, 0.0340,\n",
       "         0.8048, 0.9187, 0.8484, 0.4402, 0.0054, 0.6345, 0.1870, 0.0671, 0.6050,\n",
       "         0.3444, 0.2892, 0.4204, 0.5678, 0.3916, 0.1630, 0.5079, 0.4130, 0.7273,\n",
       "         0.2356, 0.6153, 0.9798, 0.5722, 0.0930, 0.5029, 0.3960, 0.9393, 0.0733,\n",
       "         0.3876, 0.7446, 0.6860, 0.5880, 0.7379, 0.2227, 0.2515, 0.1198, 0.7919,\n",
       "         0.8013, 0.3235, 0.4058, 0.4313, 0.6586, 0.2591, 0.9420, 0.2189, 0.7652,\n",
       "         0.0651, 0.6376, 0.2886, 0.8415, 0.2016, 0.3176, 0.0093, 0.8147, 0.2378,\n",
       "         0.6148, 0.4071, 0.1821, 0.3341, 0.6124, 0.3653, 0.1989, 0.6945]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.transpose(0,1).reshape(5,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\Optiver_v0.3.5.ipynb Cell 64\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/OptiverKaggle/Optiver_v0.3.5.ipynb#Y123sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39;49mrand(\u001b[39m10\u001b[39;49m)\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mcuda:0\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "torch.rand(10).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: upepev1k\n",
      "Sweep URL: https://wandb.ai/nickojelly/Optiver%20Sweeps/sweeps/upepev1k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: t85ko0dj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: SGD\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231208_141040-t85ko0dj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/Optiver%20Sweeps/runs/t85ko0dj' target=\"_blank\">unique-sweep-1</a></strong> to <a href='https://wandb.ai/nickojelly/Optiver%20Sweeps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/Optiver%20Sweeps/sweeps/upepev1k' target=\"_blank\">https://wandb.ai/nickojelly/Optiver%20Sweeps/sweeps/upepev1k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/Optiver%20Sweeps' target=\"_blank\">https://wandb.ai/nickojelly/Optiver%20Sweeps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/Optiver%20Sweeps/sweeps/upepev1k' target=\"_blank\">https://wandb.ai/nickojelly/Optiver%20Sweeps/sweeps/upepev1k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/Optiver%20Sweeps/runs/t85ko0dj' target=\"_blank\">https://wandb.ai/nickojelly/Optiver%20Sweeps/runs/t85ko0dj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Temp\\ipykernel_18468\\3034068021.py\", line 10, in model_pipeline\n",
      "    model = utils.torch_classes.GRUNetV3(input_size,config['hidden_size'],num_layers=config['num_layers']).to('cuda:0')\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1160, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py\", line 213, in _apply\n",
      "    ret = super()._apply(fn, recurse)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 833, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1158, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b8f16177cf4a4a8fc96eece495e9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.066 MB of 0.066 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-sweep-1</strong> at: <a href='https://wandb.ai/nickojelly/Optiver%20Sweeps/runs/t85ko0dj' target=\"_blank\">https://wandb.ai/nickojelly/Optiver%20Sweeps/runs/t85ko0dj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231208_141040-t85ko0dj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\"method\": \"random\"}\n",
    "\n",
    "metric = {\"name\": \"val_epoch_loss\", \"goal\": \"minimize\"}\n",
    "\n",
    "sweep_config[\"metric\"] = metric\n",
    "\n",
    "\n",
    "parameters_dict = {\n",
    "    \"optimizer\": {\"values\": [\"adamW\", \"adam\", \"SGD\", \"RMSprop\"]},\n",
    "    \"f0_layer_size\": {\"values\": [128]},\n",
    "    \"f1_layer_size\": {\"values\": [64]},\n",
    "    \"num_layers\": {\"values\": [2]},\n",
    "    \"hidden_size\": {\"values\": [128, 256, 512]},\n",
    "    \"learning_rate\": {\"values\": [0.001, 0.0005, 0.0001, 0.00005, 0.00001]},\n",
    "    \"epochs\": {\"value\": 500}\n",
    "    # 'batch_norm':{'values':[0,1,2]}\n",
    "}\n",
    "\n",
    "sweep_config[\"parameters\"] = parameters_dict\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"Optiver Sweeps\")\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "wandb.agent(sweep_id, function=model_pipeline, count=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
