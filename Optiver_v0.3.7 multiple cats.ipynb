{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils.public_timeseries_testing_util as optiver2023\n",
    "from torch.nn.utils.rnn import (\n",
    "    pack_padded_sequence,\n",
    "    pack_sequence,\n",
    "    unpack_sequence,\n",
    "    unpad_sequence,\n",
    ")\n",
    "import torch\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import utils.torch_classes\n",
    "from utils.model_saver import model_saver_wandb as model_saver\n",
    "import utils.training_testing\n",
    "import utils.training_testing_dual_optim\n",
    "from itertools import combinations\n",
    "import gc\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import utils.training_testing_double\n",
    "from utils.constants import *\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: \"WANDB_NOTEBOOK_NAME\"=os.path.basename(__file__)\n"
     ]
    }
   ],
   "source": [
    "%env \"WANDB_NOTEBOOK_NAME\" os.path.basename(__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = optiver2023.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\n",
    "        \"cuda:0\"\n",
    "        \n",
    "    )  # you can continue going on here, like cuda:1 cuda:2....etc.\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "weights_df = pd.DataFrame(\n",
    "    data=list(zip(range(0, 201), WEIGHTS)), columns=[\"stock_id\", \"index_weight\"]\n",
    ")\n",
    "train = train.merge(weights_df, on=\"stock_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['stock_id', 'date_id', 'seconds_in_bucket', 'imbalance_size',\n",
       "       'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n",
       "       'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n",
       "       'ask_size', 'wap', 'target', 'time_id', 'row_id', 'index_weight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prev_race(df_in, df_g, rolling_window=10, factor=\"\"):\n",
    "    df = df_in.copy()\n",
    "    original_cols = df_in.columns\n",
    "    df[f\"wap_t-60\"] = df_g[\"wap\"].shift(6)\n",
    "    df[f\"target_t-60\"] = df_g[\"target\"].shift(6)\n",
    "    df[f\"initial_wap\"] = df_g[\"wap_calc\"].transform(\"first\")\n",
    "    df[f\"initial_bid_size\"] = df_g[\"bid_size\"].transform(\"first\")\n",
    "    df[f\"initial_ask_size\"] = df_g[\"ask_size\"].transform(\"first\")\n",
    "    cols = [\"bid_price\", \"ask_price\", \"bid_size\", \"ask_size\", \"wap\"]\n",
    "    for i in cols:\n",
    "        df[f\"{i}_t-60\"] = df_g[i].shift(-6)\n",
    "    for i in cols:\n",
    "        df[f\"{i}_t10\"] = df_g[i].shift(1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_index(df_in, df_g, rolling_window=10, factor=\"\"):\n",
    "    df = df_in.copy()\n",
    "    df[f\"index_wap\"] = df_g[\"wap_weighted\"].transform(\"mean\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_index_2(df_in, df_g, rolling_window=10, factor=\"\"):\n",
    "    df = df_in.copy()\n",
    "    df[f\"index_wap_t-60\"] = df_g[\"index_wap\"].shift(6)\n",
    "    df[f\"index_wap_init\"] = df_g[\"index_wap\"].transform(\"first\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_index_3(df_in, df_g, rolling_window=10, factor=\"\"):\n",
    "    df = df_in.copy()\n",
    "    df[f\"index_wap_t-60\"] = df_g[\"index_wap_move_to_init\"].shift(6)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"wap_calc\"] = (\n",
    "    train[\"bid_price\"] * train[\"ask_size\"] + train[\"ask_price\"] * train[\"bid_size\"]\n",
    ") / (train[\"ask_size\"] + train[\"bid_size\"])\n",
    "\n",
    "train[\"wap_weighted\"] = train[\"wap\"] * train[\"index_weight\"]\n",
    "train_g = train.groupby([\"stock_id\", \"date_id\"])\n",
    "train = generate_prev_race(train, train_g)\n",
    "train[\"delta_wap\"] = train[\"wap\"] / train[\"wap_t-60\"]\n",
    "\n",
    "train_g = train.groupby([\"seconds_in_bucket\", \"date_id\"])\n",
    "train = generate_index(train, train_g)\n",
    "\n",
    "\n",
    "train[\"wap_move_to_init\"] = train[\"wap_calc\"] / train[\"initial_wap\"]\n",
    "train_g = train.groupby([\"date_id\"])\n",
    "train = generate_index_2(train, train_g)\n",
    "\n",
    "train[\"index_wap_move_to_init\"] = train[\"index_wap\"] / train[\"index_wap_init\"]\n",
    "train_g = train.groupby([\"date_id\"])\n",
    "train = generate_index_3(train, train_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"target_calc\"] = (\n",
    "    -(\n",
    "        (train[\"wap_t-60\"] / train[\"wap\"])\n",
    "        - (train[\"index_wap_t-60\"] / train[\"index_wap_move_to_init\"])\n",
    "    )\n",
    "    * 10000\n",
    ")\n",
    "train[\"target_delta\"] = train[\"target_t-60\"] - train[\"target_calc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_vol = pd.read_csv(\"archive/MedianVolV2.csv\")\n",
    "median_vol.index.name = \"stock_id\"\n",
    "median_vol = median_vol[[\"overall_medvol\", \"first5min_medvol\", \"last5min_medvol\"]]\n",
    "median_sizes = (\n",
    "    train.groupby(\"stock_id\")[\"bid_size\"].median()\n",
    "    + train.groupby(\"stock_id\")[\"ask_size\"].median()\n",
    ")\n",
    "std_sizes = (\n",
    "    train.groupby(\"stock_id\")[\"bid_size\"].median()\n",
    "    + train.groupby(\"stock_id\")[\"ask_size\"].median()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"bid_price_target\"] = train[\"bid_price\"] - train[\"bid_price_t-60\"]\n",
    "train[\"bid_price_t-60\"] = train[\"bid_price_target\"] * 10_000\n",
    "\n",
    "train[\"wap_target\"] = train[\"wap\"] - train[\"wap_t-60\"]\n",
    "train[\"wap_price_t-60\"] = train[\"wap_target\"] * 10_000\n",
    "\n",
    "targets = [\"wap\", \"bid_price\", \"ask_price\"]\n",
    "for i in targets:\n",
    "    train[f\"{i}_prev_move\"] = (train[f\"{i}\"] - train[f\"{i}_t10\"]).fillna(0) * 10000\n",
    "\n",
    "train[\"ask_price_target\"] = train[\"ask_price\"] - train[\"ask_price_t-60\"]\n",
    "train[\"ask_price_t-60\"] = train[\"ask_price_target\"] * 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_eng(df):\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\"]]\n",
    "    df = df[cols]\n",
    "    df = df.merge(median_vol, how=\"left\", left_on=\"stock_id\", right_index=True)\n",
    "\n",
    "    df[\"bid_plus_ask_sizes\"] = df[\"bid_size\"] + train[\"ask_size\"]\n",
    "    #     df['median_size'] = df['stock_id'].map(median_sizes.to_dict())\n",
    "    df[\"std_size\"] = df[\"stock_id\"].map(std_sizes.to_dict())\n",
    "    #     df['high_volume'] = np.where(df['bid_plus_ask_sizes'] > df['median_size'], 1, 0)\n",
    "    df[\"imbalance_ratio\"] = df[\"imbalance_size\"] / df[\"matched_size\"]\n",
    "\n",
    "    df[\"imb_s1\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"imb_s2\"] = df.eval(\n",
    "        \"(imbalance_size-matched_size)/(matched_size+imbalance_size)\"\n",
    "    )\n",
    "\n",
    "    df[\"ask_x_size\"] = df.eval(\"ask_size*ask_price\")\n",
    "    df[\"bid_x_size\"] = df.eval(\"bid_size*bid_price\")\n",
    "\n",
    "    df[\"ask_minus_bid\"] = df[\"ask_x_size\"] - df[\"bid_x_size\"]\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "\n",
    "\n",
    "    df[\"bid_size_over_ask_size\"] = df[\"bid_size\"].div(df[\"ask_size\"])\n",
    "    df[\"bid_price_over_ask_price\"] = df[\"bid_price\"].div(df[\"ask_price\"])\n",
    "    \n",
    "\n",
    "    prices = [\n",
    "        \"reference_price\",\n",
    "        \"far_price\",\n",
    "        \"near_price\",\n",
    "        \"ask_price\",\n",
    "        \"bid_price\",\n",
    "        \"wap\",\n",
    "    ]\n",
    "\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_minus_{c[1]}\"] = (df[f\"{c[0]}\"] - df[f\"{c[1]}\"]).astype(np.float32)\n",
    "        df[f\"{c[0]}_times_{c[1]}\"] = (df[f\"{c[0]}\"] * df[f\"{c[1]}\"]).astype(np.float32)\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]}-{c[1]})/({c[0]}+{c[1]})\")\n",
    "\n",
    "    for c in combinations(prices, 3):\n",
    "        max_ = df[list(c)].max(axis=1)\n",
    "        min_ = df[list(c)].min(axis=1)\n",
    "        mid_ = df[list(c)].sum(axis=1) - min_ - max_\n",
    "\n",
    "        df[f\"{c[0]}_{c[1]}_{c[2]}_imb2\"] = (max_ - mid_) / (mid_ - min_)\n",
    "\n",
    "    df.drop(\n",
    "        columns=[\n",
    "            # 'date_id',\n",
    "            \"reference_price_far_price_imb\",\n",
    "            \"reference_price_minus_near_price\",\n",
    "            \"reference_price_near_price_imb\",\n",
    "            \"far_price_near_price_imb\",\n",
    "            \"far_price_ask_price_imb\",\n",
    "            \"far_price_bid_price_imb\",\n",
    "            \"far_price_minus_wap\",\n",
    "            \"std_size\",\n",
    "            \"bid_size_over_ask_size\",\n",
    "            \"ask_price_bid_price_imb\",\n",
    "            \"near_price_times_wap\",\n",
    "        ],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # gc.collect()\n",
    "\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reference_price', 'far_price', 'near_price', 'bid_price', 'ask_price', 'bid_price_t10', 'ask_price_t10', 'bid_price_prev_move', 'ask_price_prev_move', 'mid_price', 'price_spread', 'price_pressure', 'bid_price_over_ask_price', 'reference_price_minus_far_price', 'reference_price_times_far_price', 'reference_price_times_near_price', 'reference_price_minus_ask_price', 'reference_price_times_ask_price', 'reference_price_ask_price_imb', 'reference_price_minus_bid_price', 'reference_price_times_bid_price', 'reference_price_bid_price_imb', 'reference_price_minus_wap', 'reference_price_times_wap', 'reference_price_wap_imb', 'far_price_minus_near_price', 'far_price_times_near_price', 'far_price_minus_ask_price', 'far_price_times_ask_price', 'far_price_minus_bid_price', 'far_price_times_bid_price', 'far_price_times_wap', 'far_price_wap_imb', 'near_price_minus_ask_price', 'near_price_times_ask_price', 'near_price_ask_price_imb', 'near_price_minus_bid_price', 'near_price_times_bid_price', 'near_price_bid_price_imb', 'near_price_minus_wap', 'near_price_wap_imb', 'ask_price_minus_bid_price', 'ask_price_times_bid_price', 'ask_price_minus_wap', 'ask_price_times_wap', 'ask_price_wap_imb', 'bid_price_minus_wap', 'bid_price_times_wap', 'bid_price_wap_imb', 'reference_price_far_price_near_price_imb2', 'reference_price_far_price_ask_price_imb2', 'reference_price_far_price_bid_price_imb2', 'reference_price_far_price_wap_imb2', 'reference_price_near_price_ask_price_imb2', 'reference_price_near_price_bid_price_imb2', 'reference_price_near_price_wap_imb2', 'reference_price_ask_price_bid_price_imb2', 'reference_price_ask_price_wap_imb2', 'reference_price_bid_price_wap_imb2', 'far_price_near_price_ask_price_imb2', 'far_price_near_price_bid_price_imb2', 'far_price_near_price_wap_imb2', 'far_price_ask_price_bid_price_imb2', 'far_price_ask_price_wap_imb2', 'far_price_bid_price_wap_imb2', 'near_price_ask_price_bid_price_imb2', 'near_price_ask_price_wap_imb2', 'near_price_bid_price_wap_imb2', 'ask_price_bid_price_wap_imb2']\n"
     ]
    }
   ],
   "source": [
    "y = train[\"target\"].values\n",
    "X = feat_eng(train)\n",
    "prices = [\n",
    "    c for c in X.columns if (\"price\" in c) and (\"target\" not in c) and (\"60\" not in c)\n",
    "]\n",
    "print(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = [\n",
    "    c for c in X.columns if (\"price\" in c) and (\"target\" not in c) and (\"60\" not in c)\n",
    "]\n",
    "# prices = [c for c in train.columns if 'price' in c]\n",
    "pca_prices = PCA(n_components=1)\n",
    "X[\"pca_prices\"] = pca_prices.fit_transform(X[prices].fillna(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = lgb.Booster(model_file=\"data/lgbm_model_new_t60_train_target.lgb\")\n",
    "X_train = X[lgbm_columns]\n",
    "lgbm_preds = lgbm.predict(X_train)\n",
    "X[\"lgbm_preds_target\"] = lgbm_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = lgb.Booster(model_file=\"data/lgbm_model_new_t60_train_wap.lgb\")\n",
    "X_train = X[lgbm_columns]\n",
    "lgbm_preds = lgbm.predict(X_train)\n",
    "X[\"lgbm_preds_wap\"] = lgbm_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stock_id',\n",
       " 'seconds_in_bucket',\n",
       " 'imbalance_size',\n",
       " 'imbalance_buy_sell_flag',\n",
       " 'reference_price',\n",
       " 'matched_size',\n",
       " 'far_price',\n",
       " 'near_price',\n",
       " 'bid_price',\n",
       " 'bid_size',\n",
       " 'ask_price',\n",
       " 'ask_size',\n",
       " 'wap',\n",
       " 'index_weight',\n",
       " 'wap_calc',\n",
       " 'wap_weighted',\n",
       " 'initial_wap',\n",
       " 'initial_bid_size',\n",
       " 'initial_ask_size',\n",
       " 'bid_price_t10',\n",
       " 'ask_price_t10',\n",
       " 'bid_size_t10',\n",
       " 'ask_size_t10',\n",
       " 'wap_t10',\n",
       " 'index_wap',\n",
       " 'wap_move_to_init',\n",
       " 'index_wap_init',\n",
       " 'index_wap_move_to_init',\n",
       " 'wap_prev_move',\n",
       " 'bid_price_prev_move',\n",
       " 'ask_price_prev_move',\n",
       " 'overall_medvol',\n",
       " 'first5min_medvol',\n",
       " 'last5min_medvol',\n",
       " 'bid_plus_ask_sizes',\n",
       " 'imbalance_ratio',\n",
       " 'imb_s1',\n",
       " 'imb_s2',\n",
       " 'ask_x_size',\n",
       " 'bid_x_size',\n",
       " 'ask_minus_bid',\n",
       " 'bid_price_over_ask_price',\n",
       " 'reference_price_minus_far_price',\n",
       " 'reference_price_times_far_price',\n",
       " 'reference_price_times_near_price',\n",
       " 'reference_price_minus_ask_price',\n",
       " 'reference_price_times_ask_price',\n",
       " 'reference_price_ask_price_imb',\n",
       " 'reference_price_minus_bid_price',\n",
       " 'reference_price_times_bid_price',\n",
       " 'reference_price_bid_price_imb',\n",
       " 'reference_price_minus_wap',\n",
       " 'reference_price_times_wap',\n",
       " 'reference_price_wap_imb',\n",
       " 'far_price_minus_near_price',\n",
       " 'far_price_times_near_price',\n",
       " 'far_price_minus_ask_price',\n",
       " 'far_price_times_ask_price',\n",
       " 'far_price_minus_bid_price',\n",
       " 'far_price_times_bid_price',\n",
       " 'far_price_times_wap',\n",
       " 'far_price_wap_imb',\n",
       " 'near_price_minus_ask_price',\n",
       " 'near_price_times_ask_price',\n",
       " 'near_price_ask_price_imb',\n",
       " 'near_price_minus_bid_price',\n",
       " 'near_price_times_bid_price',\n",
       " 'near_price_bid_price_imb',\n",
       " 'near_price_minus_wap',\n",
       " 'near_price_wap_imb',\n",
       " 'ask_price_minus_bid_price',\n",
       " 'ask_price_times_bid_price',\n",
       " 'ask_price_minus_wap',\n",
       " 'ask_price_times_wap',\n",
       " 'ask_price_wap_imb',\n",
       " 'bid_price_minus_wap',\n",
       " 'bid_price_times_wap',\n",
       " 'bid_price_wap_imb',\n",
       " 'reference_price_far_price_near_price_imb2',\n",
       " 'reference_price_far_price_ask_price_imb2',\n",
       " 'reference_price_far_price_bid_price_imb2',\n",
       " 'reference_price_far_price_wap_imb2',\n",
       " 'reference_price_near_price_ask_price_imb2',\n",
       " 'reference_price_near_price_bid_price_imb2',\n",
       " 'reference_price_near_price_wap_imb2',\n",
       " 'reference_price_ask_price_bid_price_imb2',\n",
       " 'reference_price_ask_price_wap_imb2',\n",
       " 'reference_price_bid_price_wap_imb2',\n",
       " 'far_price_near_price_ask_price_imb2',\n",
       " 'far_price_near_price_bid_price_imb2',\n",
       " 'far_price_near_price_wap_imb2',\n",
       " 'far_price_ask_price_bid_price_imb2',\n",
       " 'far_price_ask_price_wap_imb2',\n",
       " 'far_price_bid_price_wap_imb2',\n",
       " 'near_price_ask_price_bid_price_imb2',\n",
       " 'near_price_ask_price_wap_imb2',\n",
       " 'near_price_bid_price_wap_imb2',\n",
       " 'ask_price_bid_price_wap_imb2',\n",
       " 'pca_prices']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm.feature_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.constants' from 'c:\\\\Users\\\\Nick\\\\Documents\\\\GitHub\\\\OptiverKaggle\\\\utils\\\\constants.py'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(utils.constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.constants' from 'c:\\\\Users\\\\Nick\\\\Documents\\\\GitHub\\\\OptiverKaggle\\\\utils\\\\constants.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del stat_cols\n",
    "from utils.constants import *\n",
    "importlib.reload(utils.constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.constants' from 'c:\\\\Users\\\\Nick\\\\Documents\\\\GitHub\\\\OptiverKaggle\\\\utils\\\\constants.py'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(utils.constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"stats\"] = np.split(\n",
    "    np.nan_to_num(X[stat_cols].to_numpy(), nan=-1), indices_or_sections=len(X)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.975814152508974"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(X) / (1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"stats\"] = X[\"stats\"].apply(lambda x: x.reshape(-1))\n",
    "# print(X[\"stats\"].head(10))\n",
    "# X.to_feather('train_data_with_features.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"wap_category\"] = pd.qcut(X[\"wap_price_t-60\"], q=7)\n",
    "X[\"target_category\"] = pd.qcut(X[\"target\"], q=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"wap_category2\"] = pd.qcut(X[\"wap_price_t-60\"], q=2)\n",
    "X[\"target_category2\"] = pd.qcut(X[\"target\"], q=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"wap_category3\"] = pd.qcut(X[\"wap_price_t-60\"], q=15)\n",
    "X[\"target_category3\"] = pd.qcut(X[\"target\"], q=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = (X[\"wap_category\"].value_counts(sort=False).reset_index().sort_values(\"wap_category\"))\n",
    "weights[\"norm_count\"] = 1 - (weights[\"count\"] / weights[\"count\"].sum())\n",
    "weight = torch.tensor(weights[\"norm_count\"].to_numpy(), device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\AppData\\Local\\Temp\\ipykernel_20988\\340863963.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  means_target = X.groupby('target_category')['target'].median().reset_index().reset_index(names='original_index').rename(columns={'target':'mean_target'})\n",
      "C:\\Users\\Nick\\AppData\\Local\\Temp\\ipykernel_20988\\340863963.py:3: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  means = X.groupby('wap_category')['wap_price_t-60'].median().reset_index().reset_index(names='original_index').rename(columns={'wap_price_t-60':'mean_wap'})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_index</th>\n",
       "      <th>wap_category</th>\n",
       "      <th>mean_wap</th>\n",
       "      <th>wap_cat_name</th>\n",
       "      <th>target_category</th>\n",
       "      <th>mean_target</th>\n",
       "      <th>target_cat_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(-379.691, -9.36]</td>\n",
       "      <td>-14.38</td>\n",
       "      <td>0(-379.691, -9.36]</td>\n",
       "      <td>(-385.291, -7.76]</td>\n",
       "      <td>-11.820197</td>\n",
       "      <td>0(-385.291, -7.76]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>(-9.36, -4.4]</td>\n",
       "      <td>-6.48</td>\n",
       "      <td>1(-9.36, -4.4]</td>\n",
       "      <td>(-7.76, -3.77]</td>\n",
       "      <td>-5.440116</td>\n",
       "      <td>1(-7.76, -3.77]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>(-4.4, -1.22]</td>\n",
       "      <td>-2.69</td>\n",
       "      <td>2(-4.4, -1.22]</td>\n",
       "      <td>(-3.77, -1.2]</td>\n",
       "      <td>-2.400279</td>\n",
       "      <td>2(-3.77, -1.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>(-1.22, 1.25]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3(-1.22, 1.25]</td>\n",
       "      <td>(-1.2, 1.07]</td>\n",
       "      <td>-0.060201</td>\n",
       "      <td>3(-1.2, 1.07]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>(1.25, 4.45]</td>\n",
       "      <td>2.73</td>\n",
       "      <td>4(1.25, 4.45]</td>\n",
       "      <td>(1.07, 3.63]</td>\n",
       "      <td>2.280474</td>\n",
       "      <td>4(1.07, 3.63]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>(4.45, 9.43]</td>\n",
       "      <td>6.56</td>\n",
       "      <td>5(4.45, 9.43]</td>\n",
       "      <td>(3.63, 7.61]</td>\n",
       "      <td>5.300045</td>\n",
       "      <td>5(3.63, 7.61]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>(9.43, 392.99]</td>\n",
       "      <td>14.37</td>\n",
       "      <td>6(9.43, 392.99]</td>\n",
       "      <td>(7.61, 446.07]</td>\n",
       "      <td>11.719465</td>\n",
       "      <td>6(7.61, 446.07]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_index       wap_category  mean_wap        wap_cat_name  \\\n",
       "0               0  (-379.691, -9.36]    -14.38  0(-379.691, -9.36]   \n",
       "1               1      (-9.36, -4.4]     -6.48      1(-9.36, -4.4]   \n",
       "2               2      (-4.4, -1.22]     -2.69      2(-4.4, -1.22]   \n",
       "3               3      (-1.22, 1.25]      0.01      3(-1.22, 1.25]   \n",
       "4               4       (1.25, 4.45]      2.73       4(1.25, 4.45]   \n",
       "5               5       (4.45, 9.43]      6.56       5(4.45, 9.43]   \n",
       "6               6     (9.43, 392.99]     14.37     6(9.43, 392.99]   \n",
       "\n",
       "     target_category  mean_target     target_cat_name  \n",
       "0  (-385.291, -7.76]   -11.820197  0(-385.291, -7.76]  \n",
       "1     (-7.76, -3.77]    -5.440116     1(-7.76, -3.77]  \n",
       "2      (-3.77, -1.2]    -2.400279      2(-3.77, -1.2]  \n",
       "3       (-1.2, 1.07]    -0.060201       3(-1.2, 1.07]  \n",
       "4       (1.07, 3.63]     2.280474       4(1.07, 3.63]  \n",
       "5       (3.63, 7.61]     5.300045       5(3.63, 7.61]  \n",
       "6     (7.61, 446.07]    11.719465     6(7.61, 446.07]  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means_target = X.groupby('target_category')['target'].median().reset_index().reset_index(names='original_index').rename(columns={'target':'mean_target'})\n",
    "means_target['target_cat_name'] = means_target['original_index'].astype(str)+means_target['target_category'].astype(str)\n",
    "means = X.groupby('wap_category')['wap_price_t-60'].median().reset_index().reset_index(names='original_index').rename(columns={'wap_price_t-60':'mean_wap'})\n",
    "means['wap_cat_name'] = means['original_index'].astype(str)+means['wap_category'].astype(str)\n",
    "means = means.merge(means_target,on='original_index')\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "ohe_out = ohe.fit_transform(\n",
    "    X[\"wap_category\"].fillna(X['wap_category'][0]).to_numpy().reshape(-1, 1),\n",
    ")\n",
    "X[\"wap_target_OHE\"] = [x for x in ohe_out]\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "ohe_out = ohe.fit_transform(\n",
    "    X[\"target_category\"].fillna(X['target_category'][0]).to_numpy().reshape(-1, 1),\n",
    ")\n",
    "X[\"target_OHE\"] = [x for x in ohe_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "ohe_out = ohe.fit_transform(\n",
    "    X[\"wap_category2\"].fillna(X['wap_category2'][0]).to_numpy().reshape(-1, 1),\n",
    ")\n",
    "X[\"wap_target_OHE2\"] = [x for x in ohe_out]\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "ohe_out = ohe.fit_transform(\n",
    "    X[\"target_category2\"].fillna(X['target_category2'][0]).to_numpy().reshape(-1, 1),\n",
    ")\n",
    "X[\"target_OHE2\"] = [x for x in ohe_out]\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "ohe_out = ohe.fit_transform(\n",
    "    X[\"wap_category3\"].fillna(X['wap_category3'][0]).to_numpy().reshape(-1, 1),\n",
    ")\n",
    "X[\"wap_target_OHE3\"] = [x for x in ohe_out]\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "ohe_out = ohe.fit_transform(\n",
    "    X[\"target_category3\"].fillna(X['target_category3'][0]).to_numpy().reshape(-1, 1),\n",
    ")\n",
    "X[\"target_OHE3\"] = [x for x in ohe_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function to blur the one-hot encoded vectors\n",
    "def blur_vector(vector, blur_factor=0.2):\n",
    "    idx = np.argmax(vector)  # Get the index of the one-hot class\n",
    "    blurred_vector = np.zeros_like(vector)\n",
    "    \n",
    "    blurred_vector[idx] += (1 - blur_factor)  # Highest probability to the original class\n",
    "    \n",
    "    # Distribute the blur_factor to the neighbors without wrapping around\n",
    "    for i in range(1, len(vector)):\n",
    "        left = idx - i\n",
    "        right = idx + i\n",
    "        \n",
    "        if left >= 0:\n",
    "            blurred_vector[left] += blur_factor / i\n",
    "        if right < len(vector):\n",
    "            blurred_vector[right] += blur_factor / i\n",
    "        if left < 0 and right >= len(vector):\n",
    "            break\n",
    "    \n",
    "    return blurred_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"target_OHE\"] = X[\"target_OHE\"].apply(blur_vector)\n",
    "X[\"wap_target_OHE\"] = X[\"wap_target_OHE\"].apply(blur_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"target_OHE2\"] = X[\"target_OHE2\"].apply(blur_vector)\n",
    "X[\"wap_target_OHE2\"] = X[\"wap_target_OHE2\"].apply(blur_vector)\n",
    "X[\"target_OHE3\"] = X[\"target_OHE3\"].apply(blur_vector)\n",
    "X[\"wap_target_OHE3\"] = X[\"wap_target_OHE3\"].apply(blur_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/95236 [00:00<?, ?it/s]c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\utils\\torch_classes.py:54: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  self.stocksDict[stock_id].wap_daily_ohe[day] = torch.tensor(stock_daily_data['wap_target_OHE'].to_list(), requires_grad=False, device='cuda:0')\n",
      " 10%|█         | 9544/95236 [00:22<02:46, 515.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Targets for day=438, for stock_id=19, Excluding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 47442/95236 [01:52<01:51, 429.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Targets for day=328, for stock_id=101, Excluding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 61099/95236 [02:35<01:29, 379.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Targets for day=35, for stock_id=131, Excluding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 72529/95236 [03:18<00:45, 498.62it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Targets for day=388, for stock_id=158, Excluding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95236/95236 [04:09<00:00, 382.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing stock data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 481/481 [00:03<00:00, 135.90it/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(utils.torch_classes)\n",
    "trading_data = utils.torch_classes.TradingData(X,means)\n",
    "hidden_size = 64\n",
    "# trading_data.generate_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train: 385, Length of test 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [00:01<00:00, 220.80it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 237.18it/s]\n"
     ]
    }
   ],
   "source": [
    "trading_data.generate_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.training_testing' from 'c:\\\\Users\\\\Nick\\\\Documents\\\\GitHub\\\\OptiverKaggle\\\\utils\\\\training_testing.py'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(utils.torch_classes)\n",
    "importlib.reload(utils.training_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_dict = {\n",
    "    \"RMSProp\": optim.RMSprop,\n",
    "    \"Adam\": optim.Adam,\n",
    "    \"RAdam\": optim.RAdam,\n",
    "    \"NAdam\": optim.NAdam,\n",
    "    \"AdamW\": optim.AdamW,\n",
    "    \"SGD\": optim.SGD,\n",
    "    \"Rprop\": optim.Rprop,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(\n",
    "    trading_df=trading_data, config=None, prev_model_file=None, prev_model_version=450\n",
    "):\n",
    "    trading_df = trading_data\n",
    "    with wandb.init(project=\"Optviver_new\", config=config, save_code=True):\n",
    "        wandb.define_metric(\"val_epoch_loss_l1\", summary=\"min\")\n",
    "        wandb.define_metric(\"epoch_l1_loss\", summary=\"min\")\n",
    "        wandb.define_metric(\"L1_loss_wap_epoch\", summary=\"min\")\n",
    "        wandb.define_metric(\"Accuracy\", summary=\"max\")\n",
    "        config = wandb.config\n",
    "        wandb.run.log_code(\"utils\")\n",
    "\n",
    "        input_size = len(trading_df.stocksDict[0].data_daily[0][0])\n",
    "\n",
    "        target_size_ohe = len(trading_df.stocksDict[0].target_daily_ohe[0][0])\n",
    "\n",
    "        print(target_size_ohe)\n",
    "        \n",
    "        model_ohe = utils.torch_classes.GRUNetV5_a(\n",
    "            input_size,\n",
    "            config[\"hidden_size\"],\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            fc0_size=config[\"fc0_size\"],\n",
    "            remove_first_linear=config['remove_first_linear'],\n",
    "            detach = config['detach'],\n",
    "            target_size=target_size_ohe\n",
    "        ).to(\"cuda:0\")\n",
    "\n",
    "        model_reg = utils.torch_classes.GRUNetV5_b(\n",
    "            input_size,\n",
    "            config[\"hidden_size\"],\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            fc0_size=config[\"fc0_size\"],\n",
    "            remove_first_linear=config['remove_first_linear'],\n",
    "            detach = config['detach'],\n",
    "            target_size=target_size_ohe\n",
    "        ).to(\"cuda:0\")\n",
    "\n",
    "        wandb.watch(model_ohe, log='all') \n",
    "        optimizer_ohe = optim_dict[config['optim']](model_ohe.parameters(), lr=config['learning_rate'], weight_decay=0.0001)\n",
    "\n",
    "        wandb.watch(model_ohe, log='all') \n",
    "        # optimizer_reg= optim_dict[config['optim']](model_reg.parameters(), lr=config['learning_rate'], weight_decay=0.01)\n",
    "        optimizer_reg = optim.AdamW(model_reg.parameters(), lr=config['learning_rate_reg'], weight_decay=0.0001)  \n",
    "        \n",
    "        config = wandb.config\n",
    "        print(config)\n",
    "        print(config['ohe_targets'])\n",
    "        if prev_model_file != None:\n",
    "            model_name = prev_model_file\n",
    "            config['prev_model_file'] = prev_model_file            \n",
    "            model_loc = f\"models/{model_name}/{model_name}_{prev_model_version}.pt\"\n",
    "            model_data = torch.load(model_loc, map_location=torch.device(\"cuda:0\"))\n",
    "            print(model_data[\"model_state_dict\"].keys())\n",
    "            print(model_data.keys())\n",
    "\n",
    "            # del_keys = ['fc_final.weight', 'fc_final.bias', 'fc_wap0.weight']\n",
    "            # [model_data['model_state_dict'].pop(k) for k in del_keys]\n",
    "            # model.load_state_dict(model_data[\"model_state_dict\"], strict=False)\n",
    "            # optimizer.load_state_dict(model_data[\"optim\"])\n",
    "\n",
    "        print(model_ohe)\n",
    "        print(model_reg)\n",
    "        trading_df.reset_hidden(\n",
    "            hidden_size=config[\"hidden_size\"], num_layers=config[\"num_layers\"]\n",
    "        )\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        print(criterion)\n",
    "        print(optimizer_reg)\n",
    "        print(optimizer_ohe)    \n",
    "        output = utils.training_testing_dual_optim.train_model(\n",
    "            trading_df, model_ohe,model_reg, config, optimizer_ohe,optimizer_reg, criterion\n",
    "        )\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# arroios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.training_testing_dual_optim' from 'c:\\\\Users\\\\Nick\\\\Documents\\\\GitHub\\\\OptiverKaggle\\\\utils\\\\training_testing_dual_optim.py'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(utils.torch_classes)\n",
    "importlib.reload(utils.training_testing)\n",
    "importlib.reload(utils.training_testing_double)\n",
    "importlib.reload(utils.training_testing_dual_optim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_static = {\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_rate_reg\": 0.00001,\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"batch_norm\": 1,\n",
    "    \"epochs\": 5000,\n",
    "    \"mini_batches\": 30,\n",
    "    \"fc0_size\": 128,\n",
    "    \"note\": \"New Split GRU5,confirming good results from silver plsama\",\n",
    "    'optim': 'RMSProp',\n",
    "    'ohe_targets':means,\n",
    "    'ohe_loss_weight':0.77,\n",
    "    \"l1_squared\": 1,\n",
    "    \"detach\": 1,\n",
    "    \"ohe_squared\": 0.5,\n",
    "    'weigh_dif_factor': 2,\n",
    "    'remove_first_linear':False,\n",
    "}\n",
    "config = config_static\n",
    "torch.cuda.empty_cache()\n",
    "trading_data.detach_hidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_003122-0yb3sdby</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/Optviver_new/runs/0yb3sdby' target=\"_blank\">confused-dust-496</a></strong> to <a href='https://wandb.ai/nickojelly/Optviver_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/Optviver_new' target=\"_blank\">https://wandb.ai/nickojelly/Optviver_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/Optviver_new/runs/0yb3sdby' target=\"_blank\">https://wandb.ai/nickojelly/Optviver_new/runs/0yb3sdby</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'learning_rate': 0.0001, 'learning_rate_reg': 1e-05, 'hidden_size': 128, 'num_layers': 2, 'batch_norm': 1, 'epochs': 5000, 'mini_batches': 30, 'fc0_size': 128, 'note': 'New Split GRU5,confirming good results from silver plsama', 'optim': 'RMSProp', 'ohe_targets': '   original_index       wap_category  mean_wap        wap_cat_name  \\\\\\n0               0  (-379.691, -9.36]    -14.38  0(-379.691, -9.36]   \\n1               1      (-9.36, -4.4]     -6.48      1(-9.36, -4.4]   \\n2               2      (-4.4, -1.22]     -2.69      2(-4.4, -1.22]   \\n3               3      (-1.22, 1.25]      0.01      3(-1.22, 1.25]   \\n4               4       (1.25, 4.45]      2.73       4(1.25, 4.45]   \\n5               5       (4.45, 9.43]      6.56       5(4.45, 9.43]   \\n6               6     (9.43, 392.99]     14.37     6(9.43, 392.99]   \\n\\n     target_category  mean_target     target_cat_name  \\n0  (-385.291, -7.76]   -11.820197  0(-385.291, -7.76]  \\n1     (-7.76, -3.77]    -5.440116     1(-7.76, -3.77]  \\n2      (-3.77, -1.2]    -2.400279      2(-3.77, -1.2]  \\n3       (-1.2, 1.07]    -0.060201       3(-1.2, 1.07]  \\n4       (1.07, 3.63]     2.280474       4(1.07, 3.63]  \\n5       (3.63, 7.61]     5.300045       5(3.63, 7.61]  \\n6     (7.61, 446.07]    11.719465     6(7.61, 446.07]  ', 'ohe_loss_weight': 0.77, 'l1_squared': 1, 'detach': 1, 'ohe_squared': 0.5, 'weigh_dif_factor': 2, 'remove_first_linear': False}\n",
      "   original_index       wap_category  mean_wap        wap_cat_name  \\\n",
      "0               0  (-379.691, -9.36]    -14.38  0(-379.691, -9.36]   \n",
      "1               1      (-9.36, -4.4]     -6.48      1(-9.36, -4.4]   \n",
      "2               2      (-4.4, -1.22]     -2.69      2(-4.4, -1.22]   \n",
      "3               3      (-1.22, 1.25]      0.01      3(-1.22, 1.25]   \n",
      "4               4       (1.25, 4.45]      2.73       4(1.25, 4.45]   \n",
      "5               5       (4.45, 9.43]      6.56       5(4.45, 9.43]   \n",
      "6               6     (9.43, 392.99]     14.37     6(9.43, 392.99]   \n",
      "\n",
      "     target_category  mean_target     target_cat_name  \n",
      "0  (-385.291, -7.76]   -11.820197  0(-385.291, -7.76]  \n",
      "1     (-7.76, -3.77]    -5.440116     1(-7.76, -3.77]  \n",
      "2      (-3.77, -1.2]    -2.400279      2(-3.77, -1.2]  \n",
      "3       (-1.2, 1.07]    -0.060201       3(-1.2, 1.07]  \n",
      "4       (1.07, 3.63]     2.280474       4(1.07, 3.63]  \n",
      "5       (3.63, 7.61]     5.300045       5(3.63, 7.61]  \n",
      "6     (7.61, 446.07]    11.719465     6(7.61, 446.07]  \n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(128, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc0): Linear(in_features=23, out_features=128, bias=True)\n",
      "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc_final): Linear(in_features=128, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3c7a6a551644d5b485b20f4d096970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\dxog9d5v.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\ppv4yjec.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\t79v6o8w.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\aseddmz3.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\k6oen7dj.table.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Temp\\ipykernel_92576\\1574468590.py\", line 70, in model_pipeline\n",
      "    output = utils.training_testing_dual_optim.train_model(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\utils\\training_testing_dual_optim.py\", line 169, in train_model\n",
      "    epoch_loss_ohe.backward(retain_graph=True)\n",
      "  File \"c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be7221d41324a5db2992573c9d3f7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.015 MB of 0.015 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▃▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>L1_loss_wap_epoch</td><td>███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch_loss_ohe</td><td>█▅▇▅▅▆▄▅▅▄▂▄▄▂▅▄▂▅▂▃▄▃▅▅▃▅▄▃▁▃▄▂▃▁▃▄▁▄▄▃</td></tr><tr><td>epoch_loss_reg</td><td>▅▅▅▃▆█▄▅▅▅▁▃▄▂█▅▅▅▁▆█▄█▄▅▅▃▄▁▄▄▅▅▁▆█▁█▄▅</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▅▄▃▃▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▂▂▂▂</td></tr><tr><td>relu_sum</td><td>█▇▇▆▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>█▇▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>█████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▅▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▅▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>▁▄▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.1994</td></tr><tr><td>L1_loss_wap_epoch</td><td>5.96393</td></tr><tr><td>epoch</td><td>40</td></tr><tr><td>epoch_loss_ohe</td><td>4.34965</td></tr><tr><td>epoch_loss_reg</td><td>1.49568</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.34777</td></tr><tr><td>relu_sum</td><td>414425.28125</td></tr><tr><td>target_calc_loss</td><td>6.13685</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>24.58731</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.39431</td></tr><tr><td>val_epoch_loss_wap</td><td>2.75437</td></tr><tr><td>val_loss_wap_ohe</td><td>2.79339</td></tr><tr><td>wap_pred_loss</td><td>7.39566</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">confused-dust-496</strong> at: <a href='https://wandb.ai/nickojelly/Optviver_new/runs/0yb3sdby' target=\"_blank\">https://wandb.ai/nickojelly/Optviver_new/runs/0yb3sdby</a><br/>Synced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_003122-0yb3sdby\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrading_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_static\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 70\u001b[0m, in \u001b[0;36mmodel_pipeline\u001b[1;34m(trading_df, config, prev_model_file, prev_model_version)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(optimizer_reg)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(optimizer_ohe)    \n\u001b[1;32m---> 70\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_testing_dual_optim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrading_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_ohe\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_ohe\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\utils\\training_testing_dual_optim.py:169\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(trading_df, model_ohe, model_reg, config, optimizer_ohe, optimizer_reg, criterion)\u001b[0m\n\u001b[0;32m    167\u001b[0m optimizer_ohe\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    168\u001b[0m optimizer_reg\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 169\u001b[0m \u001b[43mepoch_loss_ohe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m epoch_loss_reg\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    171\u001b[0m optimizer_ohe\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output = model_pipeline(trading_data, config_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ae878ias\n",
      "Sweep URL: https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e872hguf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.8508377954827773\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: RMSProp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_011350-e872hguf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/e872hguf' target=\"_blank\">treasured-sweep-1</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/e872hguf' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/e872hguf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 2, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 256, 'hidden_size': 32, 'l1_squared': 0.5, 'learning_rate': 5e-05, 'learning_rate_reg': 0.0001, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.8508377954827773, 'ohe_squared': 2, 'ohe_targets': 'ahhh', 'optim': 'RMSProp', 'remove_first_linear': True, 'warmup': 10, 'weigh_dif_factor': 0}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(23, 32, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=32, out_features=256, bias=True)\n",
      "  (fc_final): Linear(in_features=256, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0726ad8d894848939a1af673aa40b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\7q38sj5k.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\vrarhtrz.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\ntc6em4w.table.json'\n",
      "Epoch 00027: reducing learning rate of group 0 to 5.0000e-05.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\h347d8hx.table.json'\n",
      "Epoch 00038: reducing learning rate of group 0 to 2.5000e-05.\n",
      "5e-05\n",
      "0.0001\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▁▂▂▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>L1_loss_wap_epoch</td><td>███▇▇▇▆▅▅▄▃▃▂▂▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch_loss_ohe</td><td>█▇█▇▇▇▇▆▆▆▅▅▅▅▄▅▄▃▄▄▄▂▂▄▂▄▄▂▄▃▄▄▃▄▃▃▃▄▁▃</td></tr><tr><td>epoch_loss_reg</td><td>▆▆▁▄▃▃▁▅▄▂▅▃▅▃▆▄▆▆▄▄▄██▁▇▃▂▆▁▅▂▃▃▅▅▄▄▂█▄</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▇▆▅▅▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>relu_sum</td><td>███▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇██████</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>█▇▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>█████████▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>▄▃▂▁▁▁▁▂▂▃▄▄▅▆▆▇▇▇▇█████████████▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.19409</td></tr><tr><td>L1_loss_wap_epoch</td><td>5.9778</td></tr><tr><td>epoch</td><td>37</td></tr><tr><td>epoch_loss_ohe</td><td>4.11081</td></tr><tr><td>epoch_loss_reg</td><td>0.98852</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.29035</td></tr><tr><td>relu_sum</td><td>900736.8125</td></tr><tr><td>target_calc_loss</td><td>6.47452</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>2.36074</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.25626</td></tr><tr><td>val_epoch_loss_wap</td><td>2.75617</td></tr><tr><td>val_loss_wap_ohe</td><td>2.79549</td></tr><tr><td>wap_pred_loss</td><td>7.30822</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">treasured-sweep-1</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/e872hguf' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/e872hguf</a><br/>Synced 5 W&B file(s), 0 media file(s), 20 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_011350-e872hguf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: idh8lhp7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.45980332402985186\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: AdamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_012119-idh8lhp7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/idh8lhp7' target=\"_blank\">splendid-sweep-2</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/idh8lhp7' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/idh8lhp7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 1, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 256, 'hidden_size': 32, 'l1_squared': 2, 'learning_rate': 5e-05, 'learning_rate_reg': 5e-05, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.45980332402985186, 'ohe_squared': 1, 'ohe_targets': 'ahhh', 'optim': 'AdamW', 'remove_first_linear': True, 'warmup': 0, 'weigh_dif_factor': 2}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(23, 32, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=32, out_features=256, bias=True)\n",
      "  (fc_final): Linear(in_features=256, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa4ef3b1bd24a059ac1f603c25c10aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\eenvs00w.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\lo67519e.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\rnc1gj7q.table.json'\n",
      "Epoch 00025: reducing learning rate of group 0 to 2.5000e-05.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\l2f54lzc.table.json'\n",
      "Epoch 00036: reducing learning rate of group 0 to 1.2500e-05.\n",
      "2.5e-05\n",
      "5e-05\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e13bfae0ce4d17aa5446d392d4b4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.012 MB of 0.012 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▂▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇████████</td></tr><tr><td>L1_loss_wap_epoch</td><td>█████▇▇▆▅▄▃▂▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch_loss_ohe</td><td>█▆▄▅▄▄▅▄▅▃▄▄▂▄▃▃▃▃▃▃▂▂▂▂▃▃▃▂▂▂▂▂▁▂▂▄▂▂▁▁</td></tr><tr><td>epoch_loss_reg</td><td>▇▇▁▄▂▃▄▄█▃▄█▁▅▃▃▃▆▆▇▃▃▂▃▄▅▃▃▄▇▆█▁▃▃▄▅▃▃▂</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▇▆▅▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>relu_sum</td><td>███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>█▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>██████████▇▇▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>█▄▃▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.19117</td></tr><tr><td>L1_loss_wap_epoch</td><td>5.99829</td></tr><tr><td>epoch</td><td>35</td></tr><tr><td>epoch_loss_ohe</td><td>13.58357</td></tr><tr><td>epoch_loss_reg</td><td>12.62956</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.30151</td></tr><tr><td>relu_sum</td><td>989670.5</td></tr><tr><td>target_calc_loss</td><td>6.50558</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>25.8278</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.28836</td></tr><tr><td>val_epoch_loss_wap</td><td>2.76007</td></tr><tr><td>val_loss_wap_ohe</td><td>2.80078</td></tr><tr><td>wap_pred_loss</td><td>7.48579</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">splendid-sweep-2</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/idh8lhp7' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/idh8lhp7</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_012119-idh8lhp7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e7atooht with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.1191388348292025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: RMSProp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_012821-e7atooht</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/e7atooht' target=\"_blank\">vocal-sweep-3</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/e7atooht' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/e7atooht</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 1, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 256, 'hidden_size': 64, 'l1_squared': 2, 'learning_rate': 0.0001, 'learning_rate_reg': 1e-05, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.1191388348292025, 'ohe_squared': 1, 'ohe_targets': 'ahhh', 'optim': 'RMSProp', 'remove_first_linear': True, 'warmup': 100, 'weigh_dif_factor': 0}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(23, 64, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "  (fc_final): Linear(in_features=256, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c61b305ed0455b97e7a351bc1243a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\ioytrlxb.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\0oot39o4.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\wa66shqk.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\epr9de5h.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\mniqtga1.table.json'\n",
      "Epoch 00045: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 00050: reducing learning rate of group 0 to 5.0000e-05.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\jo3imjnj.table.json'\n",
      "Epoch 00056: reducing learning rate of group 0 to 2.5000e-06.\n",
      "5e-06\n",
      "1e-05\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8abfa89a5c76479d8bac1d327046c698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.018 MB of 0.026 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.679809…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▃▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇█████████████</td></tr><tr><td>L1_loss_wap_epoch</td><td>███▇▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▄▄▄▅▅</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch_loss_ohe</td><td>██▆▆▅▃▅▅▃▅▄▃▃▄▁▁▄▄▄▄▄▃▃▃▄▄▄▃▄▄▄▂▄▄▂▄▄▁▄▄</td></tr><tr><td>epoch_loss_reg</td><td>▅▃▃▃▄▆▂▃▆▂▃▅▆▃██▄▃▃▁▃▅▃▄▃▃▃▃▃▂▂▆▂▃▅▁▃▇▁▃</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▆▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>relu_sum</td><td>█▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>██████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▇▆▇▇▇▇▆▆▆▅▅▅▅▅▄▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▆▆▆▆▆▆▆▆▆▅▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>▁▃▄▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇████▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.19829</td></tr><tr><td>L1_loss_wap_epoch</td><td>5.97898</td></tr><tr><td>epoch</td><td>55</td></tr><tr><td>epoch_loss_ohe</td><td>0.28344</td></tr><tr><td>epoch_loss_reg</td><td>33.13773</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.30001</td></tr><tr><td>relu_sum</td><td>661719.9375</td></tr><tr><td>target_calc_loss</td><td>6.45793</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>2.3463</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.20042</td></tr><tr><td>val_epoch_loss_wap</td><td>2.75606</td></tr><tr><td>val_loss_wap_ohe</td><td>2.79452</td></tr><tr><td>wap_pred_loss</td><td>7.49496</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vocal-sweep-3</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/e7atooht' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/e7atooht</a><br/>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_012821-e7atooht\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q9t4pq4i with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.6535386216887106\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: RMSProp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_013851-q9t4pq4i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/q9t4pq4i' target=\"_blank\">wise-sweep-4</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/q9t4pq4i' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/q9t4pq4i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 0, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 256, 'hidden_size': 256, 'l1_squared': 2, 'learning_rate': 5e-05, 'learning_rate_reg': 5e-06, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.6535386216887106, 'ohe_squared': 1, 'ohe_targets': 'ahhh', 'optim': 'RMSProp', 'remove_first_linear': False, 'warmup': 20, 'weigh_dif_factor': 2}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(256, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc0): Linear(in_features=23, out_features=256, bias=True)\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc_final): Linear(in_features=256, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5e-06\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734c3ac590eb47c5b0b528a67ef42bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\an2gs3ix.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\zoyx3j9b.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\iu09moux.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\03fk7pc2.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\yr9mkdsq.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\fsxofwsm.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\w7yh9f7g.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\534brcfr.table.json'\n",
      "Epoch 00073: reducing learning rate of group 0 to 2.5000e-06.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\pcxbvuvn.table.json'\n",
      "Epoch 00084: reducing learning rate of group 0 to 1.2500e-06.\n",
      "2.5e-06\n",
      "5e-06\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3832169226344f10a4c994c2fa06f001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.026 MB of 0.026 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▃▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>L1_loss_wap_epoch</td><td>█▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch_loss_ohe</td><td>▇▆▅▆▃▄▂▅▄▃▆▄▅▄▇█▄▄▅▂▄▂▄▄▃▆▄▅▄▆█▄▁▅▂▄▂▄▄▃</td></tr><tr><td>epoch_loss_reg</td><td>▄▃▂▄▁▂▁▅▆▂█▃▆▄▅▄▃▂▃▁▂▁▂▅▁█▃▃▄▅▃▂▂▃▁▂▁▂▅▁</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▅▃▂▂▁▁▁▁▂▂▂▃▃▃▃▄▄▅▄▄▅▅▅▆▆▆▆▆▆▆▆▇▇▆▇▇▇▆▇</td></tr><tr><td>relu_sum</td><td>█▆▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>█▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▇▆▆▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>▁▇▅▄▄▄▃▃▃▃▃▃▄▄▄▅▅▆▅▅▆▅▆▆▆▇▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.21026</td></tr><tr><td>L1_loss_wap_epoch</td><td>5.95568</td></tr><tr><td>epoch</td><td>83</td></tr><tr><td>epoch_loss_ohe</td><td>20.01069</td></tr><tr><td>epoch_loss_reg</td><td>5.07578</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.39132</td></tr><tr><td>relu_sum</td><td>665041.0625</td></tr><tr><td>target_calc_loss</td><td>6.34236</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>24.07924</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.19842</td></tr><tr><td>val_epoch_loss_wap</td><td>2.7503</td></tr><tr><td>val_loss_wap_ohe</td><td>2.78691</td></tr><tr><td>wap_pred_loss</td><td>7.62075</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wise-sweep-4</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/q9t4pq4i' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/q9t4pq4i</a><br/>Synced 5 W&B file(s), 0 media file(s), 9 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_013851-q9t4pq4i\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zjmgq5zc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.4272821769383866\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: AdamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_025115-zjmgq5zc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/zjmgq5zc' target=\"_blank\">fresh-sweep-5</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/zjmgq5zc' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/zjmgq5zc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 0, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 128, 'hidden_size': 128, 'l1_squared': 0.5, 'learning_rate': 1e-05, 'learning_rate_reg': 1e-05, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.4272821769383866, 'ohe_squared': 1, 'ohe_targets': 'ahhh', 'optim': 'AdamW', 'remove_first_linear': False, 'warmup': 100, 'weigh_dif_factor': 2}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(128, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc0): Linear(in_features=23, out_features=128, bias=True)\n",
      "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc_final): Linear(in_features=128, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432899be5383421e84041cfb6b876547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\t04u5cx0.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\f51qfztl.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\0hnh27k8.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\hwgty3oa.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\q0mc49xu.table.json'\n",
      "Epoch 00047: reducing learning rate of group 0 to 5.0000e-06.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\6oymnfka.table.json'\n",
      "Epoch 00058: reducing learning rate of group 0 to 2.5000e-06.\n",
      "5e-06\n",
      "1e-05\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc5eadabbb734aab86c6b081a2a30999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.018 MB of 0.018 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▂▃▃▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>L1_loss_wap_epoch</td><td>███████▇▇▇▇▇▆▆▆▅▅▄▄▃▃▂▂▁▁▁▁▁▁▂▂▃▃▄▄▄▄▅▅▅</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>epoch_loss_ohe</td><td>█▇▆▆▆▇▄▅▆▄▅▅▃▄▅▃▄▄▄▄▄▃▅▃▃▃▄▄▁▃▄▂▃▃▁▃▄▁▃▃</td></tr><tr><td>epoch_loss_reg</td><td>▅▄▄▃▅▇▂▃▇▂▄▆▁▃█▁▄▄▆▅▆▄▅▄▄▃▄▇▂▃▇▂▄▆▁▄█▁▄▄</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▇▇▆▅▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>relu_sum</td><td>███▇▇▇▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>█▇▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>█████████████▇▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>█▂▁▂▂▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.19488</td></tr><tr><td>L1_loss_wap_epoch</td><td>5.9908</td></tr><tr><td>epoch</td><td>57</td></tr><tr><td>epoch_loss_ohe</td><td>12.54656</td></tr><tr><td>epoch_loss_reg</td><td>1.93565</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.37112</td></tr><tr><td>relu_sum</td><td>461831.25</td></tr><tr><td>target_calc_loss</td><td>6.42516</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>26.01281</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.30207</td></tr><tr><td>val_epoch_loss_wap</td><td>2.7579</td></tr><tr><td>val_loss_wap_ohe</td><td>2.79781</td></tr><tr><td>wap_pred_loss</td><td>7.74606</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-sweep-5</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/zjmgq5zc' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/zjmgq5zc</a><br/>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_025115-zjmgq5zc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jb2nrvq5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.5340307201906076\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: AdamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_031134-jb2nrvq5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/jb2nrvq5' target=\"_blank\">splendid-sweep-6</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/jb2nrvq5' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/jb2nrvq5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 0, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 128, 'hidden_size': 32, 'l1_squared': 2, 'learning_rate': 5e-06, 'learning_rate_reg': 0.0001, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.5340307201906076, 'ohe_squared': 1, 'ohe_targets': 'ahhh', 'optim': 'AdamW', 'remove_first_linear': False, 'warmup': 5, 'weigh_dif_factor': 0}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(32, 32, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc0): Linear(in_features=23, out_features=32, bias=True)\n",
      "  (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "  (fc_final): Linear(in_features=128, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5e-06\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2d05327fca4a9eb05d2d0e48f5a071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\ir2pqu96.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\5wvsn76s.table.json'\n",
      "Epoch 00012: reducing learning rate of group 0 to 5.0000e-05.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\7ffupdah.table.json'\n",
      "Epoch 00023: reducing learning rate of group 0 to 2.5000e-05.\n",
      "5e-05\n",
      "0.0001\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d989d296518343f7b133e03aa79b2099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▁▁▁▁▁▁▂▂▂▃▃▄▄▅▄▄▄▄▄▅▆█</td></tr><tr><td>L1_loss_wap_epoch</td><td>▅▃▁▁▂▂▂▂▃▅▅▅▅▆▆▇▇▇█▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch_loss_ohe</td><td>██▆▇▇█▇▇▆▃▆▅▆▅▅▄▅▆▅▄▅▂▅▃▅▄▂▄▂▅▄▂▃▃▃▁▄▃▂▃</td></tr><tr><td>epoch_loss_reg</td><td>▃▂▆▄▃▁▂▂▂█▃▄▃▃▃▄▃▁▃▃▂▇▃▆▁▃▆▃▅▂▃▆▃▃▄█▂▄▅▄</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▇▇▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>relu_sum</td><td>▆▇▇█████████▇▇▇▆▅▅▄▃▃▂▁</td></tr><tr><td>target_calc_loss</td><td>▇██▇▆▆▆▅▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>█▇▇▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>█▆▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▇▆▆▅▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▇▇▆▅▅▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>▁▁▁▁▂▂▂▃▃▄▄▅▅▆▇▇█████▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.14533</td></tr><tr><td>L1_loss_wap_epoch</td><td>6.00785</td></tr><tr><td>epoch</td><td>22</td></tr><tr><td>epoch_loss_ohe</td><td>1.33434</td></tr><tr><td>epoch_loss_reg</td><td>9.44445</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.46096</td></tr><tr><td>relu_sum</td><td>567144.25</td></tr><tr><td>target_calc_loss</td><td>6.07111</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>2.50017</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.49176</td></tr><tr><td>val_epoch_loss_wap</td><td>2.7904</td></tr><tr><td>val_loss_wap_ohe</td><td>2.83481</td></tr><tr><td>wap_pred_loss</td><td>10.35201</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">splendid-sweep-6</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/jb2nrvq5' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/jb2nrvq5</a><br/>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_031134-jb2nrvq5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gsb8su53 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.006191730821211783\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: AdamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_031614-gsb8su53</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/gsb8su53' target=\"_blank\">fine-sweep-7</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/gsb8su53' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/gsb8su53</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 0, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 128, 'hidden_size': 128, 'l1_squared': 0.5, 'learning_rate': 0.0001, 'learning_rate_reg': 5e-05, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.006191730821211783, 'ohe_squared': 0.5, 'ohe_targets': 'ahhh', 'optim': 'AdamW', 'remove_first_linear': False, 'warmup': 100, 'weigh_dif_factor': 2}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(128, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc0): Linear(in_features=23, out_features=128, bias=True)\n",
      "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc_final): Linear(in_features=128, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7cc80337d62452eb32c948eba48fadc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\090edqru.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\xarg9ltp.table.json'\n",
      "Epoch 00018: reducing learning rate of group 0 to 2.5000e-05.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\migzsseh.table.json'\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.2500e-05.\n",
      "2.5e-05\n",
      "5e-05\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3033e29288614e9a8ec1d4af8724ddcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.017 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.540905…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▃▅▆▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>L1_loss_wap_epoch</td><td>██▇▅▃▁▁▂▄▅▆▇▇▇▇▆▆▆▆▆▆▆▆▆▆▅▆▅▅</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch_loss_ohe</td><td>█▇█▆▄▆▇▅▂▆▅▃▂▄▃▂▄▅▂▃▄▅▅▄▅▄▃▅▅▄▁▅▄▂▁▄▂▁▃▅</td></tr><tr><td>epoch_loss_reg</td><td>▆▅▄▄▄▃██▁▄▇▃▂▅▆▁▅▃▂▃▃▅▅▅▃▃▃▂▇▇▁▄▆▃▂▄▅▁▅▃</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▃▁▁▁▁▂▁▂▂▂▂▂▂▂▂▂▃▂▂▃▂▃▃▃▃▃▃▃</td></tr><tr><td>relu_sum</td><td>█▆▅▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▁▂▃▄▅▆▇▇▇▇████████████████</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>█▇▆▅▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>████▇▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▅▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▅▅▅▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>▁▂▅▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.19745</td></tr><tr><td>L1_loss_wap_epoch</td><td>5.98666</td></tr><tr><td>epoch</td><td>28</td></tr><tr><td>epoch_loss_ohe</td><td>0.4129</td></tr><tr><td>epoch_loss_reg</td><td>2.54724</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.3866</td></tr><tr><td>relu_sum</td><td>432560.0625</td></tr><tr><td>target_calc_loss</td><td>6.6071</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>24.49914</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.12502</td></tr><tr><td>val_epoch_loss_wap</td><td>2.7539</td></tr><tr><td>val_loss_wap_ohe</td><td>2.79208</td></tr><tr><td>wap_pred_loss</td><td>7.0389</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fine-sweep-7</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/gsb8su53' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/gsb8su53</a><br/>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_031614-gsb8su53\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xyt4vun0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.7001363558153945\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: RMSProp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_032622-xyt4vun0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/xyt4vun0' target=\"_blank\">fluent-sweep-8</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/xyt4vun0' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/xyt4vun0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 2, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 256, 'hidden_size': 32, 'l1_squared': 0.5, 'learning_rate': 1e-05, 'learning_rate_reg': 5e-05, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.7001363558153945, 'ohe_squared': 0.5, 'ohe_targets': 'ahhh', 'optim': 'RMSProp', 'remove_first_linear': True, 'warmup': 10, 'weigh_dif_factor': 2}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(23, 32, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=32, out_features=256, bias=True)\n",
      "  (fc_final): Linear(in_features=256, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1236c77f99554ced8395e6fc8db7506c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\jqvzddyr.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\fnl739k5.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\pdneuixl.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\ma6d6c7i.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\1gfkyle9.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\wtlxcsn7.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\gn5lna7h.table.json'\n",
      "Epoch 00069: reducing learning rate of group 0 to 5.0000e-06.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\0fpz4s9o.table.json'\n",
      "Epoch 00071: reducing learning rate of group 0 to 2.5000e-05.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\smj1xuml.table.json'\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.2500e-05.\n",
      "2.5e-05\n",
      "5e-05\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78121de1b1104e809005e9ef25a1a96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.026 MB of 0.026 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇█████████</td></tr><tr><td>L1_loss_wap_epoch</td><td>█▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch_loss_ohe</td><td>█▇▆▇▄▆▄▆▆▅▇▆▅▃▆▄▅▄▅▆▅▆▄▅▁▃▁▄▃▄▄▅▃▁▃▂▂▄▃▃</td></tr><tr><td>epoch_loss_reg</td><td>▅▄▃▄▂▃▂▄▆▄█▅▄▁▆▄▄▄▄▇▅▆▃▅▁▂▁▄▆▅▆▅▃▂▄▃▃▇▃▆</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>relu_sum</td><td>████▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>█▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>████████▇▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▇▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▆▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>█▄▂▁▁▁▁▁▁▂▂▂▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.18134</td></tr><tr><td>L1_loss_wap_epoch</td><td>5.97352</td></tr><tr><td>epoch</td><td>81</td></tr><tr><td>epoch_loss_ohe</td><td>4.55236</td></tr><tr><td>epoch_loss_reg</td><td>1.40477</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.30893</td></tr><tr><td>relu_sum</td><td>1024416.875</td></tr><tr><td>target_calc_loss</td><td>6.26383</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>27.34891</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.38743</td></tr><tr><td>val_epoch_loss_wap</td><td>2.76205</td></tr><tr><td>val_loss_wap_ohe</td><td>2.80145</td></tr><tr><td>wap_pred_loss</td><td>7.28371</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fluent-sweep-8</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/xyt4vun0' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/xyt4vun0</a><br/>Synced 5 W&B file(s), 0 media file(s), 9 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_032622-xyt4vun0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hbx7iml6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.42261067504219335\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: AdamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_034136-hbx7iml6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/hbx7iml6' target=\"_blank\">crisp-sweep-9</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/hbx7iml6' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/hbx7iml6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 2, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 256, 'hidden_size': 256, 'l1_squared': 2, 'learning_rate': 5e-06, 'learning_rate_reg': 1e-05, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.42261067504219335, 'ohe_squared': 0.5, 'ohe_targets': 'ahhh', 'optim': 'AdamW', 'remove_first_linear': False, 'warmup': 5, 'weigh_dif_factor': 2}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(256, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc0): Linear(in_features=23, out_features=256, bias=True)\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc_final): Linear(in_features=256, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5e-06\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0ebcf9df39406a99231a75c31eb743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\ewq07oti.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\fdxpo562.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\1f4b3nbe.table.json'\n",
      "Epoch 00030: reducing learning rate of group 0 to 2.5000e-06.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\rbgohz4b.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\6prjo1nw.table.json'\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.2500e-06.\n",
      "Epoch 00041: reducing learning rate of group 0 to 5.0000e-06.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\e0dn8qpq.table.json'\n",
      "Epoch 00052: reducing learning rate of group 0 to 2.5000e-06.\n",
      "5e-06\n",
      "1e-05\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▁▂▂▄▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>L1_loss_wap_epoch</td><td>█████▇▇▇▇▆▆▅▅▄▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch_loss_ohe</td><td>█▇▇▅█▇▅▆▅▆▆▅▆▅▄▃▆▅▅▂▃▄▂▅▄▄▃▃▃▁▄▁▃▄▃▄▄▄▁▂</td></tr><tr><td>epoch_loss_reg</td><td>▅▃▃▂█▄▃▃▃▄▆▃▄▆▃▂█▄▃▂▃▄▂▅▅▆▃▂▂▁▃▂▆▅▂▅▅▄▁▂</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▇▇▆▅▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>relu_sum</td><td>████▇▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>▇██▇▆▆▆▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>████████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▆▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▆▅▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>▁▃▃▃▃▃▃▄▅▅▆▆▆▇▇▇▇███████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.18977</td></tr><tr><td>L1_loss_wap_epoch</td><td>5.98018</td></tr><tr><td>epoch</td><td>51</td></tr><tr><td>epoch_loss_ohe</td><td>3.55739</td></tr><tr><td>epoch_loss_reg</td><td>14.39977</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.39315</td></tr><tr><td>relu_sum</td><td>949567.0</td></tr><tr><td>target_calc_loss</td><td>6.3611</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>26.97666</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.34034</td></tr><tr><td>val_epoch_loss_wap</td><td>2.76185</td></tr><tr><td>val_loss_wap_ohe</td><td>2.80042</td></tr><tr><td>wap_pred_loss</td><td>7.60648</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crisp-sweep-9</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/hbx7iml6' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/hbx7iml6</a><br/>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_034136-hbx7iml6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wpg13tac with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.7106348414278701\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: AdamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_042557-wpg13tac</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/wpg13tac' target=\"_blank\">amber-sweep-10</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/wpg13tac' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/wpg13tac</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 2, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 256, 'hidden_size': 128, 'l1_squared': 2, 'learning_rate': 5e-05, 'learning_rate_reg': 1e-05, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.7106348414278701, 'ohe_squared': 1, 'ohe_targets': 'ahhh', 'optim': 'AdamW', 'remove_first_linear': True, 'warmup': 20, 'weigh_dif_factor': 1}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(23, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (fc_final): Linear(in_features=256, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86afb53cce3640f99ed2eca76bd9fc74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\bot0q4xz.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\25tz6tn8.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\6vzea6o0.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\bua6mft6.table.json'\n",
      "Epoch 00040: reducing learning rate of group 0 to 5.0000e-06.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\spxnj7gt.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\7iuhdi7l.table.json'\n",
      "Epoch 00051: reducing learning rate of group 0 to 2.5000e-06.\n",
      "5e-06\n",
      "1e-05\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▃▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>L1_loss_wap_epoch</td><td>████▇▇▇▇▆▆▅▄▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch_loss_ohe</td><td>█▇▆▅▅▅▅▄▄▄▃▃▃▃▃▃▃▁▃▃▂▃▃▃▁▂▁▂▂▃▃▁▂▁▂▂▃▅▁▃</td></tr><tr><td>epoch_loss_reg</td><td>▅▃▃▃█▄▃▃▃▄▂▅▅▆▃▂▃▁▆▂▆▅▂▃▃▅▁▃▇▃▄▃▃▁▆▅▃▄▅▃</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▇▆▅▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>relu_sum</td><td>███▇▇▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>█▆▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>███████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▆▅▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▆▅▅▅▅▅▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>▁▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.20129</td></tr><tr><td>L1_loss_wap_epoch</td><td>5.96446</td></tr><tr><td>epoch</td><td>50</td></tr><tr><td>epoch_loss_ohe</td><td>5.16901</td></tr><tr><td>epoch_loss_reg</td><td>3.57882</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.34176</td></tr><tr><td>relu_sum</td><td>860797.9375</td></tr><tr><td>target_calc_loss</td><td>6.46836</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>6.57556</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.19658</td></tr><tr><td>val_epoch_loss_wap</td><td>2.75383</td></tr><tr><td>val_loss_wap_ohe</td><td>2.79365</td></tr><tr><td>wap_pred_loss</td><td>7.25967</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-sweep-10</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/wpg13tac' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/wpg13tac</a><br/>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_042557-wpg13tac\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: aq359sbr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.3452290360125382\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: AdamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_044343-aq359sbr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/aq359sbr' target=\"_blank\">cerulean-sweep-11</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/aq359sbr' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/aq359sbr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 1, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 256, 'hidden_size': 128, 'l1_squared': 0.5, 'learning_rate': 5e-06, 'learning_rate_reg': 1e-05, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.3452290360125382, 'ohe_squared': 2, 'ohe_targets': 'ahhh', 'optim': 'AdamW', 'remove_first_linear': False, 'warmup': 5, 'weigh_dif_factor': 1}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(128, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc0): Linear(in_features=23, out_features=128, bias=True)\n",
      "  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (fc_final): Linear(in_features=256, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5e-06\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6cf5db08d6b477689a962dba42a2a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\x9zx03po.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\sqlkjrs0.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\33xy9qb0.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\vfqylwvn.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\cffryg96.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\bo4p12ka.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\94yc9kn8.table.json'\n",
      "Epoch 00066: reducing learning rate of group 0 to 2.5000e-06.\n",
      "Epoch 00069: reducing learning rate of group 0 to 5.0000e-06.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\2m15mxlb.table.json'\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.2500e-06.\n",
      "Epoch 00080: reducing learning rate of group 0 to 2.5000e-06.\n",
      "5e-06\n",
      "1e-05\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1b622091f340659e892ed35369b25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.023 MB of 0.023 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▁▂▂▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇█████████████</td></tr><tr><td>L1_loss_wap_epoch</td><td>██████▇▇▇▇▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▂▂▂▂▂▂</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch_loss_ohe</td><td>█▇▆▇▆▆▇▆▆▅▇▆▅▃▅▅▅▅▄▅▃▄▃▄▄▄▅▄▃▂▃▂▃▃▂▃▁▄▂▃</td></tr><tr><td>epoch_loss_reg</td><td>▅▄▂▄▃▃▇▄▆▄█▅▃▁▃▆▅▃▃▄▂▃▂▄▆▄█▅▃▁▃▃▅▃▃▇▂▆▂▅</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>relu_sum</td><td>███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>██▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>█████████████▇▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>▁▃▇▇▇▇▇▇▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.19242</td></tr><tr><td>L1_loss_wap_epoch</td><td>5.97393</td></tr><tr><td>epoch</td><td>79</td></tr><tr><td>epoch_loss_ohe</td><td>6.73218</td></tr><tr><td>epoch_loss_reg</td><td>2.07452</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.36712</td></tr><tr><td>relu_sum</td><td>1073921.125</td></tr><tr><td>target_calc_loss</td><td>6.30545</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>7.15494</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.3801</td></tr><tr><td>val_epoch_loss_wap</td><td>2.75777</td></tr><tr><td>val_loss_wap_ohe</td><td>2.79743</td></tr><tr><td>wap_pred_loss</td><td>7.56207</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cerulean-sweep-11</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/aq359sbr' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/aq359sbr</a><br/>Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_044343-aq359sbr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s6e00pq2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.9742398947433007\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: AdamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_051155-s6e00pq2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/s6e00pq2' target=\"_blank\">devout-sweep-12</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/s6e00pq2' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/s6e00pq2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 0, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 128, 'hidden_size': 128, 'l1_squared': 0.5, 'learning_rate': 1e-05, 'learning_rate_reg': 1e-05, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.9742398947433007, 'ohe_squared': 0.5, 'ohe_targets': 'ahhh', 'optim': 'AdamW', 'remove_first_linear': False, 'warmup': 5, 'weigh_dif_factor': 0}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(128, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc0): Linear(in_features=23, out_features=128, bias=True)\n",
      "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc_final): Linear(in_features=128, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e105a2d600274a2c962154cfc72dae06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\5i6wetkb.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\lsho4tlf.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\sjc5c7ts.table.json'\n",
      "Epoch 00030: reducing learning rate of group 0 to 5.0000e-06.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\spyerpmb.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\0qntdjyi.table.json'\n",
      "Epoch 00041: reducing learning rate of group 0 to 2.5000e-06.\n",
      "Epoch 00047: reducing learning rate of group 0 to 5.0000e-06.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\rrw1bnmp.table.json'\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.2500e-06.\n",
      "Epoch 00058: reducing learning rate of group 0 to 2.5000e-06.\n",
      "5e-06\n",
      "1e-05\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9919564bb5e04a65b9b54b69ca2adeea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.018 MB of 0.026 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.674010…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▁▂▃▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>L1_loss_wap_epoch</td><td>███████▇▇▇▆▆▆▅▅▅▄▃▃▂▂▂▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>epoch_loss_ohe</td><td>██▇▇▆▅▆▆▄▆▅▄▅▅▂▅▄▄▃▃▂▄▄▄▃▄▃▂▃▄▁▄▃▂▄▃▁▄▃▃</td></tr><tr><td>epoch_loss_reg</td><td>▅▄▄▃▄▇▂▃▇▂▄▆▁▃█▁▄▄▆▅▆▄▅▄▄▃▄▇▂▃▇▂▄▆▁▄█▁▄▄</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▇▆▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>relu_sum</td><td>██▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>█▇▇▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>█████████▇▇▇▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▅▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>▁▂▁▁▁▂▂▃▄▅▅▆▆▇▇▇██████████████▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.1943</td></tr><tr><td>L1_loss_wap_epoch</td><td>5.98377</td></tr><tr><td>epoch</td><td>57</td></tr><tr><td>epoch_loss_ohe</td><td>1.53262</td></tr><tr><td>epoch_loss_reg</td><td>0.41106</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.37421</td></tr><tr><td>relu_sum</td><td>505149.75</td></tr><tr><td>target_calc_loss</td><td>6.34842</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>2.39005</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.35545</td></tr><tr><td>val_epoch_loss_wap</td><td>2.76054</td></tr><tr><td>val_loss_wap_ohe</td><td>2.80082</td></tr><tr><td>wap_pred_loss</td><td>7.90636</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devout-sweep-12</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/s6e00pq2' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/s6e00pq2</a><br/>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_051155-s6e00pq2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: plqmzg8z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.4677260993356761\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: AdamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_053220-plqmzg8z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/plqmzg8z' target=\"_blank\">celestial-sweep-13</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/plqmzg8z' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/plqmzg8z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 2, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 256, 'hidden_size': 64, 'l1_squared': 1, 'learning_rate': 5e-05, 'learning_rate_reg': 0.0001, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.4677260993356761, 'ohe_squared': 0.5, 'ohe_targets': 'ahhh', 'optim': 'AdamW', 'remove_first_linear': True, 'warmup': 100, 'weigh_dif_factor': 2}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(23, 64, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "  (fc_final): Linear(in_features=256, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccdf7195ef894c6592c2050765fed434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\ikfms3zw.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\h3fgjz9i.table.json'\n",
      "Epoch 00017: reducing learning rate of group 0 to 5.0000e-05.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\f5vhsskv.table.json'\n",
      "Epoch 00028: reducing learning rate of group 0 to 2.5000e-05.\n",
      "5e-05\n",
      "0.0001\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▂▂▃▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█████</td></tr><tr><td>L1_loss_wap_epoch</td><td>█▇▅▃▁▁▁▂▂▃▃▃▃▃▄▄▃▃▄▄▄▄▄▄▄▅▅▅</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch_loss_ohe</td><td>█▇▆▅▅▅▆▆▅▅▅▅▄▄▄▄▄▄▄▄▄▂▃▃▃▃▃▂▂▃▃▁▃▃▂▁▂▂▁▃</td></tr><tr><td>epoch_loss_reg</td><td>▆▅▄▄▄▂██▄▄▇▄▃▄▆▃▆▇▃▆▅▃▅▅▆▃▃▃▂▇█▁▄▆▃▂▄▅▁▄</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▇▆▆▅▅▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>relu_sum</td><td>███▇▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▂▂▂▃▃▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>█▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>████▇▇▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>▂▁▁▂▃▄▅▅▆▆▇▇▇▇██████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.19146</td></tr><tr><td>L1_loss_wap_epoch</td><td>5.99128</td></tr><tr><td>epoch</td><td>27</td></tr><tr><td>epoch_loss_ohe</td><td>3.69355</td></tr><tr><td>epoch_loss_reg</td><td>3.49096</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.3443</td></tr><tr><td>relu_sum</td><td>947135.6875</td></tr><tr><td>target_calc_loss</td><td>6.58573</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>25.17469</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.21777</td></tr><tr><td>val_epoch_loss_wap</td><td>2.76033</td></tr><tr><td>val_loss_wap_ohe</td><td>2.80006</td></tr><tr><td>wap_pred_loss</td><td>7.09962</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">celestial-sweep-13</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/plqmzg8z' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/plqmzg8z</a><br/>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_053220-plqmzg8z\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jddsd3vo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.980245898558833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: RMSProp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_053749-jddsd3vo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/jddsd3vo' target=\"_blank\">twilight-sweep-14</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/jddsd3vo' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/jddsd3vo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 1, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 128, 'hidden_size': 256, 'l1_squared': 2, 'learning_rate': 1e-05, 'learning_rate_reg': 5e-05, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.980245898558833, 'ohe_squared': 0.5, 'ohe_targets': 'ahhh', 'optim': 'RMSProp', 'remove_first_linear': True, 'warmup': 20, 'weigh_dif_factor': 0}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(23, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc_final): Linear(in_features=128, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c3aac71df54aeba541ec4355ebf444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\6yjqxlae.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\uav91j6g.table.json'\n",
      "Epoch 00019: reducing learning rate of group 0 to 2.5000e-05.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\d97rpttj.table.json'\n",
      "Epoch 00025: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.2500e-05.\n",
      "2.5e-05\n",
      "5e-05\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d0a0a5b0f04d9a9b6770892670d244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.172 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.054779…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▂▃▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>L1_loss_wap_epoch</td><td>▇▆▅▄▃▂▁▁▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>epoch_loss_ohe</td><td>███▇▇▇▇▃▆▅▆▆▅▄▅▄▃▅▅▄▄▅▄▄▄▄▄▁▁▄▃▄▄▄▃▄▂▁▄▃</td></tr><tr><td>epoch_loss_reg</td><td>▅▄▃▃▃▂▃█▁▄▃▃▃▆▃▅▇▂▃▃▃▄▄▃▃▃▂▇█▁▄▃▂▂▄▂▅▆▁▃</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▇▆▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>relu_sum</td><td>██▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▁▂▂▂▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>█▇▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>███▇▇▇▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>▁▂▄▅▇▇█████▇▇▇▆▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.18875</td></tr><tr><td>L1_loss_wap_epoch</td><td>6.01363</td></tr><tr><td>epoch</td><td>29</td></tr><tr><td>epoch_loss_ohe</td><td>1.5372</td></tr><tr><td>epoch_loss_reg</td><td>0.01698</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.37022</td></tr><tr><td>relu_sum</td><td>445073.0625</td></tr><tr><td>target_calc_loss</td><td>6.4607</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>2.38779</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.3088</td></tr><tr><td>val_epoch_loss_wap</td><td>2.76371</td></tr><tr><td>val_loss_wap_ohe</td><td>2.8025</td></tr><tr><td>wap_pred_loss</td><td>7.62329</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twilight-sweep-14</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/jddsd3vo' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/jddsd3vo</a><br/>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_053749-jddsd3vo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pkac2yhk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.039745818309555814\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: RMSProp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_060159-pkac2yhk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/pkac2yhk' target=\"_blank\">noble-sweep-15</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/pkac2yhk' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/pkac2yhk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 1, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 128, 'hidden_size': 128, 'l1_squared': 0.5, 'learning_rate': 5e-05, 'learning_rate_reg': 0.0001, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.039745818309555814, 'ohe_squared': 1, 'ohe_targets': 'ahhh', 'optim': 'RMSProp', 'remove_first_linear': False, 'warmup': 0, 'weigh_dif_factor': 0}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(128, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc0): Linear(in_features=23, out_features=128, bias=True)\n",
      "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc_final): Linear(in_features=128, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0996ee8a5e740d5870c42e9c20ce382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\b1i5zskp.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\e0gkaoxx.table.json'\n",
      "Epoch 00020: reducing learning rate of group 0 to 5.0000e-05.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\h0huhynb.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\plleq5je.table.json'\n",
      "Epoch 00031: reducing learning rate of group 0 to 2.5000e-05.\n",
      "5e-05\n",
      "0.0001\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▂▃▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇█████████</td></tr><tr><td>L1_loss_wap_epoch</td><td>▅▅▅▄▃▃▂▁▁▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>epoch_loss_ohe</td><td>███▇▇▇▆▆▅▄▅▆▅▄▅▄▅▃▃▄▃▃▃▄▄▄▁▁▄▃▂▄▄▃▄▂▁▄▄▃</td></tr><tr><td>epoch_loss_reg</td><td>▆▆▄▄▄▃▄▅▅▇▄▃▄▆▁▅▃▆▆▄▅▆▆▁▄▂██▁▅▇▃▂▅▃▆▇▂▃▄</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▆▅▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>relu_sum</td><td>█▇▆▆▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▁▁▂▂▂▃▃▄▄▅▅▅▅▆▆▆▇▇▇▇▇▇██████</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>█▆▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>█████▇▇▇▇▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▆▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>▃▂▁▁▁▂▂▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.19169</td></tr><tr><td>L1_loss_wap_epoch</td><td>6.03113</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>epoch_loss_ohe</td><td>0.09474</td></tr><tr><td>epoch_loss_reg</td><td>2.51483</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.25739</td></tr><tr><td>relu_sum</td><td>367915.375</td></tr><tr><td>target_calc_loss</td><td>6.4988</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>2.35772</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.23644</td></tr><tr><td>val_epoch_loss_wap</td><td>2.76045</td></tr><tr><td>val_loss_wap_ohe</td><td>2.8003</td></tr><tr><td>wap_pred_loss</td><td>7.51066</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">noble-sweep-15</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/pkac2yhk' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/pkac2yhk</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_060159-pkac2yhk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z96mt7au with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.022277894149245293\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: AdamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_061318-z96mt7au</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/z96mt7au' target=\"_blank\">effortless-sweep-16</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/z96mt7au' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/z96mt7au</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 1, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 128, 'hidden_size': 256, 'l1_squared': 2, 'learning_rate': 1e-05, 'learning_rate_reg': 5e-05, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.022277894149245293, 'ohe_squared': 2, 'ohe_targets': 'ahhh', 'optim': 'AdamW', 'remove_first_linear': False, 'warmup': 0, 'weigh_dif_factor': 2}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(256, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc0): Linear(in_features=23, out_features=256, bias=True)\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc_final): Linear(in_features=128, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0d6ee86cb847d39f751af83c1960df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\2ws2uimk.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\qgpmihcy.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\mlizeryj.table.json'\n",
      "Epoch 00021: reducing learning rate of group 0 to 2.5000e-05.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\ze4ookpd.table.json'\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.2500e-05.\n",
      "2.5e-05\n",
      "5e-05\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▃▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>L1_loss_wap_epoch</td><td>██▇▆▅▄▃▂▁▁▁▁▂▂▃▃▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>epoch_loss_ohe</td><td>██▇▆▅▄▅▆▆▆▄▃▅▄▅▅▃▄▄▄▄▄▂▄▃▄▄▁▄▄▃▃▃▁▃▃▃▃▁▅</td></tr><tr><td>epoch_loss_reg</td><td>▆▅▃▃▃▃▃▄▄▇▃▂▄▃▆▇▂▃▃▅▅▆▁▃▂██▁▄▃▂▃▆▁▄▂▅▅▃▄</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▇▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>relu_sum</td><td>███▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▁▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>██▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>█████▇▇▇▇▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▅▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>█▆▆▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.19551</td></tr><tr><td>L1_loss_wap_epoch</td><td>6.00326</td></tr><tr><td>epoch</td><td>31</td></tr><tr><td>epoch_loss_ohe</td><td>0.4355</td></tr><tr><td>epoch_loss_reg</td><td>41.25852</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.33982</td></tr><tr><td>relu_sum</td><td>423099.6875</td></tr><tr><td>target_calc_loss</td><td>6.54154</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>26.24695</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.2654</td></tr><tr><td>val_epoch_loss_wap</td><td>2.75879</td></tr><tr><td>val_loss_wap_ohe</td><td>2.79827</td></tr><tr><td>wap_pred_loss</td><td>7.74742</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">effortless-sweep-16</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/z96mt7au' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/z96mt7au</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_061318-z96mt7au\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: re0tlqsm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.08483377737531905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: AdamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_064041-re0tlqsm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/re0tlqsm' target=\"_blank\">stellar-sweep-17</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/re0tlqsm' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/re0tlqsm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 2, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 256, 'hidden_size': 256, 'l1_squared': 2, 'learning_rate': 5e-06, 'learning_rate_reg': 0.0001, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.08483377737531905, 'ohe_squared': 0.5, 'ohe_targets': 'ahhh', 'optim': 'AdamW', 'remove_first_linear': False, 'warmup': 10, 'weigh_dif_factor': 0}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(256, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc0): Linear(in_features=23, out_features=256, bias=True)\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc_final): Linear(in_features=256, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5e-06\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41a8e0e655f4881874d200dca5e90fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\tay7fpor.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\y4lzitql.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\utyuthe8.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\51bv2afo.table.json'\n",
      "Epoch 00032: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 00033: reducing learning rate of group 0 to 2.5000e-06.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\tee6aogv.table.json'\n",
      "Epoch 00043: reducing learning rate of group 0 to 2.5000e-05.\n",
      "5e-05\n",
      "0.0001\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d7b769a6d54547a899635562476873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.015 MB of 0.015 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>L1_loss_wap_epoch</td><td>██▇▇▆▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch_loss_ohe</td><td>█▇█▇▅▇▆▆▆▅▆▃▆▄▅▄▅▄▅▄▄▄▄▄▂▂▂▂▂▂▃▃▃▃▃▃▁▃▃▃</td></tr><tr><td>epoch_loss_reg</td><td>▆▆▃▃█▄▄▃▂▄▁▇▂▅▃▅▃▃▂▃▄▄▃▂▆▅▇▅▅▅▄▃▃▂▃▁▆▃▃▂</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▇▇▆▆▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>relu_sum</td><td>█████▇▇▇▇▇▇▇▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>█▇▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>███████▇▇▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▆▅▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▆▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>█▂▁▁▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.18948</td></tr><tr><td>L1_loss_wap_epoch</td><td>5.97736</td></tr><tr><td>epoch</td><td>42</td></tr><tr><td>epoch_loss_ohe</td><td>0.45288</td></tr><tr><td>epoch_loss_reg</td><td>36.50203</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.388</td></tr><tr><td>relu_sum</td><td>998852.9375</td></tr><tr><td>target_calc_loss</td><td>6.4005</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>2.39661</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.32364</td></tr><tr><td>val_epoch_loss_wap</td><td>2.76002</td></tr><tr><td>val_loss_wap_ohe</td><td>2.79905</td></tr><tr><td>wap_pred_loss</td><td>7.56951</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-sweep-17</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/re0tlqsm' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/re0tlqsm</a><br/>Synced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_064041-re0tlqsm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: t3cu064a with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.9559395908697382\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: RMSProp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_071726-t3cu064a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/t3cu064a' target=\"_blank\">classic-sweep-18</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/t3cu064a' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/t3cu064a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 1, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 128, 'hidden_size': 64, 'l1_squared': 2, 'learning_rate': 5e-06, 'learning_rate_reg': 5e-06, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.9559395908697382, 'ohe_squared': 0.5, 'ohe_targets': 'ahhh', 'optim': 'RMSProp', 'remove_first_linear': True, 'warmup': 100, 'weigh_dif_factor': 2}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(23, 64, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (fc_final): Linear(in_features=128, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5e-06\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 5e-06\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a03769dda654f6198485596930527c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\3k2d7wrm.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\kvkz2lyv.table.json'\n",
      "Epoch 00016: reducing learning rate of group 0 to 2.5000e-06.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\906kfa02.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\gpaqp3tq.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\05br3o31.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\61clff2h.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\so4fu9rx.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\nwps8cw7.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\s4emlg1y.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\f79jn64q.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\k90mar9e.table.json'\n",
      "Epoch 00104: reducing learning rate of group 0 to 2.5000e-06.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\r9bhbfkm.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\xoyezlm5.table.json'\n",
      "Epoch 00125: reducing learning rate of group 0 to 1.2500e-06.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\7jsziuca.table.json'\n",
      "Epoch 00136: reducing learning rate of group 0 to 6.2500e-07.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\kshcono7.table.json'\n",
      "Epoch 00147: reducing learning rate of group 0 to 3.1250e-07.\n",
      "Epoch 00147: reducing learning rate of group 0 to 1.2500e-06.\n",
      "2.5e-06\n",
      "5e-06\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea91e5a2b36541ffb8551dcdc83530bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.042 MB of 0.042 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>L1_loss_wap_epoch</td><td>██████████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch_loss_ohe</td><td>▇▇▆██▅▇▆▅▆▆▄▅▆▇▅▄▆▄▅▅▄▂▆▄▅▄▂▁▅▄▃▄▁▄▃▃▄▂▁</td></tr><tr><td>epoch_loss_reg</td><td>▃▃▃▆▅▂▄▆▃▃▄▂▃▆█▃▂▄▃▄▅▃▁█▆▆▄▂▂▇▅▃▃▁▅▃▃▄▁▁</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▇▇▆▆▅▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>relu_sum</td><td>███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>██▇▇▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>█████████████▇▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▆▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▆▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>█▇▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.18585</td></tr><tr><td>L1_loss_wap_epoch</td><td>5.96751</td></tr><tr><td>epoch</td><td>146</td></tr><tr><td>epoch_loss_ohe</td><td>5.31459</td></tr><tr><td>epoch_loss_reg</td><td>0.08374</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.33888</td></tr><tr><td>relu_sum</td><td>508197.125</td></tr><tr><td>target_calc_loss</td><td>6.19053</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>27.28899</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.40739</td></tr><tr><td>val_epoch_loss_wap</td><td>2.76065</td></tr><tr><td>val_loss_wap_ohe</td><td>2.79893</td></tr><tr><td>wap_pred_loss</td><td>7.89141</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">classic-sweep-18</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/t3cu064a' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/t3cu064a</a><br/>Synced 5 W&B file(s), 0 media file(s), 15 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_071726-t3cu064a\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jd6ek3jm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.4487716508305972\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: RMSProp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_074457-jd6ek3jm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/jd6ek3jm' target=\"_blank\">devoted-sweep-19</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/jd6ek3jm' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/jd6ek3jm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 1, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 256, 'hidden_size': 32, 'l1_squared': 0.5, 'learning_rate': 1e-05, 'learning_rate_reg': 1e-05, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.4487716508305972, 'ohe_squared': 2, 'ohe_targets': 'ahhh', 'optim': 'RMSProp', 'remove_first_linear': False, 'warmup': 100, 'weigh_dif_factor': 2}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(32, 32, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc0): Linear(in_features=23, out_features=32, bias=True)\n",
      "  (fc1): Linear(in_features=32, out_features=256, bias=True)\n",
      "  (fc_final): Linear(in_features=256, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06eb7a5b40e4b68abe8403a2f1ae069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\4u2ywlbj.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\fomlcxs8.table.json'\n",
      "Epoch 00012: reducing learning rate of group 0 to 5.0000e-06.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\y6h8y8w9.table.json'\n",
      "Epoch 00023: reducing learning rate of group 0 to 2.5000e-06.\n",
      "5e-06\n",
      "1e-05\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▂▃▅▆▇██████▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>L1_loss_wap_epoch</td><td>▄▆▇███████▇▇▇▆▆▆▅▅▄▄▃▂▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch_loss_ohe</td><td>▅▆▇▆▆▃▄▃▅█▄▆▄▅▄▅▅▂▄▄▃▆▃▆▁▄▆▄▆▂▄▄▃▃▄▇▁▄▅▄</td></tr><tr><td>epoch_loss_reg</td><td>▃▃▆▅▃▁▂▂▃█▃▄▄▄▃▅▄▁▃▄▂▇▃▆▁▄▇▄▆▂▄▆▄▄▅█▂▄▆▅</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▇▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>relu_sum</td><td>█▇▇▇▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▂▂▁▁▁▁▁▁▁▂▂▂▃▃▃▄▄▅▅▆▆▇█</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>█▇▇▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>██▇▇▇▆▅▅▅▄▄▄▄▄▃▃▂▃▃▂▂▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▆▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>█▇▆▅▄▃▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.14757</td></tr><tr><td>L1_loss_wap_epoch</td><td>6.00739</td></tr><tr><td>epoch</td><td>22</td></tr><tr><td>epoch_loss_ohe</td><td>201.69682</td></tr><tr><td>epoch_loss_reg</td><td>1.905</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.38323</td></tr><tr><td>relu_sum</td><td>1096527.0</td></tr><tr><td>target_calc_loss</td><td>6.07237</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>30.89445</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.492</td></tr><tr><td>val_epoch_loss_wap</td><td>2.76962</td></tr><tr><td>val_loss_wap_ohe</td><td>2.80727</td></tr><tr><td>wap_pred_loss</td><td>7.58247</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devoted-sweep-19</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/jd6ek3jm' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/jd6ek3jm</a><br/>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_074457-jd6ek3jm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2ids63u5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.23180718255575236\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: AdamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_074942-2ids63u5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/2ids63u5' target=\"_blank\">summer-sweep-20</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/2ids63u5' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/2ids63u5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 1, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 256, 'hidden_size': 64, 'l1_squared': 0.5, 'learning_rate': 1e-05, 'learning_rate_reg': 5e-06, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.23180718255575236, 'ohe_squared': 1, 'ohe_targets': 'ahhh', 'optim': 'AdamW', 'remove_first_linear': False, 'warmup': 5, 'weigh_dif_factor': 1}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(64, 64, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc0): Linear(in_features=23, out_features=64, bias=True)\n",
      "  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "  (fc_final): Linear(in_features=256, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5e-06\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daca752f09ca4e93a186797905e7e66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\sxo2cg60.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\fo3z8hwd.table.json'\n",
      "Epoch 00012: reducing learning rate of group 0 to 2.5000e-06.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\ketyyh80.table.json'\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.2500e-06.\n",
      "2.5e-06\n",
      "5e-06\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a137ea463bb64450b5e7782d64cd1d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▃▄▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>L1_loss_wap_epoch</td><td>▂▁▁▁▂▃▄▅▆▆▇▇███████▇▇▆▆</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch_loss_ohe</td><td>██▇▆▅▄▄▄▄▅▃▃▃▃▃▃▃▂▃▂▂▃▂▃▁▂▃▂▃▁▂▂▂▂▂▃▁▂▂▂</td></tr><tr><td>epoch_loss_reg</td><td>▃▃▆▅▃▁▂▂▃█▃▄▄▄▃▅▄▁▃▄▂▇▃▆▁▄▇▄▆▂▄▆▄▄▅█▂▄▆▅</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>relu_sum</td><td>█▇▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▂▂▂▁▁▁</td></tr><tr><td>target_calc_loss</td><td>█▇▇▇▇▆▅▄▃▂▂▂▁▁▁▁▁▁▂▂▃▄▄</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>█▆▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>█▇▆▅▅▅▄▅▄▅▄▄▄▄▃▄▄▃▃▂▂▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>█▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.16286</td></tr><tr><td>L1_loss_wap_epoch</td><td>6.00737</td></tr><tr><td>epoch</td><td>22</td></tr><tr><td>epoch_loss_ohe</td><td>1.80984</td></tr><tr><td>epoch_loss_reg</td><td>2.24915</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.39423</td></tr><tr><td>relu_sum</td><td>1065320.25</td></tr><tr><td>target_calc_loss</td><td>6.07188</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>7.69579</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.49255</td></tr><tr><td>val_epoch_loss_wap</td><td>2.76458</td></tr><tr><td>val_loss_wap_ohe</td><td>2.80092</td></tr><tr><td>wap_pred_loss</td><td>6.5975</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">summer-sweep-20</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/2ids63u5' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/2ids63u5</a><br/>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_074942-2ids63u5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b89c7e9q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.3802552769150187\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: RMSProp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_075423-b89c7e9q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/b89c7e9q' target=\"_blank\">generous-sweep-21</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/b89c7e9q' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/b89c7e9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 2, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 256, 'hidden_size': 32, 'l1_squared': 1, 'learning_rate': 0.0001, 'learning_rate_reg': 5e-05, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.3802552769150187, 'ohe_squared': 1, 'ohe_targets': 'ahhh', 'optim': 'RMSProp', 'remove_first_linear': True, 'warmup': 30, 'weigh_dif_factor': 2}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(23, 32, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=32, out_features=256, bias=True)\n",
      "  (fc_final): Linear(in_features=256, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa47475b8214720be0b5b522a1332c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\f0o7saxa.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\jxe9ll0c.table.json'\n",
      "Epoch 00017: reducing learning rate of group 0 to 2.5000e-05.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\fg2dwjz3.table.json'\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.2500e-05.\n",
      "2.5e-05\n",
      "5e-05\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64066bf0886d462d9518272156f76ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▂▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇███████</td></tr><tr><td>L1_loss_wap_epoch</td><td>█▇▅▄▂▁▁▂▄▅▆▆▇▆▆▆▆▆▅▅▅▅▅▅▅▅▄▄</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch_loss_ohe</td><td>█▆▆▆▅▅▆▆▄▆▆▄▅▅▃▄▅▄▃▅▅▂▅▅▃▄▄▃▁▄▄▁▅▅▂▂▅▃▁▃</td></tr><tr><td>epoch_loss_reg</td><td>▆▅▄▄▄▂██▄▄▇▄▃▄▆▃▆▇▃▆▅▃▅▅▆▃▃▃▂▇▇▁▄▆▃▂▄▅▁▄</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▆▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>relu_sum</td><td>█▇▇▆▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▂▂▃▃▄▅▅▆▆▆▇▇▇▇▇▇▇████████</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>█▆▆▅▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>███▇▇▆▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▆▆▅▅▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>▁▁▂▂▃▃▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.19667</td></tr><tr><td>L1_loss_wap_epoch</td><td>5.98697</td></tr><tr><td>epoch</td><td>27</td></tr><tr><td>epoch_loss_ohe</td><td>11.11491</td></tr><tr><td>epoch_loss_reg</td><td>4.06245</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.32014</td></tr><tr><td>relu_sum</td><td>907418.625</td></tr><tr><td>target_calc_loss</td><td>6.55997</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>25.01314</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.19894</td></tr><tr><td>val_epoch_loss_wap</td><td>2.75612</td></tr><tr><td>val_loss_wap_ohe</td><td>2.79508</td></tr><tr><td>wap_pred_loss</td><td>7.31583</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">generous-sweep-21</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/b89c7e9q' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/b89c7e9q</a><br/>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_075423-b89c7e9q\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: paztcmur with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.24009947604048276\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: AdamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_075942-paztcmur</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/paztcmur' target=\"_blank\">rich-sweep-22</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/paztcmur' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/paztcmur</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 2, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 128, 'hidden_size': 32, 'l1_squared': 2, 'learning_rate': 0.0001, 'learning_rate_reg': 0.0001, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.24009947604048276, 'ohe_squared': 2, 'ohe_targets': 'ahhh', 'optim': 'AdamW', 'remove_first_linear': False, 'warmup': 0, 'weigh_dif_factor': 2}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(32, 32, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc0): Linear(in_features=23, out_features=32, bias=True)\n",
      "  (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "  (fc_final): Linear(in_features=128, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47da7c2399d468798999b62a15f3afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\c29qpzve.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\y0prk0xo.table.json'\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\yfd83cpc.table.json'\n",
      "Epoch 00022: reducing learning rate of group 0 to 5.0000e-05.\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\hvi94ouf.table.json'\n",
      "Epoch 00033: reducing learning rate of group 0 to 2.5000e-05.\n",
      "5e-05\n",
      "0.0001\n",
      "Finished Early on reg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2fd9fdfb8d4ec89c4ebe3286fcaf35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.012 MB of 0.012 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▆▇▇▇▇▇▇█████████████</td></tr><tr><td>L1_loss_wap_epoch</td><td>▆▆▆▅▅▅▄▃▂▁▁▁▁▁▁▁▂▂▃▃▄▄▅▅▆▆▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>epoch_loss_ohe</td><td>▇█▅▇▆█▇▄▇▆▅▅▅▃▅▅▅▄▂▆▄▄▃▂▂▃▄▃▃▂▁▄▃▃▃▁▃▄▃▃</td></tr><tr><td>epoch_loss_reg</td><td>▅▆▁▃▂██▁▄▃▂▃▆▁▄▂▅▅▃▄▄▃▂▃▂▂▃▃▅▂▁▃▂▅▆▁▂▂▄▄</td></tr><tr><td>losst_to_zero</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>output_sd</td><td>█▇▅▄▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>relu_sum</td><td>███▇▇▇▆▆▅▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>target_calc_loss</td><td>▁▁▁▁▁▁▁▁▂▂▂▃▃▄▄▅▅▆▆▆▇▇▇▇▇████████</td></tr><tr><td>train_class_tgt_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_class_wap_loss</td><td>▆██▇▇▆▆▆▅▅▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_target_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_wap_loss</td><td>████████▇▇▇▆▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_epoch_loss_wap</td><td>█▆▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_wap_ohe</td><td>█▆▅▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wap_pred_loss</td><td>▁▁▁▂▂▁▂▃▄▅▆▇▇▇███████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy_wap</td><td>0.19599</td></tr><tr><td>L1_loss_wap_epoch</td><td>6.01834</td></tr><tr><td>epoch</td><td>32</td></tr><tr><td>epoch_loss_ohe</td><td>50.59181</td></tr><tr><td>epoch_loss_reg</td><td>24.96067</td></tr><tr><td>losst_to_zero</td><td>6.07133</td></tr><tr><td>output_sd</td><td>0.28248</td></tr><tr><td>relu_sum</td><td>470190.09375</td></tr><tr><td>target_calc_loss</td><td>6.65393</td></tr><tr><td>train_class_tgt_loss</td><td>0.0</td></tr><tr><td>train_class_wap_loss</td><td>25.2996</td></tr><tr><td>train_target_loss</td><td>0.0</td></tr><tr><td>train_wap_loss</td><td>6.20779</td></tr><tr><td>val_epoch_loss_wap</td><td>2.75927</td></tr><tr><td>val_loss_wap_ohe</td><td>2.79934</td></tr><tr><td>wap_pred_loss</td><td>7.79385</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rich-sweep-22</strong> at: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/paztcmur' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/paztcmur</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_075942-paztcmur\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0du0hwpi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdetach: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf0_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc0_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_squared: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_reg: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmini_batches: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_loss_weight: 0.35410106635315547\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_squared: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tohe_targets: ahhh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: RMSProp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tremove_first_linear: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweigh_dif_factor: 2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\OptiverKaggle\\wandb\\run-20231219_080624-0du0hwpi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/0du0hwpi' target=\"_blank\">jumping-sweep-23</a></strong> to <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/sweeps/ae878ias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/0du0hwpi' target=\"_blank\">https://wandb.ai/nickojelly/New_final_sweeps_Optiver/runs/0du0hwpi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'detach': 0, 'epochs': 200, 'f0_layer_size': 128, 'f1_layer_size': 64, 'fc0_size': 256, 'hidden_size': 256, 'l1_squared': 2, 'learning_rate': 1e-05, 'learning_rate_reg': 1e-05, 'mini_batches': 10, 'num_layers': 2, 'ohe_loss_weight': 0.35410106635315547, 'ohe_squared': 2, 'ohe_targets': 'ahhh', 'optim': 'RMSProp', 'remove_first_linear': False, 'warmup': 20, 'weigh_dif_factor': 2}\n",
      "ahhh\n",
      "GRUNetV5_a(\n",
      "  (gru): GRU(256, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (batch_norm): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc0): Linear(in_features=23, out_features=256, bias=True)\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc_final): Linear(in_features=256, out_features=7, bias=True)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "GRUNetV5_b(\n",
      "  (relu): ReLU()\n",
      "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc_reg0): Linear(in_features=14, out_features=128, bias=True)\n",
      "  (fc_reg1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_reg2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9139b6f6fd4d5bb5a21efc24d9b8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nick\\\\AppData\\\\Local\\\\Temp\\\\tmpduqf0sibwandb-media\\\\82zj1ow6.table.json'\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\"method\": \"bayes\"}\n",
    "\n",
    "metric = {\"name\": \"L1_loss_wap_epoch.min\", \"goal\": \"minimize\"}\n",
    "\n",
    "sweep_config[\"metric\"] = metric\n",
    "\n",
    "\n",
    "parameters_dict = {\n",
    "    \"optim\": {\"values\": [\"RMSProp\", \"AdamW\"]},\n",
    "    \"f0_layer_size\": {\"values\": [128]},\n",
    "    \"f1_layer_size\": {\"values\": [64]},\n",
    "    \"num_layers\": {\"values\": [2]},\n",
    "    \"hidden_size\": {\"values\": [32,64,128,256]},\n",
    "    \"fc0_size\": {\"values\": [128,256]},\n",
    "    \"epochs\": {\"value\": 200},\n",
    "    \"mini_batches\": {\"value\": 10},\n",
    "    'ohe_targets': {\"value\": 'ahhh'},\n",
    "    'ohe_loss_weight': {\"max\": 1.0, \"min\": 0.0001},\n",
    "    'learning_rate': {\"values\": [0.0001,0.00005,0.00001,0.000005]},\n",
    "    'learning_rate_reg': {\"values\": [0.0001,0.00005,0.00001,0.000005]},\n",
    "    \"l1_squared\": {\"values\": [0.5,1,2]},\n",
    "    \"detach\": {\"values\": [0,1,2]},\n",
    "    \"ohe_squared\": {\"values\": [0.5,1,2]},\n",
    "    'weigh_dif_factor': {\"values\": [0,1,2]},\n",
    "    'remove_first_linear':{\"values\":[True,False]},\n",
    "    \"warmup\": {\"values\": [0,5,10,20,30,100]},\n",
    "}\n",
    "\n",
    "sweep_config[\"parameters\"] = parameters_dict\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"New_final_sweeps_Optiver\")\n",
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "wandb.agent(sweep_id, function=model_pipeline, count=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
